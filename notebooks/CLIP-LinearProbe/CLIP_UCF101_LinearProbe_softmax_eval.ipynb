{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "new-repo-CLIP_UCF101_LinearProbe_softmax_eval.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "RGlVnbugxFK2",
        "3rKe3HqM523g",
        "OLU-gp7n8__E",
        "9z1WQnXdLHy2",
        "zU_gxQ0KbwMh",
        "UxTa8MVsvQCN",
        "iLbRqaYxzbr7"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2wCVCAfpyE0A"
      },
      "source": [
        "# Mount Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4-7JU3Q7q_XT",
        "outputId": "73fb4aa1-1566-4310-b654-1a25398bc1a1"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vXAPOzOK4mTl",
        "outputId": "f2077a3c-5779-4d69-8e7b-d83d88e0ee47"
      },
      "source": [
        "!pip install -U -q PyDrive\n",
        "!pip install httplib2==0.15.0\n",
        "import os\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from pydrive.files import GoogleDriveFileList\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "from getpass import getpass\n",
        "import urllib\n",
        "\n",
        "# 1. Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "# Cloning CLIPPER to access modules.\n",
        "if 'CLIPPER' not in os.listdir():\n",
        "    cmd_string = 'git clone https://github.com/PAL-ML/CLIPPER.git'\n",
        "    os.system(cmd_string)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting httplib2==0.15.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/be/83/5e006e25403871ffbbf587c7aa4650158c947d46e89f2d50dcaf018464de/httplib2-0.15.0-py3-none-any.whl (94kB)\n",
            "\r\u001b[K     |███▌                            | 10kB 18.4MB/s eta 0:00:01\r\u001b[K     |███████                         | 20kB 22.7MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 30kB 26.1MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 40kB 21.3MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 51kB 12.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 61kB 10.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 71kB 11.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 81kB 12.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 92kB 13.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 102kB 7.2MB/s \n",
            "\u001b[?25hInstalling collected packages: httplib2\n",
            "  Found existing installation: httplib2 0.17.4\n",
            "    Uninstalling httplib2-0.17.4:\n",
            "      Successfully uninstalled httplib2-0.17.4\n",
            "Successfully installed httplib2-0.15.0\n",
            "Github User name: ashadhaz\n",
            "Password: ··········\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RGlVnbugxFK2"
      },
      "source": [
        "# Installation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gmgIrfT8hDNE"
      },
      "source": [
        "## Install multi label metrics dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b6xXPAFbe6Gp",
        "outputId": "136b8889-a42e-42c5-d2a0-4d6af97a701e"
      },
      "source": [
        "! pip install scikit-learn==0.24"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting scikit-learn==0.24\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b1/ed/ab51a8da34d2b3f4524b21093081e7f9e2ddf1c9eac9f795dcf68ad0a57d/scikit_learn-0.24.0-cp37-cp37m-manylinux2010_x86_64.whl (22.3MB)\n",
            "\u001b[K     |████████████████████████████████| 22.3MB 122kB/s \n",
            "\u001b[?25hCollecting threadpoolctl>=2.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/f7/12/ec3f2e203afa394a149911729357aa48affc59c20e2c1c8297a60f33f133/threadpoolctl-2.1.0-py3-none-any.whl\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.24) (1.0.1)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.24) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.24) (1.19.5)\n",
            "Installing collected packages: threadpoolctl, scikit-learn\n",
            "  Found existing installation: scikit-learn 0.22.2.post1\n",
            "    Uninstalling scikit-learn-0.22.2.post1:\n",
            "      Successfully uninstalled scikit-learn-0.22.2.post1\n",
            "Successfully installed scikit-learn-0.24.0 threadpoolctl-2.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3rKe3HqM523g"
      },
      "source": [
        "## Install CLIP dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "poS-WNDixIhY",
        "outputId": "d8c4e287-ae32-4746-94aa-b17225a3a60f"
      },
      "source": [
        "import subprocess\n",
        "\n",
        "CUDA_version = [s for s in subprocess.check_output([\"nvcc\", \"--version\"]).decode(\"UTF-8\").split(\", \") if s.startswith(\"release\")][0].split(\" \")[-1]\n",
        "print(\"CUDA version:\", CUDA_version)\n",
        "\n",
        "if CUDA_version == \"10.0\":\n",
        "    torch_version_suffix = \"+cu100\"\n",
        "elif CUDA_version == \"10.1\":\n",
        "    torch_version_suffix = \"+cu101\"\n",
        "elif CUDA_version == \"10.2\":\n",
        "    torch_version_suffix = \"\"\n",
        "else:\n",
        "    torch_version_suffix = \"+cu110\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CUDA version: 11.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uA-69W8M59nA",
        "outputId": "b2cf8b6d-b634-47da-b845-d5bc04c8e7bc"
      },
      "source": [
        "! pip install torch==1.7.1{torch_version_suffix} torchvision==0.8.2{torch_version_suffix} -f https://download.pytorch.org/whl/torch_stable.html ftfy regex"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Collecting torch==1.7.1+cu110\n",
            "\u001b[?25l  Downloading https://download.pytorch.org/whl/cu110/torch-1.7.1%2Bcu110-cp37-cp37m-linux_x86_64.whl (1156.8MB)\n",
            "\u001b[K     |███████████████████████         | 834.1MB 1.4MB/s eta 0:03:46tcmalloc: large alloc 1147494400 bytes == 0x563179920000 @  0x7ff5019bd615 0x563141186cdc 0x56314126652a 0x563141189afd 0x56314127afed 0x5631411fd988 0x5631411f84ae 0x56314118b3ea 0x5631411fd7f0 0x5631411f84ae 0x56314118b3ea 0x5631411fa32a 0x56314127be36 0x5631411f9853 0x56314127be36 0x5631411f9853 0x56314127be36 0x5631411f9853 0x56314127be36 0x5631412fe3e1 0x56314125e6a9 0x5631411c9cc4 0x56314118a559 0x5631411fe4f8 0x56314118b30a 0x5631411f93b5 0x5631411f87ad 0x56314118b3ea 0x5631411f93b5 0x56314118b30a 0x5631411f93b5\n",
            "\u001b[K     |█████████████████████████████▏  | 1055.7MB 1.4MB/s eta 0:01:12tcmalloc: large alloc 1434370048 bytes == 0x5631bdf76000 @  0x7ff5019bd615 0x563141186cdc 0x56314126652a 0x563141189afd 0x56314127afed 0x5631411fd988 0x5631411f84ae 0x56314118b3ea 0x5631411fd7f0 0x5631411f84ae 0x56314118b3ea 0x5631411fa32a 0x56314127be36 0x5631411f9853 0x56314127be36 0x5631411f9853 0x56314127be36 0x5631411f9853 0x56314127be36 0x5631412fe3e1 0x56314125e6a9 0x5631411c9cc4 0x56314118a559 0x5631411fe4f8 0x56314118b30a 0x5631411f93b5 0x5631411f87ad 0x56314118b3ea 0x5631411f93b5 0x56314118b30a 0x5631411f93b5\n",
            "\u001b[K     |████████████████████████████████| 1156.7MB 1.3MB/s eta 0:00:01tcmalloc: large alloc 1445945344 bytes == 0x563213762000 @  0x7ff5019bd615 0x563141186cdc 0x56314126652a 0x563141189afd 0x56314127afed 0x5631411fd988 0x5631411f84ae 0x56314118b3ea 0x5631411f960e 0x5631411f84ae 0x56314118b3ea 0x5631411f960e 0x5631411f84ae 0x56314118b3ea 0x5631411f960e 0x5631411f84ae 0x56314118b3ea 0x5631411f960e 0x5631411f84ae 0x56314118b3ea 0x5631411f960e 0x56314118b30a 0x5631411f960e 0x5631411f84ae 0x56314118b3ea 0x5631411fa32a 0x5631411f84ae 0x56314118b3ea 0x5631411fa32a 0x5631411f84ae 0x56314118ba81\n",
            "\u001b[K     |████████████████████████████████| 1156.8MB 16kB/s \n",
            "\u001b[?25hCollecting torchvision==0.8.2+cu110\n",
            "\u001b[?25l  Downloading https://download.pytorch.org/whl/cu110/torchvision-0.8.2%2Bcu110-cp37-cp37m-linux_x86_64.whl (12.9MB)\n",
            "\u001b[K     |████████████████████████████████| 12.9MB 224kB/s \n",
            "\u001b[?25hCollecting ftfy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ce/b5/5da463f9c7823e0e575e9908d004e2af4b36efa8d02d3d6dad57094fcb11/ftfy-6.0.1.tar.gz (63kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 3.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (2019.12.20)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.7.1+cu110) (3.7.4.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.7.1+cu110) (1.19.5)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.8.2+cu110) (7.1.2)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from ftfy) (0.2.5)\n",
            "Building wheels for collected packages: ftfy\n",
            "  Building wheel for ftfy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ftfy: filename=ftfy-6.0.1-cp37-none-any.whl size=41573 sha256=5fc9dfeb3cd771deb1b243c610c3a1bb5a20d3bd8eb5deafcbf3532a55b001e4\n",
            "  Stored in directory: /root/.cache/pip/wheels/ae/73/c7/9056e14b04919e5c262fe80b54133b1a88d73683d05d7ac65c\n",
            "Successfully built ftfy\n",
            "\u001b[31mERROR: torchtext 0.9.1 has requirement torch==1.8.1, but you'll have torch 1.7.1+cu110 which is incompatible.\u001b[0m\n",
            "Installing collected packages: torch, torchvision, ftfy\n",
            "  Found existing installation: torch 1.8.1+cu101\n",
            "    Uninstalling torch-1.8.1+cu101:\n",
            "      Successfully uninstalled torch-1.8.1+cu101\n",
            "  Found existing installation: torchvision 0.9.1+cu101\n",
            "    Uninstalling torchvision-0.9.1+cu101:\n",
            "      Successfully uninstalled torchvision-0.9.1+cu101\n",
            "Successfully installed ftfy-6.0.1 torch-1.7.1+cu110 torchvision-0.8.2+cu110\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sYwBZS1N6A3d",
        "outputId": "ed0a58c6-9419-48ee-8f45-ebaba0b1218f"
      },
      "source": [
        "! pip install ftfy regex\n",
        "! wget https://openaipublic.azureedge.net/clip/bpe_simple_vocab_16e6.txt.gz -O bpe_simple_vocab_16e6.txt.gz"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.7/dist-packages (6.0.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (2019.12.20)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from ftfy) (0.2.5)\n",
            "--2021-05-19 14:58:31--  https://openaipublic.azureedge.net/clip/bpe_simple_vocab_16e6.txt.gz\n",
            "Resolving openaipublic.azureedge.net (openaipublic.azureedge.net)... 13.107.246.39, 13.107.213.39, 2620:1ec:bdf::39, ...\n",
            "Connecting to openaipublic.azureedge.net (openaipublic.azureedge.net)|13.107.246.39|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1356917 (1.3M) [application/octet-stream]\n",
            "Saving to: ‘bpe_simple_vocab_16e6.txt.gz’\n",
            "\n",
            "bpe_simple_vocab_16 100%[===================>]   1.29M  --.-KB/s    in 0.08s   \n",
            "\n",
            "2021-05-19 14:58:32 (16.2 MB/s) - ‘bpe_simple_vocab_16e6.txt.gz’ saved [1356917/1356917]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9oIcNBYB8lz3",
        "outputId": "c9b23a79-b731-4c69-db5f-e84cb31b5981"
      },
      "source": [
        "!pip install git+https://github.com/Sri-vatsa/CLIP # using this fork because of visualization capabilities"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/Sri-vatsa/CLIP\n",
            "  Cloning https://github.com/Sri-vatsa/CLIP to /tmp/pip-req-build-6cziaoy7\n",
            "  Running command git clone -q https://github.com/Sri-vatsa/CLIP /tmp/pip-req-build-6cziaoy7\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (6.0.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (2019.12.20)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (4.41.1)\n",
            "Requirement already satisfied: torch~=1.7.1 in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (1.7.1+cu110)\n",
            "Requirement already satisfied: torchvision~=0.8.2 in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (0.8.2+cu110)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from ftfy->clip==1.0) (0.2.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch~=1.7.1->clip==1.0) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch~=1.7.1->clip==1.0) (3.7.4.3)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision~=0.8.2->clip==1.0) (7.1.2)\n",
            "Building wheels for collected packages: clip\n",
            "  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for clip: filename=clip-1.0-cp37-none-any.whl size=1368623 sha256=dfe43040f8b0342c66252896fde6037e06235273ba4d65df4f61f7da58774865\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-763anu_h/wheels/cc/55/69/0d411dabbd5009fd069d47b47cf7839c54e595dc61725b307b\n",
            "Successfully built clip\n",
            "Installing collected packages: clip\n",
            "Successfully installed clip-1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OLU-gp7n8__E"
      },
      "source": [
        "## Install clustering dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6TLg9ozo9Hvc"
      },
      "source": [
        "!pip -q install umap-learn>=0.3.7"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9z1WQnXdLHy2"
      },
      "source": [
        "## Install dataset manager dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J1vvMx7_LLSp",
        "outputId": "251f11a2-c5f3-48f2-e26b-767345c19243"
      },
      "source": [
        "!pip install wget"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting wget\n",
            "  Downloading https://files.pythonhosted.org/packages/47/6a/62e288da7bcda82b935ff0c6cfe542970f04e29c756b0e147251b2fb251f/wget-3.2.zip\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-cp37-none-any.whl size=9681 sha256=2c299af5eacf2bd19e58e7a4c269c3f15b0bf775893973e6aa6676931dfc1962\n",
            "  Stored in directory: /root/.cache/pip/wheels/40/15/30/7d8f7cea2902b4db79e3fea550d7d7b85ecb27ef992b618f3f\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NzsubsEm72rr"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KZI62a6G74kw"
      },
      "source": [
        "# ML Libraries\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "import keras\n",
        "\n",
        "# Data processing\n",
        "import PIL\n",
        "import base64\n",
        "import imageio\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "\n",
        "from PIL import Image\n",
        "import cv2\n",
        "import imgaug.augmenters as iaa\n",
        "\n",
        "# Plotting\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from IPython.core.display import display, HTML\n",
        "from matplotlib import cm\n",
        "\n",
        "# Models\n",
        "import clip\n",
        "\n",
        "# Datasets\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "# Misc\n",
        "import progressbar\n",
        "import logging\n",
        "from abc import ABC, abstractmethod\n",
        "import time\n",
        "import urllib.request\n",
        "import os\n",
        "import itertools\n",
        "\n",
        "# Modules\n",
        "from CLIPPER.code.ExperimentModules import embedding_models\n",
        "from CLIPPER.code.ExperimentModules import simclr_data_augmentations\n",
        "from CLIPPER.code.ExperimentModules.dataset_manager import DatasetManager\n",
        "from CLIPPER.code.ExperimentModules.weight_imprinting_classifier import WeightImprintingClassifier\n",
        "from CLIPPER.code.ExperimentModules.utils import (save_npy, load_npy, \n",
        "                                                       get_folder_id, \n",
        "                                                       create_expt_dir, \n",
        "                                                       save_to_drive, \n",
        "                                                       load_all_from_drive_folder, \n",
        "                                                       download_file_by_name, \n",
        "                                                       delete_file_by_name)\n",
        "\n",
        "logging.getLogger('googleapicliet.discovery_cache').setLevel(logging.ERROR)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zU_gxQ0KbwMh"
      },
      "source": [
        "# Initialization & Constants\n",
        "\n",
        "**Edited**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bih5tBPdbx3u",
        "outputId": "ef46ccf8-4a63-4a59-9acf-39c00e016fb9"
      },
      "source": [
        "dataset_name = 'UCF101'\n",
        "folder_name = \"UCF101-Embeddings-28-02-21\"\n",
        "\n",
        "# Change parentid to match that of experiments root folder in gdrive\n",
        "parentid = '1bK72W-Um20EQDEyChNhNJthUNbmoSEjD'\n",
        "\n",
        "# Filepaths\n",
        "train_labels_filename = \"train_labels.npz\"\n",
        "val_labels_filename = \"val_labels.npz\"\n",
        "\n",
        "train_embeddings_filename_suffix = \"_embeddings_train.npz\"\n",
        "val_embeddings_filename_suffix = \"_embeddings_val.npz\"\n",
        "\n",
        "\n",
        "# Initialize sepcific experiment folder in drive\n",
        "folderid = create_expt_dir(drive, parentid, folder_name)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "title: UCF101-Embeddings-28-02-21, id: 1AR8qp1QPFv0M3jCIQKK8VA2xsNilo_pi\n",
            "Experiment folder already exists. WARNING: Following with this run might overwrite existing results stored.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UxTa8MVsvQCN"
      },
      "source": [
        "# Load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n6S124Jfwuu5",
        "outputId": "47586fd9-5ec7-4d6b-8575-60e19e3a4e52"
      },
      "source": [
        "def get_ndarray_from_drive(drive, folderid, filename):\n",
        "    download_file_by_name(drive, folderid, filename)\n",
        "    return np.load(filename)['data']\n",
        "\n",
        "# train_labels = get_ndarray_from_drive(drive, folderid, train_labels_filename)\n",
        "val_labels = get_ndarray_from_drive(drive, folderid, val_labels_filename)\n",
        "# test_labels = get_ndarray_from_drive(drive, folderid, test_labels_filename)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading val_labels.npz from GDrive\n",
            "Downloading val_labels.npz from GDrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4p2JpaGHmqbl",
        "outputId": "d3098d9b-ed1c-4288-b076-3dd70509a2a8"
      },
      "source": [
        "dm = DatasetManager()\n",
        "\n",
        "test_data_generator = dm.load_dataset('ucf_101', split='val')\n",
        "\n",
        "class_names = dm.get_class_names()\n",
        "print(class_names)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1NwERNU-266cCK_uofbEXraArleazADRS\n",
            "To: /content/UCF_Images.zip\n",
            "218MB [00:03, 62.7MB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Found 13320 files belonging to 101 classes.\n",
            "Using 2664 files for validation.\n",
            "['Apply Eye Makeup', 'Apply Lipstick', 'Archery', 'Baby Crawling', 'Balance Beam', 'Band Marching', 'Baseball Pitch', 'Basketball', 'Basketball Dunk', 'Bench Press', 'Biking', 'Billiards', 'Blow Dry Hair', 'Blowing Candles', 'Body Weight Squats', 'Bowling', 'Boxing Punching Bag', 'Boxing Speed Bag', 'Breast Stroke', 'Brushing Teeth', 'Clean And Jerk', 'Cliff Diving', 'Cricket Bowling', 'Cricket Shot', 'Cutting In Kitchen', 'Diving', 'Drumming', 'Fencing', 'Field Hockey Penalty', 'Floor Gymnastics', 'Frisbee Catch', 'Front Crawl', 'Golf Swing', 'Haircut', 'Hammer Throw', 'Hammering', 'Handstand Pushups', 'Handstand Walking', 'Head Massage', 'High Jump', 'Horse Race', 'Horse Riding', 'Hula Hoop', 'Ice Dancing', 'Javelin Throw', 'Juggling Balls', 'Jump Rope', 'Jumping Jack', 'Kayaking', 'Knitting', 'Long Jump', 'Lunges', 'Military Parade', 'Mixing', 'Mopping Floor', 'Nunchucks', 'Parallel Bars', 'Pizza Tossing', 'Playing Cello', 'Playing Daf', 'Playing Dhol', 'Playing Flute', 'Playing Guitar', 'Playing Piano', 'Playing Sitar', 'Playing Tabla', 'Playing Violin', 'Pole Vault', 'Pommel Horse', 'Pull Ups', 'Punch', 'Push Ups', 'Rafting', 'Rock Climbing Indoor', 'Rope Climbing', 'Rowing', 'Salsa Spin', 'Shaving Beard', 'Shotput', 'Skate Boarding', 'Skiing', 'Skijet', 'Sky Diving', 'Soccer Juggling', 'Soccer Penalty', 'Still Rings', 'Sumo Wrestling', 'Surfing', 'Swing', 'Table Tennis Shot', 'Tai Chi', 'Tennis Swing', 'Throw Discus', 'Trampoline Jumping', 'Typing', 'Uneven Bars', 'Volleyball Spiking', 'Walking With Dog', 'Wall Pushups', 'Writing On Board', 'Yo Yo']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iLbRqaYxzbr7"
      },
      "source": [
        "# Create label dictionary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Emz85fNX0Vif",
        "outputId": "5439968f-6c1d-439b-8e61-0ae459d7d811"
      },
      "source": [
        "unique_labels = np.unique(val_labels)\n",
        "print(len(unique_labels))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "22\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0TubS-RLzeVM"
      },
      "source": [
        "label_dictionary = {la:[] for la in unique_labels}\n",
        "\n",
        "for i in range(len(val_labels)):\n",
        "    la = val_labels[i]\n",
        "\n",
        "    label_dictionary[la].append(i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xP6ftkDCvKul"
      },
      "source": [
        "# CLIP Linear Probe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJIGo_R86GQi"
      },
      "source": [
        "## Function definitions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pv68q8TH6tvG"
      },
      "source": [
        "def start_progress_bar(bar_len):\n",
        "    widgets = [\n",
        "        ' [', \n",
        "        progressbar.Timer(format= 'elapsed time: %(elapsed)s'), \n",
        "        '] ', \n",
        "        progressbar.Bar('*'),' (', \n",
        "        progressbar.ETA(), ') ', \n",
        "        ]\n",
        "    pbar = progressbar.ProgressBar(\n",
        "        max_value=bar_len, widgets=widgets\n",
        "        ).start()\n",
        "\n",
        "    return pbar"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DtoopzLRStTO"
      },
      "source": [
        "def prepare_indices(\n",
        "    num_ways,\n",
        "    num_shot,\n",
        "    num_eval,\n",
        "    num_episodes,\n",
        "    label_dictionary,\n",
        "    labels,\n",
        "    shuffle=False\n",
        "):\n",
        "    eval_indices = []\n",
        "    train_indices = []\n",
        "    wi_y = []\n",
        "    eval_y = []\n",
        "\n",
        "    label_dictionary = {la:label_dictionary[la] for la in label_dictionary if len(label_dictionary[la]) >= (num_shot+num_eval)}\n",
        "    unique_labels = list(label_dictionary.keys())\n",
        "\n",
        "    pbar = start_progress_bar(num_episodes)\n",
        "\n",
        "    for s in range(num_episodes):\n",
        "        # Setting random seed for replicability\n",
        "        np.random.seed(s)\n",
        "\n",
        "        _train_indices = []\n",
        "        _eval_indices = []\n",
        "\n",
        "        selected_labels = np.random.choice(unique_labels, size=num_ways, replace=False)\n",
        "        for la in selected_labels:\n",
        "            la_indices = label_dictionary[la]\n",
        "            select = np.random.choice(la_indices, size = num_shot+num_eval, replace=False)\n",
        "            tr_idx = list(select[:num_shot])\n",
        "            ev_idx = list(select[num_shot:])\n",
        "\n",
        "            _train_indices = _train_indices + tr_idx\n",
        "            _eval_indices = _eval_indices + ev_idx\n",
        "\n",
        "        if shuffle:\n",
        "            np.random.shuffle(_train_indices)\n",
        "            np.random.shuffle(_eval_indices)\n",
        "\n",
        "        train_indices.append(_train_indices)\n",
        "        eval_indices.append(_eval_indices)\n",
        "\n",
        "        _wi_y = labels[_train_indices]\n",
        "        _eval_y = labels[_eval_indices]\n",
        "\n",
        "        wi_y.append(_wi_y)\n",
        "        eval_y.append(_eval_y)\n",
        "\n",
        "        pbar.update(s+1)\n",
        "        \n",
        "    return train_indices, eval_indices, wi_y, eval_y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4M5aHABX7k8a"
      },
      "source": [
        "def embed_images(\n",
        "    embedding_model, \n",
        "    train_indices, \n",
        "    num_augmentations,\n",
        "    trivial=False\n",
        "    ):\n",
        "\n",
        "    def augment_image(image, num_augmentations, trivial):\n",
        "        \"\"\" Perform SimCLR augmentations on the image\n",
        "        \"\"\"\n",
        "\n",
        "        if np.max(image) > 1:\n",
        "            image = image/255\n",
        "\n",
        "        augmented_images = [image]\n",
        "\n",
        "        # augmentations = iaa.Sequential([\n",
        "        #     iaa.Affine(\n",
        "        #         translate_percent={'x':(-0.1, 0.1), 'y':(-0.1, 0.1)}, \n",
        "        #         rotate=(-15, 15),\n",
        "        #         shear=(-15, 15),\n",
        "        #     ),\n",
        "        #     iaa.Fliplr(0.5) \n",
        "        # ])\n",
        "\n",
        "        def _run_filters(image):\n",
        "            width = image.shape[1]\n",
        "            height = image.shape[0]\n",
        "            image_aug = simclr_data_augmentations.random_crop_with_resize(\n",
        "                image,\n",
        "                height,\n",
        "                width\n",
        "                )\n",
        "            image_aug = tf.image.random_flip_left_right(image_aug)\n",
        "            image_aug = simclr_data_augmentations.random_color_jitter(image_aug)\n",
        "            image_aug = simclr_data_augmentations.random_blur(\n",
        "                image_aug,\n",
        "                height,\n",
        "                width\n",
        "                )\n",
        "            image_aug = tf.reshape(image_aug, [image.shape[0], image.shape[1], 3])\n",
        "            image_aug = tf.clip_by_value(image_aug, 0., 1.)\n",
        "\n",
        "            return image_aug.numpy()\n",
        "\n",
        "        for _ in range(num_augmentations):\n",
        "            # aug_image = augmentations(image=image)\n",
        "\n",
        "            if trivial:\n",
        "                aug_image = image\n",
        "            else:\n",
        "                aug_image = _run_filters(image)\n",
        "            augmented_images.append(aug_image)\n",
        "\n",
        "        augmented_images = np.stack(augmented_images)\n",
        "        return augmented_images\n",
        "\n",
        "    embedding_model.load_model()\n",
        "\n",
        "    unique_indices = np.unique(np.array(train_indices))\n",
        "\n",
        "    ds = dm.load_dataset('ucf_101', split='val')\n",
        "    embeddings = []\n",
        "    IMAGE_IDX = 0\n",
        "    pbar = start_progress_bar(unique_indices.size+1)\n",
        "    num_done=0\n",
        "    for idx, item in enumerate(ds):\n",
        "        if idx in unique_indices:\n",
        "            image = item[IMAGE_IDX][0]\n",
        "            if num_augmentations > 0:\n",
        "                aug_images = augment_image(image, num_augmentations, trivial)\n",
        "            else:\n",
        "                aug_images = image\n",
        "            \n",
        "            processed_images = embedding_model.preprocess_data(aug_images)\n",
        "            embedding = embedding_model.embed_images(processed_images)\n",
        "            embeddings.append(embedding)\n",
        "            \n",
        "            num_done += 1\n",
        "            pbar.update(num_done+1)\n",
        "\n",
        "        if idx == unique_indices[-1]:\n",
        "            break\n",
        "\n",
        "    embeddings = np.stack(embeddings)\n",
        "\n",
        "    return unique_indices, embeddings"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r3DO9vntM6sV"
      },
      "source": [
        "def train_model_for_episode(\n",
        "    indices_and_embeddings,\n",
        "    train_indices,\n",
        "    wi_y,\n",
        "    num_augmentations,\n",
        "    train_epochs=None,\n",
        "    train_batch_size=5,\n",
        "    multi_label=True\n",
        "):\n",
        "\n",
        "    train_embeddings = []\n",
        "    train_labels = []\n",
        "    ind = indices_and_embeddings[0]\n",
        "    emb = indices_and_embeddings[1]\n",
        "\n",
        "    for idx, tr_idx in enumerate(train_indices):\n",
        "        train_embeddings.append(emb[np.argwhere(ind==tr_idx)[0][0]])\n",
        "        train_labels += [wi_y[idx] for _ in range(num_augmentations+1)]\n",
        "    train_embeddings = np.concatenate(train_embeddings)\n",
        "\n",
        "    train_embeddings = WeightImprintingClassifier.preprocess_input(train_embeddings)\n",
        "\n",
        "    wi_weights, label_mapping = WeightImprintingClassifier.get_imprinting_weights(\n",
        "        train_embeddings, train_labels, False, multi_label\n",
        "        )\n",
        "\n",
        "    wi_parameters = {\n",
        "        \"num_classes\": num_ways,\n",
        "        \"input_dims\": train_embeddings.shape[-1],\n",
        "        \"scale\": False,\n",
        "        \"kaiming_init\": None,\n",
        "        \"multi_label\": multi_label\n",
        "    }\n",
        "\n",
        "    wi_cls = WeightImprintingClassifier(wi_parameters)\n",
        "\n",
        "    if train_epochs:\n",
        "        # ep_y = train_labels\n",
        "        rev_label_mapping = {label_mapping[val]:val for val in label_mapping}\n",
        "        train_y = np.zeros((len(train_labels), num_ways))\n",
        "        for idx_y, l in enumerate(train_labels):\n",
        "            if multi_label:\n",
        "                for _l in l:\n",
        "                    train_y[idx_y, rev_label_mapping[_l]] = 1\n",
        "            else:\n",
        "                train_y[idx_y, rev_label_mapping[l]] = 1\n",
        "\n",
        "        wi_cls.train(train_embeddings, train_y, train_epochs, train_batch_size)\n",
        "\n",
        "    return wi_cls, label_mapping"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VrYMC-cTM6sW"
      },
      "source": [
        "# chenni change\n",
        "def evaluate_model_for_episode(\n",
        "    model,\n",
        "    eval_x,\n",
        "    eval_y,\n",
        "    label_mapping,\n",
        "    metrics=['hamming', 'jaccard', 'subset_accuracy', 'ap', 'map', 'c_f1', 'o_f1', 'c_precision', 'o_precision', 'c_recall', 'o_recall', 'top1_accuracy', 'top5_accuracy', 'classwise_accuracy', 'c_accuracy'],\n",
        "    threshold=0.7,\n",
        "    multi_label=True\n",
        "):\n",
        "    eval_x = WeightImprintingClassifier.preprocess_input(eval_x)\n",
        "    #cc\n",
        "    logits = model.predict_scores(eval_x).tolist()\n",
        "\n",
        "    if multi_label:\n",
        "        pred_y = model.predict_multi_label(eval_x, threshold)\n",
        "        pred_y = [[label_mapping[v] for v in l] for l in pred_y]\n",
        "\n",
        "        met = model.evaluate_multi_label_metrics(\n",
        "            eval_x, eval_y, label_mapping, threshold, metrics\n",
        "            )\n",
        "    else:\n",
        "        pred_y = model.predict_single_label(eval_x)\n",
        "        pred_y = [label_mapping[l] for l in pred_y]\n",
        "\n",
        "        met = model.evaluate_single_label_metrics(\n",
        "            eval_x, eval_y, label_mapping, metrics\n",
        "            )\n",
        "#cc\n",
        "    return pred_y, met, logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fmp4SLVkM6sW"
      },
      "source": [
        "# chenni change\n",
        "def run_episode_through_model(\n",
        "    indices_and_embeddings,\n",
        "    train_indices, \n",
        "    eval_indices, \n",
        "    wi_y, \n",
        "    eval_y,\n",
        "    thresholds=None,\n",
        "    num_augmentations=0,\n",
        "    train_epochs=None,\n",
        "    train_batch_size=5,\n",
        "    metrics=['hamming', 'jaccard', 'subset_accuracy', 'ap', 'map', 'c_f1', 'o_f1', 'c_precision', 'o_precision', 'c_recall', 'o_recall', 'top1_accuracy', 'top5_accuracy', 'classwise_accuracy', 'c_accuracy'],\n",
        "    embeddings=None,\n",
        "    multi_label=True\n",
        "):\n",
        "    metrics_values = {m:[] for m in metrics}\n",
        "\n",
        "    wi_cls, label_mapping = train_model_for_episode(\n",
        "        indices_and_embeddings,\n",
        "        train_indices,\n",
        "        wi_y,\n",
        "        num_augmentations,\n",
        "        train_epochs,\n",
        "        train_batch_size,\n",
        "        multi_label=multi_label\n",
        "    )\n",
        "\n",
        "    # if embeddings is None:\n",
        "    #     eval_x = []\n",
        "    #     IMAGE_IDX=0\n",
        "    #     for ev_idx in eval_indices:\n",
        "    #         image = get_item_from_dataset(ev_idx)[IMAGE_IDX]\n",
        "    #         processed_image = embedding_model.preprocess_data(image)\n",
        "    #         embedding = embedding_model.embed_images(processed_image)\n",
        "    #         eval_x.append(embedding)\n",
        "    #     eval_x = np.concatenate(eval_x)\n",
        "    # else:\n",
        "    eval_x = embeddings[eval_indices]\n",
        "    #cc\n",
        "    ep_logits = []\n",
        "\n",
        "    if multi_label:\n",
        "        for t in thresholds:\n",
        "          #cc\n",
        "            pred_labels, met, logits = evaluate_model_for_episode(\n",
        "                wi_cls,\n",
        "                eval_x,\n",
        "                eval_y,\n",
        "                label_mapping,\n",
        "                threshold=t,\n",
        "                metrics=metrics,\n",
        "                multi_label=True\n",
        "            )\n",
        "            #cc\n",
        "            ep_logits.append(logits)\n",
        "            for m in metrics:\n",
        "                metrics_values[m].append(met[m])\n",
        "    else:\n",
        "      #cc\n",
        "        pred_labels, metrics_values, logits = evaluate_model_for_episode(\n",
        "            wi_cls,\n",
        "            eval_x,\n",
        "            eval_y,\n",
        "            label_mapping,\n",
        "            metrics=metrics,\n",
        "            multi_label=False\n",
        "        )\n",
        "        #cc\n",
        "        ep_logits = logits\n",
        "#cc\n",
        "    return metrics_values, ep_logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iPyO6W5SM6sX"
      },
      "source": [
        "# chenni change\n",
        "def run_evaluations(\n",
        "    indices_and_embeddings,\n",
        "    train_indices, \n",
        "    eval_indices, \n",
        "    wi_y, \n",
        "    eval_y, \n",
        "    num_episodes, \n",
        "    num_ways,\n",
        "    thresholds,\n",
        "    verbose=True,\n",
        "    normalize=True,\n",
        "    train_epochs=None,\n",
        "    train_batch_size=5,\n",
        "    metrics=['accuracy', 'ap', 'map', 'c_f1', 'o_f1', 'c_precision', 'o_precision', 'c_recall', 'o_recall', 'top1_accuracy', 'top5_accuracy', 'classwise_accuracy', 'c_accuracy'],\n",
        "    embeddings=None,\n",
        "    num_augmentations=0,\n",
        "    multi_label=True\n",
        "):\n",
        "    metrics_values = {m:[] for m in metrics}\n",
        "\n",
        "#cc\n",
        "    all_logits = []\n",
        "\n",
        "    if verbose:\n",
        "        pbar = start_progress_bar(num_episodes)\n",
        "\n",
        "    for idx_ep in range(num_episodes):\n",
        "        _train_indices = train_indices[idx_ep]\n",
        "        _eval_indices = eval_indices[idx_ep]\n",
        "        _wi_y = [label for label in wi_y[idx_ep]]\n",
        "        _eval_y = [label for label in eval_y[idx_ep]]\n",
        "\n",
        "#cc\n",
        "        met, ep_logits = run_episode_through_model(\n",
        "            indices_and_embeddings,\n",
        "            _train_indices, \n",
        "            _eval_indices, \n",
        "            _wi_y, \n",
        "            _eval_y,\n",
        "            num_augmentations=num_augmentations,\n",
        "            train_epochs=train_epochs,\n",
        "            train_batch_size=train_batch_size,\n",
        "            embeddings=embeddings,\n",
        "            thresholds=thresholds,\n",
        "            metrics=metrics,\n",
        "            multi_label=multi_label\n",
        "        )\n",
        "\n",
        "#cc\n",
        "        all_logits.append(ep_logits)\n",
        "\n",
        "        for m in metrics:\n",
        "            metrics_values[m].append(met[m])\n",
        "\n",
        "        if verbose:\n",
        "            pbar.update(idx_ep+1)\n",
        "#cc\n",
        "    return metrics_values, all_logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gkIveK4s7zQX"
      },
      "source": [
        "# def run_train_loop(\n",
        "#     embeddings, \n",
        "#     train_indices, \n",
        "#     eval_indices, \n",
        "#     wi_y, \n",
        "#     eval_y, \n",
        "#     num_episodes, \n",
        "#     num_ways,\n",
        "#     verbose=True,\n",
        "#     normalize=True,\n",
        "#     train_epochs_loop=[5],\n",
        "#     train_batch_size=5,\n",
        "#     metrics=[\"hamming\", \"jaccard\", \"f1_score\"],\n",
        "#     threshold=0.72\n",
        "# ):\n",
        "#     metrics_values = [{m:[] for m in metrics} for _ in range(len(train_epochs_loop)+1)]\n",
        "\n",
        "#     if verbose:\n",
        "#         pbar = start_progress_bar(num_episodes)\n",
        "\n",
        "#     for idx_ep in range(num_episodes):\n",
        "#         wi_x = embeddings[train_indices[idx_ep]]\n",
        "#         eval_x = embeddings[eval_indices[idx_ep]]\n",
        "\n",
        "#         if normalize:\n",
        "#             wi_x = WeightImprintingClassifier.preprocess_input(wi_x)\n",
        "#             eval_x = WeightImprintingClassifier.preprocess_input(eval_x)\n",
        "\n",
        "#         wi_weights, label_mapping = WeightImprintingClassifier.get_imprinting_weights(\n",
        "#             wi_x, wi_y[idx_ep], False, True\n",
        "#             )\n",
        "\n",
        "#         wi_parameters = {\n",
        "#             \"num_classes\": num_ways,\n",
        "#             \"input_dims\": wi_x.shape[-1],\n",
        "#             \"scale\": False,\n",
        "#             \"dense_layer_weights\": wi_weights,\n",
        "#             \"multi_label\": True\n",
        "#         }\n",
        "\n",
        "#         wi_cls = WeightImprintingClassifier(wi_parameters)\n",
        "\n",
        "#         met = wi_cls.evaluate_multi_label_metrics(\n",
        "#             eval_x, eval_y[idx_ep], label_mapping, threshold, metrics\n",
        "#             )\n",
        "#         for m in met:\n",
        "#             metrics_values[0][m].append(met[m])\n",
        "        \n",
        "\n",
        "#         for idx_tr_ep in range(len(train_epochs_loop)):\n",
        "#             ep_y = wi_y[idx_ep]\n",
        "#             rev_label_mapping = {label_mapping[val]:val for val in label_mapping}\n",
        "#             train_y = np.zeros((len(ep_y), num_ways))\n",
        "#             for idx_y, _y in enumerate(ep_y):\n",
        "#                 for l in _y:\n",
        "#                     train_y[idx_y, rev_label_mapping[l]] = 1\n",
        "\n",
        "\n",
        "#             wi_cls.train(wi_x, train_y, train_epochs_loop[idx_tr_ep], train_batch_size)\n",
        "\n",
        "#             met = wi_cls.evaluate_multi_label_metrics(\n",
        "#                 eval_x, eval_y[idx_ep], label_mapping, threshold, metrics\n",
        "#                 )\n",
        "#             for m in met:\n",
        "#                 metrics_values[idx_tr_ep+1][m].append(met[m])\n",
        "\n",
        "\n",
        "#         # _pred_y = wi_cls.predict_multi_label(eval_x, threshold)\n",
        "#         # for j in range(len(_eval_y)):\n",
        "#         #     print([label_mapping[p] for p in _pred_y[j]], \" vs \", _eval_y[j])\n",
        "\n",
        "#         # met = wi_cls.evaluate_multi_label_metrics(\n",
        "#         #     eval_x, eval_y[idx_ep], label_mapping, threshold, metrics\n",
        "#         #     )\n",
        "        \n",
        "#         # for m in met:\n",
        "#         #     metrics_values[m].append(met[m])\n",
        "\n",
        "#         del wi_x\n",
        "#         del eval_x\n",
        "#         del wi_cls\n",
        "\n",
        "#         if verbose:\n",
        "#             pbar.update(idx_ep+1)\n",
        "\n",
        "#     return metrics_values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MzmivM_CtSbA"
      },
      "source": [
        "def get_max_mean_jaccard_index_by_threshold(metrics_thresholds):\n",
        "    max_mean_jaccard = np.max([np.mean(mt['jaccard']) for mt in metrics_thresholds])\n",
        "    return max_mean_jaccard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UtLfeX4w8tEZ"
      },
      "source": [
        "def get_max_mean_jaccard_index_with_threshold(metrics_thresholds):\n",
        "    # max_mean_jaccard = np.max([np.mean(mt['jaccard']) for mt in metrics_thresholds])\n",
        "    # threshold = np.argmax([np.mean(mt['jaccard']) for mt in metrics_thresholds])\n",
        "    \n",
        "    arr = np.array(metrics_thresholds['jaccard'])\n",
        "    max_mean_jaccard = np.max(np.mean(arr, 0))\n",
        "    threshold = np.argmax(np.mean(arr, 0))\n",
        "    return max_mean_jaccard, threshold\n",
        "\n",
        "def get_max_mean_f1_score_with_threshold(metrics_thresholds):\n",
        "    # max_mean_jaccard = np.max([np.mean(mt['f1_score']) for mt in metrics_thresholds])\n",
        "    # threshold = np.argmax([np.mean(mt['f1_score']) for mt in metrics_thresholds])\n",
        "\n",
        "    arr = np.array(metrics_thresholds['f1_score'])\n",
        "    max_mean_jaccard = np.max(np.mean(arr, 0))\n",
        "    threshold = np.argmax(np.mean(arr, 0))\n",
        "    return max_mean_jaccard, threshold"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wsKTx1h6tSbL"
      },
      "source": [
        "def get_mean_max_jaccard_index_by_episode(metrics_thresholds):\n",
        "    mean_max_jaccard = np.mean(np.max(np.array([mt['jaccard'] for mt in metrics_thresholds]), axis=0))\n",
        "    return mean_max_jaccard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y1mgLWl5N0hY"
      },
      "source": [
        "def plot_metrics_by_threshold(\n",
        "    metrics_thresholds, \n",
        "    thresholds, \n",
        "    metrics=['hamming', 'jaccard', 'subset_accuracy', 'ap', 'map', 'c_f1', 'o_f1', 'c_precision', 'o_precision', 'c_recall', 'o_recall', 'top1_accuracy', 'top5_accuracy', 'classwise_accuracy', 'c_accuracy'],\n",
        "    title_suffix=\"\"\n",
        "):\n",
        "    legend = []\n",
        "\n",
        "    fig = plt.figure(figsize=(10,10))\n",
        "    if 'jaccard' in metrics:\n",
        "        mean_jaccard_threshold = np.mean(np.array(metrics_thresholds['jaccard']), axis=0)\n",
        "        opt_threshold_jaccard = thresholds[np.argmax(mean_jaccard_threshold)]\n",
        "        plt.plot(thresholds, mean_jaccard_threshold, c='blue')\n",
        "        plt.axvline(opt_threshold_jaccard, ls=\"--\", c='blue')\n",
        "        legend.append(\"Jaccard Index\")\n",
        "        legend.append(opt_threshold_jaccard)\n",
        "    if 'hamming' in metrics:\n",
        "        mean_hamming_threshold = np.mean(np.array(metrics_thresholds['hamming']), axis=0)\n",
        "        opt_threshold_hamming = thresholds[np.argmin(mean_hamming_threshold)]\n",
        "        plt.plot(thresholds, mean_hamming_threshold, c='green')\n",
        "        plt.axvline(opt_threshold_hamming, ls=\"--\", c='green')\n",
        "        legend.append(\"Hamming Score\")\n",
        "        legend.append(opt_threshold_hamming)\n",
        "    if 'map' in metrics:\n",
        "        mean_f1_score_threshold = np.mean(np.array(metrics_thresholds['map']), axis=0)\n",
        "        opt_threshold_f1_score = thresholds[np.argmax(mean_f1_score_threshold)]\n",
        "        plt.plot(thresholds, mean_f1_score_threshold, c='red')\n",
        "        plt.axvline(opt_threshold_f1_score, ls=\"--\", c='red')\n",
        "        legend.append(\"mAP\")\n",
        "        legend.append(opt_threshold_f1_score)\n",
        "    if 'o_f1' in metrics:\n",
        "        mean_f1_score_threshold = np.mean(np.array(metrics_thresholds['o_f1']), axis=0)\n",
        "        opt_threshold_f1_score = thresholds[np.argmax(mean_f1_score_threshold)]\n",
        "        plt.plot(thresholds, mean_f1_score_threshold, c='yellow')\n",
        "        plt.axvline(opt_threshold_f1_score, ls=\"--\", c='yellow')\n",
        "        legend.append(\"OF1\")\n",
        "        legend.append(opt_threshold_f1_score)\n",
        "    if 'c_f1' in metrics:\n",
        "        mean_f1_score_threshold = np.mean(np.array(metrics_thresholds['c_f1']), axis=0)\n",
        "        opt_threshold_f1_score = thresholds[np.argmax(mean_f1_score_threshold)]\n",
        "        plt.plot(thresholds, mean_f1_score_threshold, c='orange')\n",
        "        plt.axvline(opt_threshold_f1_score, ls=\"--\", c='orange')\n",
        "        legend.append(\"CF1\")\n",
        "        legend.append(opt_threshold_f1_score)\n",
        "    if 'o_precision' in metrics:\n",
        "        mean_f1_score_threshold = np.mean(np.array(metrics_thresholds['o_precision']), axis=0)\n",
        "        opt_threshold_f1_score = thresholds[np.argmax(mean_f1_score_threshold)]\n",
        "        plt.plot(thresholds, mean_f1_score_threshold, c='purple')\n",
        "        plt.axvline(opt_threshold_f1_score, ls=\"--\", c='purple')\n",
        "        legend.append(\"OP\")\n",
        "        legend.append(opt_threshold_f1_score)\n",
        "    if 'c_precision' in metrics:\n",
        "        mean_f1_score_threshold = np.mean(np.array(metrics_thresholds['c_precision']), axis=0)\n",
        "        opt_threshold_f1_score = thresholds[np.argmax(mean_f1_score_threshold)]\n",
        "        plt.plot(thresholds, mean_f1_score_threshold, c='cyan')\n",
        "        plt.axvline(opt_threshold_f1_score, ls=\"--\", c='cyan')\n",
        "        legend.append(\"CP\")\n",
        "        legend.append(opt_threshold_f1_score)\n",
        "    if 'o_recall' in metrics:\n",
        "        mean_f1_score_threshold = np.mean(np.array(metrics_thresholds['o_recall']), axis=0)\n",
        "        opt_threshold_f1_score = thresholds[np.argmin(mean_f1_score_threshold)]\n",
        "        plt.plot(thresholds, mean_f1_score_threshold, c='brown')\n",
        "        plt.axvline(opt_threshold_f1_score, ls=\"--\", c='brown')\n",
        "        legend.append(\"OR\")\n",
        "        legend.append(opt_threshold_f1_score)\n",
        "    if 'c_recall' in metrics:\n",
        "        mean_f1_score_threshold = np.mean(np.array(metrics_thresholds['c_recall']), axis=0)\n",
        "        opt_threshold_f1_score = thresholds[np.argmin(mean_f1_score_threshold)]\n",
        "        plt.plot(thresholds, mean_f1_score_threshold, c='pink')\n",
        "        plt.axvline(opt_threshold_f1_score, ls=\"--\", c='pink')\n",
        "        legend.append(\"CR\")\n",
        "        legend.append(opt_threshold_f1_score)\n",
        "    if 'c_accuracy' in metrics:\n",
        "        mean_f1_score_threshold = np.mean(np.array(metrics_thresholds['c_accuracy']), axis=0)\n",
        "        opt_threshold_f1_score = thresholds[np.argmax(mean_f1_score_threshold)]\n",
        "        plt.plot(thresholds, mean_f1_score_threshold, c='maroon')\n",
        "        plt.axvline(opt_threshold_f1_score, ls=\"--\", c='maroon')\n",
        "        legend.append(\"CACC\")\n",
        "        legend.append(opt_threshold_f1_score)\n",
        "    if 'top1_accuracy' in metrics:\n",
        "        mean_f1_score_threshold = np.mean(np.array(metrics_thresholds['top1_accuracy']), axis=0)\n",
        "        opt_threshold_f1_score = thresholds[np.argmax(mean_f1_score_threshold)]\n",
        "        plt.plot(thresholds, mean_f1_score_threshold, c='magenta')\n",
        "        plt.axvline(opt_threshold_f1_score, ls=\"--\", c='magenta')\n",
        "        legend.append(\"TOP1\")\n",
        "        legend.append(opt_threshold_f1_score)\n",
        "    if 'top5_accuracy' in metrics:\n",
        "        mean_f1_score_threshold = np.mean(np.array(metrics_thresholds['top5_accuracy']), axis=0)\n",
        "        opt_threshold_f1_score = thresholds[np.argmax(mean_f1_score_threshold)]\n",
        "        plt.plot(thresholds, mean_f1_score_threshold, c='slategray')\n",
        "        plt.axvline(opt_threshold_f1_score, ls=\"--\", c='slategray')\n",
        "        legend.append(\"TOP5\")\n",
        "        legend.append(opt_threshold_f1_score)\n",
        "    \n",
        "    plt.xlabel('Threshold')\n",
        "    plt.ylabel('Value')\n",
        "    plt.legend(legend)\n",
        "    title = title_suffix+\"\\nMulti label metrics by threshold\"\n",
        "    plt.title(title)\n",
        "    plt.grid()\n",
        "\n",
        "    fname = os.path.join(PLOT_DIR, title_suffix)\n",
        "    plt.savefig(fname)\n",
        "    \n",
        "    plt.show()\n",
        "   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hHs46yaOItTW"
      },
      "source": [
        "## Setting multiple thresholds"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vlo2bBCAIyQj",
        "outputId": "198e7e68-0898-42fc-ac47-59dcbef6d954"
      },
      "source": [
        "thresholds = np.arange(0.43, 0.74, 0.01)\n",
        "# thresholds = np.arange(0.3, 0.72, 0.05)\n",
        "thresholds"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.5 , 0.51, 0.52, 0.53,\n",
              "       0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.6 , 0.61, 0.62, 0.63, 0.64,\n",
              "       0.65, 0.66, 0.67, 0.68, 0.69, 0.7 , 0.71, 0.72, 0.73])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g8CNr6I4rU5d"
      },
      "source": [
        "# 5 way 5 shot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g73bz0lqrU5e"
      },
      "source": [
        "## Picking indices"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fm6j-e0brU5f",
        "outputId": "523046b4-8c6b-41bd-a746-7944da861ee7"
      },
      "source": [
        "num_ways = 5\n",
        "num_shot = 5\n",
        "num_eval = 15\n",
        "shuffle = False\n",
        "num_episodes = 100\n",
        "\n",
        "train_indices, eval_indices, wi_y, eval_y = prepare_indices(\n",
        "    num_ways, num_shot, num_eval, num_episodes, label_dictionary, val_labels, shuffle\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " [elapsed time: 0:00:00] |**************************        | (ETA:   0:00:00) "
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KgeLYlcPQVrP",
        "outputId": "8d58f1f5-262b-4ded-f29e-c8a7d35a2a64"
      },
      "source": [
        "embedding_model = embedding_models.CLIPEmbeddingWrapper()\n",
        "num_augmentations = 0\n",
        "trivial=False\n",
        "\n",
        "indices, embeddings = embed_images(\n",
        "    embedding_model, \n",
        "    train_indices, \n",
        "    num_augmentations,\n",
        "    trivial=trivial\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r                                                                               \r\r [elapsed time: 0:00:00] |                                  | (ETA:  --:--:--) "
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Found 13320 files belonging to 101 classes.\n",
            "Using 2664 files for validation.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " [elapsed time: 0:01:05] |********************************* | (ETA:   0:00:00) "
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lLW-8vrbrU5f"
      },
      "source": [
        "## CLIP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uK5qZA3mrU5g",
        "outputId": "376fe94a-ee0c-4db9-c476-600c7b5644fd"
      },
      "source": [
        "clip_embeddings_val_fn = \"clip\" + val_embeddings_filename_suffix\n",
        "clip_embeddings_val = get_ndarray_from_drive(drive, folderid, clip_embeddings_val_fn)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading clip_embeddings_val.npz from GDrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iOLtOsBJHKS2",
        "outputId": "f42f37c5-c744-41f9-e2a5-1f35eceb7a9d"
      },
      "source": [
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "if trivial:\n",
        "  #cc\n",
        "    results_filename = \"new_metrics\"+dataset_name+\"_softmax_\"+str(num_ways)+\"w\"+str(num_shot)+\"s\"+str(num_augmentations)+\"a_trivial_clip_lp_metrics_with_logits.json\"\n",
        "else:\n",
        "  #cc\n",
        "    results_filename = \"new_metrics\"+dataset_name+\"_softmax_\"+str(num_ways)+\"w\"+str(num_shot)+\"s\"+str(num_augmentations)+\"a_clip_lp_metrics_with_logits.json\"\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "download_file_by_name(drive, folderid, results_filename)\n",
        "if results_filename in os.listdir():\n",
        "    with open(results_filename, 'r') as f:\n",
        "      #cc\n",
        "        json_loaded = json.load(f)\n",
        "        #cc\n",
        "        clip_metrics_over_train_epochs = json_loaded['metrics']\n",
        "        #cc\n",
        "        logits_over_train_epochs = json_loaded[\"logits\"]\n",
        "else:\n",
        "    clip_metrics_over_train_epochs = []  \n",
        "    #cc  \n",
        "    logits_over_train_epochs = []   \n",
        "\n",
        "train_epochs_arr = [50]\n",
        "multi_label=False\n",
        "thresholds_val = thresholds # None\n",
        "# metrics_vals = ['hamming', 'jaccard', 'f1_score'] # ['accuracy', 'f1_score']\n",
        "metrics_val = ['accuracy', 'ap', 'map', 'c_f1', 'o_f1', 'c_precision', 'o_precision', 'c_recall', 'o_recall', 'top1_accuracy', 'top5_accuracy', 'classwise_accuracy', 'c_accuracy']\n",
        "\n",
        "for idx, train_epochs in enumerate(train_epochs_arr):\n",
        "    if idx < len(clip_metrics_over_train_epochs):\n",
        "        continue\n",
        "    print(train_epochs)\n",
        "    #cc\n",
        "    clip_metrics_thresholds, all_logits = run_evaluations(\n",
        "        (indices, embeddings),\n",
        "        train_indices,\n",
        "        eval_indices,\n",
        "        wi_y,\n",
        "        eval_y,\n",
        "        num_episodes,\n",
        "        num_ways,\n",
        "        thresholds,\n",
        "        train_epochs=train_epochs,\n",
        "        num_augmentations=num_augmentations,\n",
        "        embeddings=clip_embeddings_val,\n",
        "        multi_label=multi_label,\n",
        "        metrics=metrics_val\n",
        "    )\n",
        "    clip_metrics_over_train_epochs.append(clip_metrics_thresholds)\n",
        "    #cc\n",
        "    logits_over_train_epochs.append(all_logits)\n",
        "\n",
        "#cc\n",
        "    fin_list = []\n",
        "    #cc the whole for loop\n",
        "    for a1 in wi_y:\n",
        "      fin_a1_list = []\n",
        "      for a2 in a1:\n",
        "        # fin_a2_list = []\n",
        "        # for a3 in a2:\n",
        "        #   new_val = a3.decode(\"utf-8\")\n",
        "        #   fin_a2_list.append(new_val)\n",
        "        new_val = str(a2)\n",
        "        fin_a1_list.append(new_val)\n",
        "      fin_list.append(fin_a1_list)\n",
        "\n",
        "    with open(results_filename, 'w') as f:\n",
        "      #cc\n",
        "        results = {'metrics': clip_metrics_over_train_epochs,\n",
        "                   \"logits\": logits_over_train_epochs,\n",
        "                   \"true_labels\": fin_list}\n",
        "        json.dump(results, f)\n",
        "    auth.authenticate_user()\n",
        "    gauth = GoogleAuth()\n",
        "    gauth.credentials = GoogleCredentials.get_application_default()\n",
        "    drive = GoogleDrive(gauth)\n",
        "    delete_file_by_name(drive, folderid, results_filename)\n",
        "    save_to_drive(drive, folderid, results_filename)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r                                                                               \r\r [elapsed time: 0:00:00] |                                  | (ETA:  --:--:--) "
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "50\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " [elapsed time: 0:01:45] |**********************************| (ETA:  00:00:00) "
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Uploaded new_metricsUCF101_softmax_5w5s0a_clip_lp_metrics_with_logits.json to https://drive.google.com/drive/u/1/folders/1AR8qp1QPFv0M3jCIQKK8VA2xsNilo_pi\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V8uyo_T8rU5h"
      },
      "source": [
        "if trivial:\n",
        "    PLOT_DIR = \"NewMetrics_clip_lp_softmax_UCF101\" + str(num_ways) + \"w\" + str(num_shot) + \"s\" + str(num_augmentations) + \"a_trivial_plots\"\n",
        "else:\n",
        "    PLOT_DIR = \"NewMetrics_clip_lp_softmax_UCF101\" + str(num_ways) + \"w\" + str(num_shot) + \"s\" + str(num_augmentations) + \"a_plots\"\n",
        "os.mkdir(PLOT_DIR)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZK1qySJ9VJI"
      },
      "source": [
        "# chenni change whole block\n",
        "def get_best_metric_and_threshold(mt, metric_name, thresholds, optimal='max'):\n",
        "    if optimal=='max':\n",
        "        opt_value = np.max(np.mean(np.array(mt[metric_name]), axis=0))\n",
        "        opt_threshold = thresholds[np.argmax(np.mean(np.array(mt[metric_name]), axis=0))]\n",
        "    if optimal=='min':\n",
        "        opt_value = np.min(np.mean(np.array(mt[metric_name]), axis=0))\n",
        "        opt_threshold = thresholds[np.argmin(np.mean(np.array(mt[metric_name]), axis=0))]\n",
        "\n",
        "    return opt_value, opt_threshold"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Huz_ybH14iD",
        "outputId": "193dc1f4-250f-4b3e-939d-5f6301167478"
      },
      "source": [
        "# chenni change whole block\n",
        "#all_metrics = ['hamming', 'jaccard', 'subset_accuracy', 'map', 'c_f1', 'o_f1', 'c_precision', 'o_precision', 'c_recall', 'o_recall', 'top1_accuracy', 'top5_accuracy', 'c_accuracy']\n",
        "all_metrics = ['accuracy', 'ap', 'map', 'c_f1', 'o_f1', 'c_precision', 'o_precision', 'c_recall', 'o_recall', 'top1_accuracy', 'top5_accuracy', 'classwise_accuracy', 'c_accuracy']\n",
        "\n",
        "f1_vals = []\n",
        "f1_t_vals = []\n",
        "jaccard_vals = []\n",
        "jaccard_t_vals = []\n",
        "\n",
        "final_dict = {}\n",
        "for ind_metric in all_metrics:\n",
        "  vals = []\n",
        "  t_vals = []\n",
        "  final_array = []\n",
        "  for mt in clip_metrics_over_train_epochs:\n",
        "    if ind_metric == \"hamming\":\n",
        "      ret_val, ret_t_val = get_best_metric_and_threshold(mt,ind_metric,thresholds,\"min\")\n",
        "    else:\n",
        "      ret_val, ret_t_val = get_best_metric_and_threshold(mt,ind_metric,thresholds,\"max\")\n",
        "    vals.append(ret_val)\n",
        "    t_vals.append(ret_t_val)\n",
        "\n",
        "  final_array.append(vals)\n",
        "  final_array.append(t_vals)\n",
        "  final_dict[ind_metric] = final_array\n",
        "\n",
        "if trivial:\n",
        "    graph_filename = \"new_metrics\"+dataset_name+\"_softmax_\"+str(num_ways)+\"w\"+str(num_shot)+\"s\"+str(num_augmentations)+\"a_trivial_clip_lp_metrics_graphs.json\"\n",
        "else:\n",
        "    graph_filename = \"new_metrics\"+dataset_name+\"_softmax_\"+str(num_ways)+\"w\"+str(num_shot)+\"s\"+str(num_augmentations)+\"a_clip_lp_metrics_graphs.json\"\n",
        "\n",
        "with open(graph_filename, 'w') as f:\n",
        "        json.dump(final_dict, f)\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "delete_file_by_name(drive, folderid, graph_filename)\n",
        "save_to_drive(drive, folderid, graph_filename)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uploaded new_metricsUCF101_softmax_5w5s0a_clip_lp_metrics_graphs.json to https://drive.google.com/drive/u/1/folders/1AR8qp1QPFv0M3jCIQKK8VA2xsNilo_pi\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nanaUdwXM22e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1405a7e7-3eec-4e22-ec24-8238f89a16da"
      },
      "source": [
        "zip_dirname = PLOT_DIR + \".zip\"\n",
        "zip_source = PLOT_DIR\n",
        "! zip -r $zip_dirname $zip_source\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "save_to_drive(drive, folderid, zip_dirname)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  adding: NewMetrics_clip_lp_softmax_UCF1015w5s0a_plots/ (stored 0%)\n",
            "Uploaded NewMetrics_clip_lp_softmax_UCF1015w5s0a_plots.zip to https://drive.google.com/drive/u/1/folders/1AR8qp1QPFv0M3jCIQKK8VA2xsNilo_pi\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vrit1GAzrU5u"
      },
      "source": [
        "# 20 way 5 shot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "voN88mlc_OaG"
      },
      "source": [
        "## Picking indices"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CPr-n5OQ_OaK",
        "outputId": "dfacd558-0f2d-43ae-aa79-a5e3f8312f95"
      },
      "source": [
        "num_ways = 20\n",
        "num_shot = 5\n",
        "num_eval = 5\n",
        "shuffle = False\n",
        "num_episodes = 100\n",
        "\n",
        "train_indices, eval_indices, wi_y, eval_y = prepare_indices(\n",
        "    num_ways, num_shot, num_eval, num_episodes, label_dictionary, val_labels, shuffle\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " [elapsed time: 0:00:00] |*************************         | (ETA:   0:00:00) "
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69gR29tnmucd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9f9defb-1d7c-4f3d-adcc-035b7d4af4a4"
      },
      "source": [
        "embedding_model = embedding_models.CLIPEmbeddingWrapper()\n",
        "num_augmentations = 0\n",
        "trivial=False\n",
        "\n",
        "indices, embeddings = embed_images(\n",
        "    embedding_model, \n",
        "    train_indices, \n",
        "    num_augmentations,\n",
        "    trivial=trivial\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r                                                                               \r\r [elapsed time: 0:00:00] |                                  | (ETA:  --:--:--) "
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Found 13320 files belonging to 101 classes.\n",
            "Using 2664 files for validation.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " [elapsed time: 0:01:45] |********************************* | (ETA:   0:00:00) "
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zqYM5s6l_OaM"
      },
      "source": [
        "## CLIP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ni2ZvAB-_OaN",
        "outputId": "bab47929-f34e-42c8-f720-e50a377f87cd"
      },
      "source": [
        "clip_embeddings_val_fn = \"clip\" + val_embeddings_filename_suffix\n",
        "clip_embeddings_val = get_ndarray_from_drive(drive, folderid, clip_embeddings_val_fn)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading clip_embeddings_val.npz from GDrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P5C-7RszciDZ",
        "outputId": "c8e543fc-9878-4c04-ead2-548f664b5423"
      },
      "source": [
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "if trivial:\n",
        "  #cc\n",
        "    results_filename = \"new_metrics\"+dataset_name+\"_softmax_\"+str(num_ways)+\"w\"+str(num_shot)+\"s\"+str(num_augmentations)+\"a_trivial_clip_lp_metrics_with_logits.json\"\n",
        "else:\n",
        "  #cc\n",
        "    results_filename = \"new_metrics\"+dataset_name+\"_softmax_\"+str(num_ways)+\"w\"+str(num_shot)+\"s\"+str(num_augmentations)+\"a_clip_lp_metrics_with_logits.json\"\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "download_file_by_name(drive, folderid, results_filename)\n",
        "if results_filename in os.listdir():\n",
        "    with open(results_filename, 'r') as f:\n",
        "      #cc\n",
        "        json_loaded = json.load(f)\n",
        "        #cc\n",
        "        clip_metrics_over_train_epochs = json_loaded['metrics']\n",
        "        #cc\n",
        "        logits_over_train_epochs = json_loaded[\"logits\"]\n",
        "else:\n",
        "    clip_metrics_over_train_epochs = []  \n",
        "    #cc  \n",
        "    logits_over_train_epochs = []   \n",
        "\n",
        "train_epochs_arr = [50]\n",
        "multi_label=False\n",
        "thresholds_val = thresholds # None\n",
        "# metrics_vals = ['hamming', 'jaccard', 'f1_score'] # ['accuracy', 'f1_score']\n",
        "metrics_val = ['accuracy', 'ap', 'map', 'c_f1', 'o_f1', 'c_precision', 'o_precision', 'c_recall', 'o_recall', 'top1_accuracy', 'top5_accuracy', 'classwise_accuracy', 'c_accuracy']\n",
        "\n",
        "for idx, train_epochs in enumerate(train_epochs_arr):\n",
        "    if idx < len(clip_metrics_over_train_epochs):\n",
        "        continue\n",
        "    print(train_epochs)\n",
        "    #cc\n",
        "    clip_metrics_thresholds, all_logits = run_evaluations(\n",
        "        (indices, embeddings),\n",
        "        train_indices,\n",
        "        eval_indices,\n",
        "        wi_y,\n",
        "        eval_y,\n",
        "        num_episodes,\n",
        "        num_ways,\n",
        "        thresholds,\n",
        "        train_epochs=train_epochs,\n",
        "        num_augmentations=num_augmentations,\n",
        "        embeddings=clip_embeddings_val,\n",
        "        multi_label=multi_label,\n",
        "        metrics=metrics_val\n",
        "    )\n",
        "    clip_metrics_over_train_epochs.append(clip_metrics_thresholds)\n",
        "    #cc\n",
        "    logits_over_train_epochs.append(all_logits)\n",
        "\n",
        "#cc\n",
        "    fin_list = []\n",
        "    #cc the whole for loop\n",
        "    for a1 in wi_y:\n",
        "      fin_a1_list = []\n",
        "      for a2 in a1:\n",
        "        # fin_a2_list = []\n",
        "        # for a3 in a2:\n",
        "        #   new_val = a3.decode(\"utf-8\")\n",
        "        #   fin_a2_list.append(new_val)\n",
        "        new_val = str(a2)\n",
        "        fin_a1_list.append(new_val)\n",
        "      fin_list.append(fin_a1_list)\n",
        "\n",
        "    with open(results_filename, 'w') as f:\n",
        "      #cc\n",
        "        results = {'metrics': clip_metrics_over_train_epochs,\n",
        "                   \"logits\": logits_over_train_epochs,\n",
        "                   \"true_labels\": fin_list}\n",
        "        json.dump(results, f)\n",
        "    auth.authenticate_user()\n",
        "    gauth = GoogleAuth()\n",
        "    gauth.credentials = GoogleCredentials.get_application_default()\n",
        "    drive = GoogleDrive(gauth)\n",
        "    delete_file_by_name(drive, folderid, results_filename)\n",
        "    save_to_drive(drive, folderid, results_filename)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r                                                                               \r\r [elapsed time: 0:00:00] |                                  | (ETA:  --:--:--) "
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "50\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " [elapsed time: 0:03:30] |**********************************| (ETA:  00:00:00) "
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Uploaded new_metricsUCF101_softmax_20w5s0a_clip_lp_metrics_with_logits.json to https://drive.google.com/drive/u/1/folders/1AR8qp1QPFv0M3jCIQKK8VA2xsNilo_pi\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EdniozPrciDk"
      },
      "source": [
        "if trivial:\n",
        "    PLOT_DIR = \"NewMetrics_clip_lp_softmax_UCF101\" + str(num_ways) + \"w\" + str(num_shot) + \"s\" + str(num_augmentations) + \"a_trivial_plots\"\n",
        "else:\n",
        "    PLOT_DIR = \"NewMetrics_clip_lp_softmax_UCF101\" + str(num_ways) + \"w\" + str(num_shot) + \"s\" + str(num_augmentations) + \"a_plots\"\n",
        "os.mkdir(PLOT_DIR)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jb_cfeA7ciDk"
      },
      "source": [
        "# chenni change whole block\n",
        "def get_best_metric_and_threshold(mt, metric_name, thresholds, optimal='max'):\n",
        "    if optimal=='max':\n",
        "        opt_value = np.max(np.mean(np.array(mt[metric_name]), axis=0))\n",
        "        opt_threshold = thresholds[np.argmax(np.mean(np.array(mt[metric_name]), axis=0))]\n",
        "    if optimal=='min':\n",
        "        opt_value = np.min(np.mean(np.array(mt[metric_name]), axis=0))\n",
        "        opt_threshold = thresholds[np.argmin(np.mean(np.array(mt[metric_name]), axis=0))]\n",
        "\n",
        "    return opt_value, opt_threshold"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "runj1tYzciDk",
        "outputId": "1115787a-5f9b-4bae-954d-8205d927a360"
      },
      "source": [
        "# chenni change whole block\n",
        "\n",
        "f1_vals = []\n",
        "f1_t_vals = []\n",
        "jaccard_vals = []\n",
        "jaccard_t_vals = []\n",
        "\n",
        "final_dict = {}\n",
        "for ind_metric in all_metrics:\n",
        "  vals = []\n",
        "  t_vals = []\n",
        "  final_array = []\n",
        "  for mt in clip_metrics_over_train_epochs:\n",
        "    if ind_metric == \"hamming\":\n",
        "      ret_val, ret_t_val = get_best_metric_and_threshold(mt,ind_metric,thresholds,\"min\")\n",
        "    else:\n",
        "      ret_val, ret_t_val = get_best_metric_and_threshold(mt,ind_metric,thresholds,\"max\")\n",
        "    vals.append(ret_val)\n",
        "    t_vals.append(ret_t_val)\n",
        "\n",
        "  final_array.append(vals)\n",
        "  final_array.append(t_vals)\n",
        "  final_dict[ind_metric] = final_array\n",
        "\n",
        "if trivial:\n",
        "    graph_filename = \"new_metrics\"+dataset_name+\"_softmax_\"+str(num_ways)+\"w\"+str(num_shot)+\"s\"+str(num_augmentations)+\"a_trivial_clip_lp_metrics_graphs.json\"\n",
        "else:\n",
        "    graph_filename = \"new_metrics\"+dataset_name+\"_softmax_\"+str(num_ways)+\"w\"+str(num_shot)+\"s\"+str(num_augmentations)+\"a_clip_lp_metrics_graphs.json\"\n",
        "\n",
        "with open(graph_filename, 'w') as f:\n",
        "        json.dump(final_dict, f)\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "delete_file_by_name(drive, folderid, graph_filename)\n",
        "save_to_drive(drive, folderid, graph_filename)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uploaded new_metricsUCF101_softmax_20w5s0a_clip_lp_metrics_graphs.json to https://drive.google.com/drive/u/1/folders/1AR8qp1QPFv0M3jCIQKK8VA2xsNilo_pi\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BBP5r1MGciDl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed6d2dde-5b11-4989-c20c-25f6e27ba4c0"
      },
      "source": [
        "zip_dirname = PLOT_DIR + \".zip\"\n",
        "zip_source = PLOT_DIR\n",
        "! zip -r $zip_dirname $zip_source\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "save_to_drive(drive, folderid, zip_dirname)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  adding: NewMetrics_clip_lp_softmax_UCF10120w5s0a_plots/ (stored 0%)\n",
            "Uploaded NewMetrics_clip_lp_softmax_UCF10120w5s0a_plots.zip to https://drive.google.com/drive/u/1/folders/1AR8qp1QPFv0M3jCIQKK8VA2xsNilo_pi\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RlIMyHeVfFEE"
      },
      "source": [
        "# 5 way 1 shot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VnwSGFznAVvv"
      },
      "source": [
        "## Picking indices"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w1adV1UtAVv5",
        "outputId": "9186e107-07a5-466f-bd5c-06f54fe068bc"
      },
      "source": [
        "num_ways = 5\n",
        "num_shot = 1\n",
        "num_eval = 19\n",
        "shuffle = False\n",
        "num_episodes = 100\n",
        "\n",
        "train_indices, eval_indices, wi_y, eval_y = prepare_indices(\n",
        "    num_ways, num_shot, num_eval, num_episodes, label_dictionary, val_labels, shuffle\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " [elapsed time: 0:00:00] |************************          | (ETA:   0:00:00) "
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NjY5D1QHnO4z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21546d36-ea59-4cae-a495-5c9669bc6e3e"
      },
      "source": [
        "embedding_model = embedding_models.CLIPEmbeddingWrapper()\n",
        "num_augmentations = 0\n",
        "trivial=False\n",
        "\n",
        "indices, embeddings = embed_images(\n",
        "    embedding_model, \n",
        "    train_indices, \n",
        "    num_augmentations,\n",
        "    trivial=trivial\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r                                                                               \r\r [elapsed time: 0:00:00] |                                  | (ETA:  --:--:--) "
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Found 13320 files belonging to 101 classes.\n",
            "Using 2664 files for validation.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " [elapsed time: 0:00:25] |**********************************| (ETA:  00:00:00) "
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wW83nP3YAVv-"
      },
      "source": [
        "## CLIP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "idgO_q8jAVv_",
        "outputId": "d3b52d01-5419-4dd5-9048-64ae9cc31dfa"
      },
      "source": [
        "clip_embeddings_val_fn = \"clip\" + val_embeddings_filename_suffix\n",
        "clip_embeddings_val = get_ndarray_from_drive(drive, folderid, clip_embeddings_val_fn)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading clip_embeddings_val.npz from GDrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vwsOTyT8ckIf",
        "outputId": "41912978-59c8-4ff6-c489-8644fc49977c"
      },
      "source": [
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "if trivial:\n",
        "  #cc\n",
        "    results_filename = \"new_metrics\"+dataset_name+\"_softmax_\"+str(num_ways)+\"w\"+str(num_shot)+\"s\"+str(num_augmentations)+\"a_trivial_clip_lp_metrics_with_logits.json\"\n",
        "else:\n",
        "  #cc\n",
        "    results_filename = \"new_metrics\"+dataset_name+\"_softmax_\"+str(num_ways)+\"w\"+str(num_shot)+\"s\"+str(num_augmentations)+\"a_clip_lp_metrics_with_logits.json\"\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "download_file_by_name(drive, folderid, results_filename)\n",
        "if results_filename in os.listdir():\n",
        "    with open(results_filename, 'r') as f:\n",
        "      #cc\n",
        "        json_loaded = json.load(f)\n",
        "        #cc\n",
        "        clip_metrics_over_train_epochs = json_loaded['metrics']\n",
        "        #cc\n",
        "        logits_over_train_epochs = json_loaded[\"logits\"]\n",
        "else:\n",
        "    clip_metrics_over_train_epochs = []  \n",
        "    #cc  \n",
        "    logits_over_train_epochs = []   \n",
        "\n",
        "train_epochs_arr = [50]\n",
        "multi_label=False\n",
        "thresholds_val = thresholds # None\n",
        "# metrics_vals = ['hamming', 'jaccard', 'f1_score'] # ['accuracy', 'f1_score']\n",
        "metrics_val = ['accuracy', 'ap', 'map', 'c_f1', 'o_f1', 'c_precision', 'o_precision', 'c_recall', 'o_recall', 'top1_accuracy', 'top5_accuracy', 'classwise_accuracy', 'c_accuracy']\n",
        "\n",
        "for idx, train_epochs in enumerate(train_epochs_arr):\n",
        "    if idx < len(clip_metrics_over_train_epochs):\n",
        "        continue\n",
        "    print(train_epochs)\n",
        "    #cc\n",
        "    clip_metrics_thresholds, all_logits = run_evaluations(\n",
        "        (indices, embeddings),\n",
        "        train_indices,\n",
        "        eval_indices,\n",
        "        wi_y,\n",
        "        eval_y,\n",
        "        num_episodes,\n",
        "        num_ways,\n",
        "        thresholds,\n",
        "        train_epochs=train_epochs,\n",
        "        num_augmentations=num_augmentations,\n",
        "        embeddings=clip_embeddings_val,\n",
        "        multi_label=multi_label,\n",
        "        metrics=metrics_val\n",
        "    )\n",
        "    clip_metrics_over_train_epochs.append(clip_metrics_thresholds)\n",
        "    #cc\n",
        "    logits_over_train_epochs.append(all_logits)\n",
        "\n",
        "#cc\n",
        "    fin_list = []\n",
        "    #cc the whole for loop\n",
        "    for a1 in wi_y:\n",
        "      fin_a1_list = []\n",
        "      for a2 in a1:\n",
        "        # fin_a2_list = []\n",
        "        # for a3 in a2:\n",
        "        #   new_val = a3.decode(\"utf-8\")\n",
        "        #   fin_a2_list.append(new_val)\n",
        "        new_val = str(a2)\n",
        "        fin_a1_list.append(new_val)\n",
        "      fin_list.append(fin_a1_list)\n",
        "\n",
        "    with open(results_filename, 'w') as f:\n",
        "      #cc\n",
        "        results = {'metrics': clip_metrics_over_train_epochs,\n",
        "                   \"logits\": logits_over_train_epochs,\n",
        "                   \"true_labels\": fin_list}\n",
        "        json.dump(results, f)\n",
        "    auth.authenticate_user()\n",
        "    gauth = GoogleAuth()\n",
        "    gauth.credentials = GoogleCredentials.get_application_default()\n",
        "    drive = GoogleDrive(gauth)\n",
        "    delete_file_by_name(drive, folderid, results_filename)\n",
        "    save_to_drive(drive, folderid, results_filename)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r                                                                               \r\r [elapsed time: 0:00:00] |                                  | (ETA:  --:--:--) "
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "50\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " [elapsed time: 0:01:18] |**********************************| (ETA:  00:00:00) "
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Uploaded new_metricsUCF101_softmax_5w1s0a_clip_lp_metrics_with_logits.json to https://drive.google.com/drive/u/1/folders/1AR8qp1QPFv0M3jCIQKK8VA2xsNilo_pi\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jpbvZiBsckIg"
      },
      "source": [
        "if trivial:\n",
        "    PLOT_DIR = \"NewMetrics_clip_lp_softmax_UCF101\" + str(num_ways) + \"w\" + str(num_shot) + \"s\" + str(num_augmentations) + \"a_trivial_plots\"\n",
        "else:\n",
        "    PLOT_DIR = \"NewMetrics_clip_lp_softmax_UCF101\" + str(num_ways) + \"w\" + str(num_shot) + \"s\" + str(num_augmentations) + \"a_plots\"\n",
        "os.mkdir(PLOT_DIR)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XW0mU9-pckIg"
      },
      "source": [
        "# chenni change whole block\n",
        "def get_best_metric_and_threshold(mt, metric_name, thresholds, optimal='max'):\n",
        "    if optimal=='max':\n",
        "        opt_value = np.max(np.mean(np.array(mt[metric_name]), axis=0))\n",
        "        opt_threshold = thresholds[np.argmax(np.mean(np.array(mt[metric_name]), axis=0))]\n",
        "    if optimal=='min':\n",
        "        opt_value = np.min(np.mean(np.array(mt[metric_name]), axis=0))\n",
        "        opt_threshold = thresholds[np.argmin(np.mean(np.array(mt[metric_name]), axis=0))]\n",
        "\n",
        "    return opt_value, opt_threshold"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pg6YGBGEckIg",
        "outputId": "f84be2c8-f314-4748-e1ee-62173762336b"
      },
      "source": [
        "# chenni change whole block\n",
        "\n",
        "f1_vals = []\n",
        "f1_t_vals = []\n",
        "jaccard_vals = []\n",
        "jaccard_t_vals = []\n",
        "\n",
        "final_dict = {}\n",
        "for ind_metric in all_metrics:\n",
        "  vals = []\n",
        "  t_vals = []\n",
        "  final_array = []\n",
        "  for mt in clip_metrics_over_train_epochs:\n",
        "    if ind_metric == \"hamming\":\n",
        "      ret_val, ret_t_val = get_best_metric_and_threshold(mt,ind_metric,thresholds,\"min\")\n",
        "    else:\n",
        "      ret_val, ret_t_val = get_best_metric_and_threshold(mt,ind_metric,thresholds,\"max\")\n",
        "    vals.append(ret_val)\n",
        "    t_vals.append(ret_t_val)\n",
        "\n",
        "  final_array.append(vals)\n",
        "  final_array.append(t_vals)\n",
        "  final_dict[ind_metric] = final_array\n",
        "\n",
        "if trivial:\n",
        "    graph_filename = \"new_metrics\"+dataset_name+\"_softmax_\"+str(num_ways)+\"w\"+str(num_shot)+\"s\"+str(num_augmentations)+\"a_trivial_clip_lp_metrics_graphs.json\"\n",
        "else:\n",
        "    graph_filename = \"new_metrics\"+dataset_name+\"_softmax_\"+str(num_ways)+\"w\"+str(num_shot)+\"s\"+str(num_augmentations)+\"a_clip_lp_metrics_graphs.json\"\n",
        "\n",
        "with open(graph_filename, 'w') as f:\n",
        "        json.dump(final_dict, f)\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "delete_file_by_name(drive, folderid, graph_filename)\n",
        "save_to_drive(drive, folderid, graph_filename)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uploaded new_metricsUCF101_softmax_5w1s0a_clip_lp_metrics_graphs.json to https://drive.google.com/drive/u/1/folders/1AR8qp1QPFv0M3jCIQKK8VA2xsNilo_pi\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6g7bv2XqckIh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84f593de-a218-4ba0-f8c9-74fafcfc8d52"
      },
      "source": [
        "zip_dirname = PLOT_DIR + \".zip\"\n",
        "zip_source = PLOT_DIR\n",
        "! zip -r $zip_dirname $zip_source\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "save_to_drive(drive, folderid, zip_dirname)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  adding: NewMetrics_clip_lp_softmax_UCF1015w1s0a_plots/ (stored 0%)\n",
            "Uploaded NewMetrics_clip_lp_softmax_UCF1015w1s0a_plots.zip to https://drive.google.com/drive/u/1/folders/1AR8qp1QPFv0M3jCIQKK8VA2xsNilo_pi\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBhFDrzlfTHf"
      },
      "source": [
        "# 20 way 1 shot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pPNIJPo5A5SF"
      },
      "source": [
        "## Picking indices"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3aGftwJwA5SL",
        "outputId": "e166b171-9d1c-40ac-f715-710485efc3be"
      },
      "source": [
        "num_ways = 20\n",
        "num_shot = 1\n",
        "num_eval = 10\n",
        "shuffle = False\n",
        "num_episodes = 100\n",
        "\n",
        "train_indices, eval_indices, wi_y, eval_y = prepare_indices(\n",
        "    num_ways, num_shot, num_eval, num_episodes, label_dictionary, val_labels, shuffle\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " [elapsed time: 0:00:00] |********************************  | (ETA:   0:00:00) "
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IBytwLmdnPmu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01b8bdf9-3845-4d89-c3a4-f223a0eed28c"
      },
      "source": [
        "embedding_model = embedding_models.CLIPEmbeddingWrapper()\n",
        "num_augmentations = 0\n",
        "trivial=False\n",
        "\n",
        "indices, embeddings = embed_images(\n",
        "    embedding_model, \n",
        "    train_indices, \n",
        "    num_augmentations,\n",
        "    trivial=trivial\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r                                                                               \r\r [elapsed time: 0:00:00] |                                  | (ETA:  --:--:--) "
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Found 13320 files belonging to 101 classes.\n",
            "Using 2664 files for validation.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " [elapsed time: 0:00:59] |**********************************| (ETA:  00:00:00) "
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tlEE-NSuA5SO"
      },
      "source": [
        "## CLIP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cqbgv1AFA5SO",
        "outputId": "39ef5481-fae6-4390-dd38-e61138e3dfe2"
      },
      "source": [
        "clip_embeddings_val_fn = \"clip\" + val_embeddings_filename_suffix\n",
        "clip_embeddings_val = get_ndarray_from_drive(drive, folderid, clip_embeddings_val_fn)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading clip_embeddings_val.npz from GDrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ITMSxYcrcmAm",
        "outputId": "930c5ddc-2f32-48ec-8e3e-975ebf8defe6"
      },
      "source": [
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "if trivial:\n",
        "  #cc\n",
        "    results_filename = \"new_metrics\"+dataset_name+\"_softmax_\"+str(num_ways)+\"w\"+str(num_shot)+\"s\"+str(num_augmentations)+\"a_trivial_clip_lp_metrics_with_logits.json\"\n",
        "else:\n",
        "  #cc\n",
        "    results_filename = \"new_metrics\"+dataset_name+\"_softmax_\"+str(num_ways)+\"w\"+str(num_shot)+\"s\"+str(num_augmentations)+\"a_clip_lp_metrics_with_logits.json\"\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "download_file_by_name(drive, folderid, results_filename)\n",
        "if results_filename in os.listdir():\n",
        "    with open(results_filename, 'r') as f:\n",
        "      #cc\n",
        "        json_loaded = json.load(f)\n",
        "        #cc\n",
        "        clip_metrics_over_train_epochs = json_loaded['metrics']\n",
        "        #cc\n",
        "        logits_over_train_epochs = json_loaded[\"logits\"]\n",
        "else:\n",
        "    clip_metrics_over_train_epochs = []  \n",
        "    #cc  \n",
        "    logits_over_train_epochs = []   \n",
        "\n",
        "train_epochs_arr = [50]\n",
        "multi_label=False\n",
        "thresholds_val = thresholds # None\n",
        "# metrics_vals = ['hamming', 'jaccard', 'f1_score'] # ['accuracy', 'f1_score']\n",
        "metrics_val = ['accuracy', 'ap', 'map', 'c_f1', 'o_f1', 'c_precision', 'o_precision', 'c_recall', 'o_recall', 'top1_accuracy', 'top5_accuracy', 'classwise_accuracy', 'c_accuracy']\n",
        "\n",
        "for idx, train_epochs in enumerate(train_epochs_arr):\n",
        "    if idx < len(clip_metrics_over_train_epochs):\n",
        "        continue\n",
        "    print(train_epochs)\n",
        "    #cc\n",
        "    clip_metrics_thresholds, all_logits = run_evaluations(\n",
        "        (indices, embeddings),\n",
        "        train_indices,\n",
        "        eval_indices,\n",
        "        wi_y,\n",
        "        eval_y,\n",
        "        num_episodes,\n",
        "        num_ways,\n",
        "        thresholds,\n",
        "        train_epochs=train_epochs,\n",
        "        num_augmentations=num_augmentations,\n",
        "        embeddings=clip_embeddings_val,\n",
        "        multi_label=multi_label,\n",
        "        metrics=metrics_val\n",
        "    )\n",
        "    clip_metrics_over_train_epochs.append(clip_metrics_thresholds)\n",
        "    #cc\n",
        "    logits_over_train_epochs.append(all_logits)\n",
        "\n",
        "#cc\n",
        "    fin_list = []\n",
        "    #cc the whole for loop\n",
        "    for a1 in wi_y:\n",
        "      fin_a1_list = []\n",
        "      for a2 in a1:\n",
        "        # fin_a2_list = []\n",
        "        # for a3 in a2:\n",
        "        #   new_val = a3.decode(\"utf-8\")\n",
        "        #   fin_a2_list.append(new_val)\n",
        "        new_val = str(a2)\n",
        "        fin_a1_list.append(new_val)\n",
        "      fin_list.append(fin_a1_list)\n",
        "\n",
        "    with open(results_filename, 'w') as f:\n",
        "      #cc\n",
        "        results = {'metrics': clip_metrics_over_train_epochs,\n",
        "                   \"logits\": logits_over_train_epochs,\n",
        "                   \"true_labels\": fin_list}\n",
        "        json.dump(results, f)\n",
        "    auth.authenticate_user()\n",
        "    gauth = GoogleAuth()\n",
        "    gauth.credentials = GoogleCredentials.get_application_default()\n",
        "    drive = GoogleDrive(gauth)\n",
        "    delete_file_by_name(drive, folderid, results_filename)\n",
        "    save_to_drive(drive, folderid, results_filename)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r                                                                               \r\r [elapsed time: 0:00:00] |                                  | (ETA:  --:--:--) "
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "50\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " [elapsed time: 0:01:44] |**********************************| (ETA:  00:00:00) "
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Uploaded new_metricsUCF101_softmax_20w1s0a_clip_lp_metrics_with_logits.json to https://drive.google.com/drive/u/1/folders/1AR8qp1QPFv0M3jCIQKK8VA2xsNilo_pi\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WDddbNwZcmAm"
      },
      "source": [
        "if trivial:\n",
        "    PLOT_DIR = \"NewMetrics_clip_lp_softmax_UCF101\" + str(num_ways) + \"w\" + str(num_shot) + \"s\" + str(num_augmentations) + \"a_trivial_plots\"\n",
        "else:\n",
        "    PLOT_DIR = \"NewMetrics_clip_lp_softmax_UCF101\" + str(num_ways) + \"w\" + str(num_shot) + \"s\" + str(num_augmentations) + \"a_plots\"\n",
        "os.mkdir(PLOT_DIR)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MTCpGXkgcmAp"
      },
      "source": [
        "# chenni change whole block\n",
        "def get_best_metric_and_threshold(mt, metric_name, thresholds, optimal='max'):\n",
        "    if optimal=='max':\n",
        "        opt_value = np.max(np.mean(np.array(mt[metric_name]), axis=0))\n",
        "        opt_threshold = thresholds[np.argmax(np.mean(np.array(mt[metric_name]), axis=0))]\n",
        "    if optimal=='min':\n",
        "        opt_value = np.min(np.mean(np.array(mt[metric_name]), axis=0))\n",
        "        opt_threshold = thresholds[np.argmin(np.mean(np.array(mt[metric_name]), axis=0))]\n",
        "\n",
        "    return opt_value, opt_threshold"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wAHEIHaTcmAp",
        "outputId": "efbce1e3-c25e-46bd-f9a1-d639f915c411"
      },
      "source": [
        "# chenni change whole block\n",
        "\n",
        "f1_vals = []\n",
        "f1_t_vals = []\n",
        "jaccard_vals = []\n",
        "jaccard_t_vals = []\n",
        "\n",
        "final_dict = {}\n",
        "for ind_metric in all_metrics:\n",
        "  vals = []\n",
        "  t_vals = []\n",
        "  final_array = []\n",
        "  for mt in clip_metrics_over_train_epochs:\n",
        "    if ind_metric == \"hamming\":\n",
        "      ret_val, ret_t_val = get_best_metric_and_threshold(mt,ind_metric,thresholds,\"min\")\n",
        "    else:\n",
        "      ret_val, ret_t_val = get_best_metric_and_threshold(mt,ind_metric,thresholds,\"max\")\n",
        "    vals.append(ret_val)\n",
        "    t_vals.append(ret_t_val)\n",
        "\n",
        "  final_array.append(vals)\n",
        "  final_array.append(t_vals)\n",
        "  final_dict[ind_metric] = final_array\n",
        "\n",
        "if trivial:\n",
        "    graph_filename = \"new_metrics\"+dataset_name+\"_softmax_\"+str(num_ways)+\"w\"+str(num_shot)+\"s\"+str(num_augmentations)+\"a_trivial_clip_lp_metrics_graphs.json\"\n",
        "else:\n",
        "    graph_filename = \"new_metrics\"+dataset_name+\"_softmax_\"+str(num_ways)+\"w\"+str(num_shot)+\"s\"+str(num_augmentations)+\"a_clip_lp_metrics_graphs.json\"\n",
        "\n",
        "with open(graph_filename, 'w') as f:\n",
        "        json.dump(final_dict, f)\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "delete_file_by_name(drive, folderid, graph_filename)\n",
        "save_to_drive(drive, folderid, graph_filename)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uploaded new_metricsUCF101_softmax_20w1s0a_clip_lp_metrics_graphs.json to https://drive.google.com/drive/u/1/folders/1AR8qp1QPFv0M3jCIQKK8VA2xsNilo_pi\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49fOGr9xcmAq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5609c2b6-9b90-444a-afe2-a5b51ba28c02"
      },
      "source": [
        "zip_dirname = PLOT_DIR + \".zip\"\n",
        "zip_source = PLOT_DIR\n",
        "! zip -r $zip_dirname $zip_source\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "save_to_drive(drive, folderid, zip_dirname)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  adding: NewMetrics_clip_lp_softmax_UCF10120w1s0a_plots/ (stored 0%)\n",
            "Uploaded NewMetrics_clip_lp_softmax_UCF10120w1s0a_plots.zip to https://drive.google.com/drive/u/1/folders/1AR8qp1QPFv0M3jCIQKK8VA2xsNilo_pi\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}