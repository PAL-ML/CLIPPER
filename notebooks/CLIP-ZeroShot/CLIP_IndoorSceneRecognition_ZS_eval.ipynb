{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "new-repo-CLIP_IndoorSceneRecognition_ZS_eval.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "RGlVnbugxFK2",
        "3rKe3HqM523g",
        "OLU-gp7n8__E",
        "9z1WQnXdLHy2",
        "zU_gxQ0KbwMh",
        "tq-oEmFbThWq",
        "iLbRqaYxzbr7",
        "xP6ftkDCvKul",
        "g73bz0lqrU5e",
        "B5G7GsGDrU5u",
        "YnvVBO2brU5u",
        "RlIMyHeVfFEE",
        "sBhFDrzlfTHf"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2wCVCAfpyE0A"
      },
      "source": [
        "# Mount Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4-7JU3Q7q_XT",
        "outputId": "84c643fa-849b-4b37-e41c-9f2cdc957d1e"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vXAPOzOK4mTl",
        "outputId": "002235dc-e9aa-4131-dce9-912f592f771a"
      },
      "source": [
        "!pip install -U -q PyDrive\n",
        "!pip install httplib2==0.15.0\n",
        "import os\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from pydrive.files import GoogleDriveFileList\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "from getpass import getpass\n",
        "import urllib\n",
        "\n",
        "# 1. Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "# Cloning CLIPPER to access modules.\n",
        "if 'CLIPPER' not in os.listdir():\n",
        "    cmd_string = 'git clone https://github.com/PAL-ML/CLIPPER.git'\n",
        "    os.system(cmd_string)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: httplib2==0.15.0 in /usr/local/lib/python3.7/dist-packages (0.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RGlVnbugxFK2"
      },
      "source": [
        "# Installation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gmgIrfT8hDNE"
      },
      "source": [
        "## Install multi label metrics dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b6xXPAFbe6Gp",
        "outputId": "21fc80a3-a391-4274-c000-0d35f902b562"
      },
      "source": [
        "! pip install scikit-learn==0.24"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting scikit-learn==0.24\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b1/ed/ab51a8da34d2b3f4524b21093081e7f9e2ddf1c9eac9f795dcf68ad0a57d/scikit_learn-0.24.0-cp37-cp37m-manylinux2010_x86_64.whl (22.3MB)\n",
            "\u001b[K     |████████████████████████████████| 22.3MB 135kB/s \n",
            "\u001b[?25hRequirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.24) (1.0.1)\n",
            "Collecting threadpoolctl>=2.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/f7/12/ec3f2e203afa394a149911729357aa48affc59c20e2c1c8297a60f33f133/threadpoolctl-2.1.0-py3-none-any.whl\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.24) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.24) (1.19.5)\n",
            "Installing collected packages: threadpoolctl, scikit-learn\n",
            "  Found existing installation: scikit-learn 0.22.2.post1\n",
            "    Uninstalling scikit-learn-0.22.2.post1:\n",
            "      Successfully uninstalled scikit-learn-0.22.2.post1\n",
            "Successfully installed scikit-learn-0.24.0 threadpoolctl-2.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3rKe3HqM523g"
      },
      "source": [
        "## Install CLIP dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "poS-WNDixIhY",
        "outputId": "cb7f221c-18a2-4995-a632-46972b36f9b1"
      },
      "source": [
        "import subprocess\n",
        "\n",
        "CUDA_version = [s for s in subprocess.check_output([\"nvcc\", \"--version\"]).decode(\"UTF-8\").split(\", \") if s.startswith(\"release\")][0].split(\" \")[-1]\n",
        "print(\"CUDA version:\", CUDA_version)\n",
        "\n",
        "if CUDA_version == \"10.0\":\n",
        "    torch_version_suffix = \"+cu100\"\n",
        "elif CUDA_version == \"10.1\":\n",
        "    torch_version_suffix = \"+cu101\"\n",
        "elif CUDA_version == \"10.2\":\n",
        "    torch_version_suffix = \"\"\n",
        "else:\n",
        "    torch_version_suffix = \"+cu110\""
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CUDA version: 11.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uA-69W8M59nA",
        "outputId": "0955c253-e41a-4f44-bd3b-470dad854ed5"
      },
      "source": [
        "! pip install torch==1.7.1{torch_version_suffix} torchvision==0.8.2{torch_version_suffix} -f https://download.pytorch.org/whl/torch_stable.html ftfy regex"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Collecting torch==1.7.1+cu110\n",
            "\u001b[?25l  Downloading https://download.pytorch.org/whl/cu110/torch-1.7.1%2Bcu110-cp37-cp37m-linux_x86_64.whl (1156.8MB)\n",
            "\u001b[K     |███████████████████▋            | 709.4MB 1.4MB/s eta 0:05:27"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sYwBZS1N6A3d"
      },
      "source": [
        "! pip install ftfy regex\n",
        "! wget https://openaipublic.azureedge.net/clip/bpe_simple_vocab_16e6.txt.gz -O bpe_simple_vocab_16e6.txt.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9oIcNBYB8lz3"
      },
      "source": [
        "!pip install git+https://github.com/Sri-vatsa/CLIP # using this fork because of visualization capabilities"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OLU-gp7n8__E"
      },
      "source": [
        "## Install clustering dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6TLg9ozo9Hvc"
      },
      "source": [
        "!pip -q install umap-learn>=0.3.7"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9z1WQnXdLHy2"
      },
      "source": [
        "## Install dataset manager dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J1vvMx7_LLSp"
      },
      "source": [
        "!pip install wget"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NzsubsEm72rr"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KZI62a6G74kw"
      },
      "source": [
        "# ML Libraries\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "import keras\n",
        "\n",
        "# Data processing\n",
        "import PIL\n",
        "import base64\n",
        "import imageio\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "\n",
        "from PIL import Image\n",
        "import cv2\n",
        "import imgaug.augmenters as iaa\n",
        "\n",
        "# Plotting\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from IPython.core.display import display, HTML\n",
        "from matplotlib import cm\n",
        "\n",
        "# Models\n",
        "import clip\n",
        "\n",
        "# Datasets\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "# Misc\n",
        "import progressbar\n",
        "import logging\n",
        "from abc import ABC, abstractmethod\n",
        "import time\n",
        "import urllib.request\n",
        "import os\n",
        "import itertools\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Modules\n",
        "from CLIPPER.code.ExperimentModules import embedding_models\n",
        "from CLIPPER.code.ExperimentModules import simclr_data_augmentations\n",
        "from CLIPPER.code.ExperimentModules.dataset_manager import DatasetManager\n",
        "from CLIPPER.code.ExperimentModules.clip_few_shot import CLIPFewShotClassifier\n",
        "from CLIPPER.code.ExperimentModules.utils import (save_npy, load_npy, \n",
        "                                                       get_folder_id, \n",
        "                                                       create_expt_dir, \n",
        "                                                       save_to_drive, \n",
        "                                                       load_all_from_drive_folder, \n",
        "                                                       download_file_by_name, \n",
        "                                                       delete_file_by_name)\n",
        "\n",
        "logging.getLogger('googleapicliet.discovery_cache').setLevel(logging.ERROR)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zU_gxQ0KbwMh"
      },
      "source": [
        "# Initialization & Constants\n",
        "\n",
        "**Edited**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bih5tBPdbx3u"
      },
      "source": [
        "dataset_name = \"ISR\"\n",
        "folder_name = \"IndoorSceneRecognition-Embeddings-28-02-21\"\n",
        "\n",
        "# Change parentid to match that of experiments root folder in gdrive\n",
        "parentid = '1bK72W-Um20EQDEyChNhNJthUNbmoSEjD'\n",
        "\n",
        "# Filepaths\n",
        "train_labels_filename = \"train_labels.npz\"\n",
        "val_labels_filename = \"val_labels.npz\"\n",
        "\n",
        "train_embeddings_filename_suffix = \"_embeddings_train.npz\"\n",
        "val_embeddings_filename_suffix = \"_embeddings_val.npz\"\n",
        "\n",
        "\n",
        "# Initialize sepcific experiment folder in drive\n",
        "folderid = create_expt_dir(drive, parentid, folder_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UxTa8MVsvQCN"
      },
      "source": [
        "# Load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n6S124Jfwuu5"
      },
      "source": [
        "def get_ndarray_from_drive(drive, folderid, filename):\n",
        "    download_file_by_name(drive, folderid, filename)\n",
        "    return np.load(filename)['data']\n",
        "\n",
        "# train_labels = get_ndarray_from_drive(drive, folderid, train_labels_filename)\n",
        "val_labels = get_ndarray_from_drive(drive, folderid, val_labels_filename)\n",
        "# test_labels = get_ndarray_from_drive(drive, folderid, test_labels_filename)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4p2JpaGHmqbl"
      },
      "source": [
        "dm = DatasetManager()\n",
        "\n",
        "test_data_generator = dm.load_dataset('indoor_scene_recognition', split='val')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tq-oEmFbThWq"
      },
      "source": [
        "# Label dicts"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "54SEXo03TpA_"
      },
      "source": [
        "classes = dm.get_class_names()\n",
        "\n",
        "class_names = [x.replace(\"_\", \" \") for x in classes]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iLbRqaYxzbr7"
      },
      "source": [
        "# Create label dictionary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Emz85fNX0Vif"
      },
      "source": [
        "unique_labels = np.unique(val_labels)\n",
        "print(len(unique_labels))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0TubS-RLzeVM"
      },
      "source": [
        "label_dictionary = {la:[] for la in unique_labels}\n",
        "\n",
        "for i in range(len(val_labels)):\n",
        "    la = val_labels[i]\n",
        "\n",
        "    label_dictionary[la].append(i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W7FC96fbxMtT"
      },
      "source": [
        "print(label_dictionary)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xP6ftkDCvKul"
      },
      "source": [
        "# CLIP zero shot eval"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJIGo_R86GQi"
      },
      "source": [
        "## Function definitions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pv68q8TH6tvG"
      },
      "source": [
        "def start_progress_bar(bar_len):\n",
        "    widgets = [\n",
        "        ' [', \n",
        "        progressbar.Timer(format= 'elapsed time: %(elapsed)s'), \n",
        "        '] ', \n",
        "        progressbar.Bar('*'),' (', \n",
        "        progressbar.ETA(), ') ', \n",
        "        ]\n",
        "    pbar = progressbar.ProgressBar(\n",
        "        max_value=bar_len, widgets=widgets\n",
        "        ).start()\n",
        "\n",
        "    return pbar"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DtoopzLRStTO"
      },
      "source": [
        "def prepare_indices(\n",
        "    num_ways,\n",
        "    num_shot,\n",
        "    num_eval,\n",
        "    num_episodes,\n",
        "    label_dictionary,\n",
        "    test_labels,\n",
        "    shuffle=False\n",
        "):\n",
        "    eval_indices = []\n",
        "    train_indices = []\n",
        "    wi_y = []\n",
        "    eval_y = []\n",
        "\n",
        "    label_dictionary = {la:label_dictionary[la] for la in label_dictionary if len(label_dictionary[la]) >= (num_shot+num_eval)}\n",
        "    unique_labels = list(label_dictionary.keys())\n",
        "\n",
        "    pbar = start_progress_bar(num_episodes)\n",
        "\n",
        "    for s in range(num_episodes):\n",
        "        # Setting random seed for replicability\n",
        "        np.random.seed(s)\n",
        "\n",
        "        _train_indices = []\n",
        "        _eval_indices = []\n",
        "\n",
        "        selected_labels = np.random.choice(unique_labels, size=num_ways, replace=False)\n",
        "        for la in selected_labels:\n",
        "            la_indices = label_dictionary[la]\n",
        "            select = np.random.choice(la_indices, size = num_shot+num_eval, replace=False)\n",
        "            tr_idx = list(select[:num_shot])\n",
        "            ev_idx = list(select[num_shot:])\n",
        "\n",
        "            _train_indices = _train_indices + tr_idx\n",
        "            _eval_indices = _eval_indices + ev_idx\n",
        "\n",
        "        if shuffle:\n",
        "            np.random.shuffle(_train_indices)\n",
        "            np.random.shuffle(_eval_indices)\n",
        "\n",
        "        train_indices.append(_train_indices)\n",
        "        eval_indices.append(_eval_indices)\n",
        "\n",
        "        _wi_y = test_labels[_train_indices]\n",
        "        _eval_y = test_labels[_eval_indices]\n",
        "\n",
        "        wi_y.append(_wi_y)\n",
        "        eval_y.append(_eval_y)\n",
        "\n",
        "        pbar.update(s+1)\n",
        "        \n",
        "    return train_indices, eval_indices, wi_y, eval_y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4M5aHABX7k8a"
      },
      "source": [
        "def embed_images(\n",
        "    embedding_model, \n",
        "    train_indices, \n",
        "    num_augmentations,\n",
        "    trivial=False\n",
        "    ):\n",
        "\n",
        "    def augment_image(image, num_augmentations, trivial):\n",
        "        \"\"\" Perform SimCLR augmentations on the image\n",
        "        \"\"\"\n",
        "\n",
        "        if np.max(image) > 1:\n",
        "            image = image/255\n",
        "\n",
        "        augmented_images = [image]\n",
        "\n",
        "        # augmentations = iaa.Sequential([\n",
        "        #     iaa.Affine(\n",
        "        #         translate_percent={'x':(-0.1, 0.1), 'y':(-0.1, 0.1)}, \n",
        "        #         rotate=(-15, 15),\n",
        "        #         shear=(-15, 15),\n",
        "        #     ),\n",
        "        #     iaa.Fliplr(0.5) \n",
        "        # ])\n",
        "\n",
        "        def _run_filters(image):\n",
        "            width = image.shape[1]\n",
        "            height = image.shape[0]\n",
        "            image_aug = simclr_data_augmentations.random_crop_with_resize(\n",
        "                image,\n",
        "                height,\n",
        "                width\n",
        "                )\n",
        "            image_aug = tf.image.random_flip_left_right(image_aug)\n",
        "            image_aug = simclr_data_augmentations.random_color_jitter(image_aug)\n",
        "            image_aug = simclr_data_augmentations.random_blur(\n",
        "                image_aug,\n",
        "                height,\n",
        "                width\n",
        "                )\n",
        "            image_aug = tf.reshape(image_aug, [image.shape[0], image.shape[1], 3])\n",
        "            image_aug = tf.clip_by_value(image_aug, 0., 1.)\n",
        "\n",
        "            return image_aug.numpy()\n",
        "\n",
        "        for _ in range(num_augmentations):\n",
        "            # aug_image = augmentations(image=image)\n",
        "\n",
        "            if trivial:\n",
        "                aug_image = image\n",
        "            else:\n",
        "                aug_image = _run_filters(image)\n",
        "            augmented_images.append(aug_image)\n",
        "\n",
        "        augmented_images = np.stack(augmented_images)\n",
        "        return augmented_images\n",
        "\n",
        "    embedding_model.load_model()\n",
        "\n",
        "    unique_indices = np.unique(np.array(train_indices))\n",
        "\n",
        "    ds = dm.load_dataset('indoor_scene_recognition', split='val')\n",
        "    embeddings = []\n",
        "    IMAGE_IDX = 0\n",
        "    pbar = start_progress_bar(unique_indices.size+1)\n",
        "    num_done=0\n",
        "    for idx, item in enumerate(ds):\n",
        "        if idx in unique_indices:\n",
        "            image = item[IMAGE_IDX][0]\n",
        "            if num_augmentations > 0:\n",
        "                aug_images = augment_image(image, num_augmentations, trivial)\n",
        "            else:\n",
        "                aug_images = image\n",
        "            \n",
        "            processed_images = embedding_model.preprocess_data(aug_images)\n",
        "            embedding = embedding_model.embed_images(processed_images)\n",
        "            embeddings.append(embedding)\n",
        "            \n",
        "            num_done += 1\n",
        "            pbar.update(num_done+1)\n",
        "\n",
        "        if idx == unique_indices[-1]:\n",
        "            break\n",
        "\n",
        "    embeddings = np.stack(embeddings)\n",
        "\n",
        "    return unique_indices, embeddings"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tyW-G0AD5_rn"
      },
      "source": [
        "def evaluate_model_for_episode(\n",
        "    model,\n",
        "    eval_x,\n",
        "    eval_y,\n",
        "    label_mapping,\n",
        "    filtered_classes,\n",
        "    metrics=['hamming', 'jaccard', 'subset_accuracy', 'ap', 'map', 'c_f1', 'o_f1', 'c_precision', 'o_precision', 'c_recall', 'o_recall', 'top1_accuracy', 'top5_accuracy', 'classwise_accuracy', 'c_accuracy'],\n",
        "    multi_label=True\n",
        "):\n",
        "    zs_weights = model.zeroshot_classifier(filtered_classes)\n",
        "    logits = model.predict_scores(eval_x, zs_weights).tolist()\n",
        "    \n",
        "    pred_y = model.predict_label(eval_x, zs_weights)\n",
        "    pred_y = [label_mapping[l] for l in pred_y]\n",
        "\n",
        "    met = model.evaluate_single_label_metrics(\n",
        "            eval_x, eval_y, label_mapping, zs_weights, metrics\n",
        "            )\n",
        "\n",
        "    return pred_y, met, logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l2cWOf7SQF66"
      },
      "source": [
        "def get_label_mapping_n_class_names(eval_y, class_names):\n",
        "  label_mapping = {}\n",
        "  unique_labels = np.unique(eval_y)\n",
        "  filtered_classes = [class_names[x] for x in unique_labels]\n",
        "\n",
        "  num_classes = len(unique_labels)\n",
        "  for c in range(num_classes):\n",
        "    label_mapping[c] = unique_labels[c]\n",
        "\n",
        "  return label_mapping, filtered_classes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-H9Kex59foo1"
      },
      "source": [
        "# chenni change\n",
        "def run_episode_through_model(\n",
        "    indices_and_embeddings,\n",
        "    train_indices, \n",
        "    eval_indices, \n",
        "    wi_y, \n",
        "    eval_y,\n",
        "    class_names,\n",
        "    num_augmentations=0,\n",
        "    train_epochs=None,\n",
        "    train_batch_size=5,\n",
        "    metrics=['hamming', 'jaccard', 'subset_accuracy', 'ap', 'map', 'c_f1', 'o_f1', 'c_precision', 'o_precision', 'c_recall', 'o_recall', 'top1_accuracy', 'top5_accuracy', 'classwise_accuracy', 'c_accuracy'],\n",
        "    embeddings=None,\n",
        "    multi_label=True\n",
        "):\n",
        "    metrics_values = {m:[] for m in metrics}\n",
        "    indices_and_embeddings\n",
        "    eval_x = embeddings[eval_indices]\n",
        "    ep_logits = []\n",
        "    \n",
        "    label_mapping, filtered_classes = get_label_mapping_n_class_names(eval_y, class_names)\n",
        "\n",
        "    clip_fs_parameters = {\n",
        "        \"num_classes\": num_ways,\n",
        "        \"input_dims\": eval_x.shape[-1],\n",
        "        \"multi_label\": multi_label\n",
        "    }\n",
        "\n",
        "    clip_fs_cls = CLIPFewShotClassifier(clip_fs_parameters)\n",
        "    \n",
        "    pred_labels, metrics_values, logits = evaluate_model_for_episode(\n",
        "            clip_fs_cls,\n",
        "            eval_x,\n",
        "            eval_y,\n",
        "            label_mapping,\n",
        "            filtered_classes,\n",
        "            metrics=metrics,\n",
        "            multi_label=False\n",
        "    )\n",
        "    ep_logits = logits\n",
        "#cc\n",
        "    return metrics_values, ep_logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mUpPZ6ti6IEp"
      },
      "source": [
        "def run_evaluations(\n",
        "    indices_and_embeddings,\n",
        "    train_indices, \n",
        "    eval_indices, \n",
        "    wi_y, \n",
        "    eval_y, \n",
        "    num_episodes, \n",
        "    num_ways,\n",
        "    class_names,\n",
        "    verbose=True,\n",
        "    normalize=True,\n",
        "    train_epochs=None,\n",
        "    train_batch_size=5,\n",
        "    metrics=['accuracy', 'ap', 'map', 'c_f1', 'o_f1', 'c_precision', 'o_precision', 'c_recall', 'o_recall', 'top1_accuracy', 'top5_accuracy', 'classwise_accuracy', 'c_accuracy'],\n",
        "    embeddings=None,\n",
        "    num_augmentations=0,\n",
        "    multi_label=True\n",
        "):\n",
        "    metrics_values = {m:[] for m in metrics}\n",
        "\n",
        "    all_logits = []\n",
        "\n",
        "    if verbose:\n",
        "        pbar = start_progress_bar(num_episodes)\n",
        "\n",
        "    for idx_ep in range(num_episodes):\n",
        "        _train_indices = train_indices[idx_ep]\n",
        "        _eval_indices = eval_indices[idx_ep]\n",
        "        _wi_y = [label for label in wi_y[idx_ep]]\n",
        "        _eval_y = [label for label in eval_y[idx_ep]]\n",
        "\n",
        "        met, ep_logits = run_episode_through_model(\n",
        "            indices_and_embeddings,\n",
        "            _train_indices, \n",
        "            _eval_indices, \n",
        "            _wi_y, \n",
        "            _eval_y,\n",
        "            class_names,\n",
        "            num_augmentations=num_augmentations,\n",
        "            train_epochs=train_epochs,\n",
        "            train_batch_size=train_batch_size,\n",
        "            embeddings=embeddings,\n",
        "            metrics=metrics,\n",
        "            multi_label=multi_label\n",
        "        )\n",
        "\n",
        "        all_logits.append(ep_logits)\n",
        "\n",
        "        for m in metrics:\n",
        "            metrics_values[m].append(met[m])\n",
        "\n",
        "        if verbose:\n",
        "            pbar.update(idx_ep+1)\n",
        "\n",
        "    return metrics_values, all_logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pVO25nDT88qK"
      },
      "source": [
        "def get_best_metric(mt, metric_name, optimal='max'):\n",
        "    if optimal=='max':\n",
        "        opt_value = np.max(np.mean(np.array(mt[metric_name]), axis=0))\n",
        "    if optimal=='min':\n",
        "        opt_value = np.min(np.mean(np.array(mt[metric_name]), axis=0))\n",
        "\n",
        "    return opt_value"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g8CNr6I4rU5d"
      },
      "source": [
        "# 5 way 5 shot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g73bz0lqrU5e"
      },
      "source": [
        "## Picking indices"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fm6j-e0brU5f"
      },
      "source": [
        "num_ways = 5\n",
        "num_shot = 5\n",
        "num_eval = 15\n",
        "shuffle = False\n",
        "num_episodes = 100\n",
        "\n",
        "train_indices, eval_indices, wi_y, eval_y = prepare_indices(\n",
        "    num_ways, num_shot, num_eval, num_episodes, label_dictionary, val_labels, shuffle\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X5Ov81G5BUJe"
      },
      "source": [
        "embedding_model = embedding_models.CLIPEmbeddingWrapper()\n",
        "num_augmentations = 0\n",
        "trivial=False\n",
        "\n",
        "indices, embeddings = embed_images(\n",
        "    embedding_model, \n",
        "    train_indices, \n",
        "    num_augmentations,\n",
        "    trivial=trivial\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lLW-8vrbrU5f"
      },
      "source": [
        "## CLIP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ghmre8xB5Tft"
      },
      "source": [
        "clip_embeddings_test_fn = \"clip\" + val_embeddings_filename_suffix\n",
        "clip_embeddings_test = get_ndarray_from_drive(drive, folderid, clip_embeddings_test_fn)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XGTJsU6yN0S0"
      },
      "source": [
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "if trivial:\n",
        "  #cc\n",
        "    results_filename = \"new_metrics\"+dataset_name+\"_0t\"+str(num_ways)+\"w\"+str(num_shot)+\"s\"+str(num_augmentations)+\"a_trivial_clip_zs_metrics_with_logits.json\"\n",
        "else:\n",
        "  #cc\n",
        "    results_filename = \"new_metrics\"+dataset_name+\"_0t\"+str(num_ways)+\"w\"+str(num_shot)+\"s\"+str(num_augmentations)+\"a_clip_zs_metrics_with_logits.json\"\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "download_file_by_name(drive, folderid, results_filename)\n",
        "if results_filename in os.listdir():\n",
        "    with open(results_filename, 'r') as f:\n",
        "      #cc\n",
        "        json_loaded = json.load(f)\n",
        "        #cc\n",
        "        clip_metrics_over_train_epochs = json_loaded['metrics']\n",
        "        #cc\n",
        "        logits_over_train_epochs = json_loaded[\"logits\"]\n",
        "else:\n",
        "    clip_metrics_over_train_epochs = []  \n",
        "    logits_over_train_epochs = []   \n",
        "\n",
        "train_epochs_arr = [0]\n",
        "multi_label=False\n",
        "# metrics_vals = ['hamming', 'jaccard', 'f1_score'] # ['accuracy', 'f1_score']\n",
        "\n",
        "for idx, train_epochs in enumerate(train_epochs_arr):\n",
        "    if idx < len(clip_metrics_over_train_epochs):\n",
        "        continue\n",
        "    print(train_epochs)\n",
        "    #cc\n",
        "    clip_metrics_thresholds, all_logits = run_evaluations(\n",
        "        (indices, embeddings),\n",
        "        train_indices,\n",
        "        eval_indices,\n",
        "        wi_y,\n",
        "        eval_y,\n",
        "        num_episodes,\n",
        "        num_ways,\n",
        "        class_names,\n",
        "        train_epochs=train_epochs,\n",
        "        num_augmentations=num_augmentations,\n",
        "        embeddings=clip_embeddings_test\n",
        "    )\n",
        "    clip_metrics_over_train_epochs.append(clip_metrics_thresholds)\n",
        "    #cc\n",
        "    logits_over_train_epochs.append(all_logits)\n",
        "\n",
        "#cc\n",
        "    fin_list = []\n",
        "    #cc the whole for loop\n",
        "    for a1 in wi_y:\n",
        "      fin_a1_list = []\n",
        "      for a2 in a1:\n",
        "        new_val = str(a2)\n",
        "        fin_a1_list.append(new_val)\n",
        "      fin_list.append(fin_a1_list)\n",
        "\n",
        "    with open(results_filename, 'w') as f:\n",
        "      #cc\n",
        "        results = {'metrics': clip_metrics_over_train_epochs,\n",
        "                   \"logits\": logits_over_train_epochs,\n",
        "                   \"true_labels\": fin_list}\n",
        "        json.dump(results, f)\n",
        "    auth.authenticate_user()\n",
        "    gauth = GoogleAuth()\n",
        "    gauth.credentials = GoogleCredentials.get_application_default()\n",
        "    drive = GoogleDrive(gauth)\n",
        "    #delete_file_by_name(drive, folderid, results_filename)\n",
        "    #save_to_drive(drive, folderid, results_filename)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V8uyo_T8rU5h"
      },
      "source": [
        "if trivial:\n",
        "    PLOT_DIR = \"NewMetrics_clip_zs_Sigmoid_IndoorSceneRecognition\" + \"_0t\" + str(num_ways) + \"w\" + str(num_shot) + \"s\" + str(num_augmentations) + \"a_trivial_plots\"\n",
        "else:\n",
        "    PLOT_DIR = \"NewMetrics_clip_zs_Sigmoid_IndoorSceneRecognition\" + \"_0t\" + str(num_ways) + \"w\" + str(num_shot) + \"s\" + str(num_augmentations) + \"a_plots\"\n",
        "os.mkdir(PLOT_DIR)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QTID6tl2rFhn"
      },
      "source": [
        "# chenni change whole block\n",
        "all_metrics = ['accuracy', 'ap', 'map', 'c_f1', 'o_f1', 'c_precision', 'o_precision', 'c_recall', 'o_recall', 'top1_accuracy', 'top5_accuracy', 'classwise_accuracy', 'c_accuracy']\n",
        "\n",
        "final_dict = {}\n",
        "for ind_metric in all_metrics:\n",
        "  vals = []\n",
        "  final_array = []\n",
        "  for mt in clip_metrics_over_train_epochs:\n",
        "      ret_val = get_best_metric(mt,ind_metric,\"max\")\n",
        "      vals.append(ret_val)\n",
        "\n",
        "  final_array.append(vals)\n",
        "  final_dict[ind_metric] = final_array\n",
        "\n",
        "if trivial:\n",
        "    graph_filename = \"new_metrics\"+dataset_name+\"_0t\"+str(num_ways)+\"w\"+str(num_shot)+\"s\"+str(num_augmentations)+\"a_trivial_clip_zs_metrics_graphs.json\"\n",
        "else:\n",
        "    graph_filename = \"new_metrics\"+dataset_name+\"_0t\"+str(num_ways)+\"w\"+str(num_shot)+\"s\"+str(num_augmentations)+\"a_clip_zs_metrics_graphs.json\"\n",
        "\n",
        "with open(graph_filename, 'w') as f:\n",
        "        json.dump(final_dict, f)\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "delete_file_by_name(drive, folderid, graph_filename)\n",
        "save_to_drive(drive, folderid, graph_filename)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FRc9bHsURFDX"
      },
      "source": [
        "zip_dirname = PLOT_DIR + \".zip\"\n",
        "zip_source = PLOT_DIR\n",
        "! zip -r $zip_dirname $zip_source\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "save_to_drive(drive, folderid, zip_dirname)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vrit1GAzrU5u"
      },
      "source": [
        "# 13 way 5 shot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5G7GsGDrU5u"
      },
      "source": [
        "## Picking indices"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yvDuWsZerU5u"
      },
      "source": [
        "num_ways = 13\n",
        "num_shot = 5\n",
        "num_eval = 5\n",
        "shuffle = False\n",
        "num_episodes = 100\n",
        "\n",
        "train_indices, eval_indices, wi_y, eval_y = prepare_indices(\n",
        "    num_ways, num_shot, num_eval, num_episodes, label_dictionary, val_labels, shuffle\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eLK5V-xFE6V9"
      },
      "source": [
        "embedding_model = embedding_models.CLIPEmbeddingWrapper()\n",
        "num_augmentations = 0\n",
        "trivial=False\n",
        "\n",
        "indices, embeddings = embed_images(\n",
        "    embedding_model, \n",
        "    train_indices, \n",
        "    num_augmentations,\n",
        "    trivial=trivial\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YnvVBO2brU5u"
      },
      "source": [
        "## CLIP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EHwC6CiqrU5u"
      },
      "source": [
        "clip_embeddings_test_fn = \"clip\" + val_embeddings_filename_suffix\n",
        "clip_embeddings_test = get_ndarray_from_drive(drive, folderid, clip_embeddings_test_fn)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BHkvWSCtFJnO"
      },
      "source": [
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "if trivial:\n",
        "  #cc\n",
        "    results_filename = \"new_metrics\"+dataset_name+\"_0t\"+str(num_ways)+\"w\"+str(num_shot)+\"s\"+str(num_augmentations)+\"a_trivial_clip_zs_metrics_with_logits.json\"\n",
        "else:\n",
        "  #cc\n",
        "    results_filename = \"new_metrics\"+dataset_name+\"_0t\"+str(num_ways)+\"w\"+str(num_shot)+\"s\"+str(num_augmentations)+\"a_metrics_clip_zs_with_logits.json\"\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "download_file_by_name(drive, folderid, results_filename)\n",
        "if results_filename in os.listdir():\n",
        "    with open(results_filename, 'r') as f:\n",
        "      #cc\n",
        "        json_loaded = json.load(f)\n",
        "        #cc\n",
        "        clip_metrics_over_train_epochs = json_loaded['metrics']\n",
        "        #cc\n",
        "        logits_over_train_epochs = json_loaded[\"logits\"]\n",
        "else:\n",
        "    clip_metrics_over_train_epochs = []  \n",
        "    #cc  \n",
        "    logits_over_train_epochs = []   \n",
        "\n",
        "train_epochs_arr = [0]\n",
        "multi_label=False\n",
        "# metrics_vals = ['hamming', 'jaccard', 'f1_score'] # ['accuracy', 'f1_score']\n",
        "\n",
        "for idx, train_epochs in enumerate(train_epochs_arr):\n",
        "    if idx < len(clip_metrics_over_train_epochs):\n",
        "        continue\n",
        "    print(train_epochs)\n",
        "    #cc\n",
        "    clip_metrics_thresholds, all_logits = run_evaluations(\n",
        "        (indices, embeddings),\n",
        "        train_indices,\n",
        "        eval_indices,\n",
        "        wi_y,\n",
        "        eval_y,\n",
        "        num_episodes,\n",
        "        num_ways,\n",
        "        class_names,\n",
        "        train_epochs=train_epochs,\n",
        "        num_augmentations=num_augmentations,\n",
        "        embeddings=clip_embeddings_test\n",
        "    )\n",
        "    clip_metrics_over_train_epochs.append(clip_metrics_thresholds)\n",
        "    #cc\n",
        "    logits_over_train_epochs.append(all_logits)\n",
        "\n",
        "#cc\n",
        "    fin_list = []\n",
        "    #cc the whole for loop\n",
        "    for a1 in wi_y:\n",
        "      fin_a1_list = []\n",
        "      for a2 in a1:\n",
        "        new_val = str(a2)\n",
        "        fin_a1_list.append(new_val)\n",
        "      fin_list.append(fin_a1_list)\n",
        "\n",
        "    with open(results_filename, 'w') as f:\n",
        "      #cc\n",
        "        results = {'metrics': clip_metrics_over_train_epochs,\n",
        "                   \"logits\": logits_over_train_epochs,\n",
        "                   \"true_labels\": fin_list}\n",
        "        json.dump(results, f)\n",
        "    auth.authenticate_user()\n",
        "    gauth = GoogleAuth()\n",
        "    gauth.credentials = GoogleCredentials.get_application_default()\n",
        "    drive = GoogleDrive(gauth)\n",
        "    delete_file_by_name(drive, folderid, results_filename)\n",
        "    save_to_drive(drive, folderid, results_filename)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w7-911OnFJnP"
      },
      "source": [
        "if trivial:\n",
        "    PLOT_DIR = \"NewMetrics_clip_zs_Sigmoid_IndoorSceneRecognition\" + \"_0t\" + str(num_ways) + \"w\" + str(num_shot) + \"s\" + str(num_augmentations) + \"a_trivial_plots\"\n",
        "else:\n",
        "    PLOT_DIR = \"NewMetrics_clip_zs_Sigmoid_IndoorSceneRecognition\" + \"_0t\" + str(num_ways) + \"w\" + str(num_shot) + \"s\" + str(num_augmentations) + \"a_plots\"\n",
        "os.mkdir(PLOT_DIR)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Sht4dvfFJnP"
      },
      "source": [
        "all_metrics = ['accuracy', 'ap', 'map', 'c_f1', 'o_f1', 'c_precision', 'o_precision', 'c_recall', 'o_recall', 'top1_accuracy', 'top5_accuracy', 'classwise_accuracy', 'c_accuracy']\n",
        "\n",
        "final_dict = {}\n",
        "for ind_metric in all_metrics:\n",
        "  vals = []\n",
        "  final_array = []\n",
        "  for mt in clip_metrics_over_train_epochs:\n",
        "      ret_val = get_best_metric(mt,ind_metric,\"max\")\n",
        "      vals.append(ret_val)\n",
        "\n",
        "  final_array.append(vals)\n",
        "  final_dict[ind_metric] = final_array\n",
        "\n",
        "if trivial:\n",
        "    graph_filename = \"new_metrics\"+dataset_name+\"_0t\"+str(num_ways)+\"w\"+str(num_shot)+\"s\"+str(num_augmentations)+\"a_trivial_clip_zs_metrics_graphs.json\"\n",
        "else:\n",
        "    graph_filename = \"new_metrics\"+dataset_name+\"_0t\"+str(num_ways)+\"w\"+str(num_shot)+\"s\"+str(num_augmentations)+\"a_clip_zs_metrics_graphs.json\"\n",
        "\n",
        "with open(graph_filename, 'w') as f:\n",
        "        json.dump(final_dict, f)\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "delete_file_by_name(drive, folderid, graph_filename)\n",
        "save_to_drive(drive, folderid, graph_filename)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EWMwEGMLFJnQ"
      },
      "source": [
        "zip_dirname = PLOT_DIR + \".zip\"\n",
        "zip_source = PLOT_DIR\n",
        "! zip -r $zip_dirname $zip_source\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "save_to_drive(drive, folderid, zip_dirname)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RlIMyHeVfFEE"
      },
      "source": [
        "# 5 way 1 shot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7HPn06QE39N8"
      },
      "source": [
        "## Picking indices"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jup3-49v_iuU"
      },
      "source": [
        "num_ways = 5\n",
        "num_shot = 1\n",
        "num_eval = 19\n",
        "shuffle = False\n",
        "num_episodes = 100\n",
        "\n",
        "train_indices, eval_indices, wi_y, eval_y = prepare_indices(\n",
        "    num_ways, num_shot, num_eval, num_episodes, label_dictionary, val_labels, shuffle\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1GTEuYo1Fo27"
      },
      "source": [
        "embedding_model = embedding_models.CLIPEmbeddingWrapper()\n",
        "num_augmentations = 0\n",
        "trivial=False\n",
        "\n",
        "indices, embeddings = embed_images(\n",
        "    embedding_model, \n",
        "    train_indices, \n",
        "    num_augmentations,\n",
        "    trivial=trivial\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M9dYlVFDF_vs"
      },
      "source": [
        "## CLIP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2y6RpvnEF_vs"
      },
      "source": [
        "clip_embeddings_test_fn = \"clip\" + val_embeddings_filename_suffix\n",
        "clip_embeddings_test = get_ndarray_from_drive(drive, folderid, clip_embeddings_test_fn)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z6CQnw11FTIx"
      },
      "source": [
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "if trivial:\n",
        "  #cc\n",
        "    results_filename = \"new_metrics\"+dataset_name+\"_0t\"+str(num_ways)+\"w\"+str(num_shot)+\"s\"+str(num_augmentations)+\"a_trivial_clip_zs_metrics_with_logits.json\"\n",
        "else:\n",
        "  #cc\n",
        "    results_filename = \"new_metrics\"+dataset_name+\"_0t\"+str(num_ways)+\"w\"+str(num_shot)+\"s\"+str(num_augmentations)+\"a_clip_zs_metrics_with_logits.json\"\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "download_file_by_name(drive, folderid, results_filename)\n",
        "if results_filename in os.listdir():\n",
        "    with open(results_filename, 'r') as f:\n",
        "      #cc\n",
        "        json_loaded = json.load(f)\n",
        "        #cc\n",
        "        clip_metrics_over_train_epochs = json_loaded['metrics']\n",
        "        #cc\n",
        "        logits_over_train_epochs = json_loaded[\"logits\"]\n",
        "else:\n",
        "    clip_metrics_over_train_epochs = []  \n",
        "    #cc  \n",
        "    logits_over_train_epochs = []   \n",
        "\n",
        "train_epochs_arr = [0]\n",
        "multi_label=False\n",
        "# metrics_vals = ['hamming', 'jaccard', 'f1_score'] # ['accuracy', 'f1_score']\n",
        "\n",
        "for idx, train_epochs in enumerate(train_epochs_arr):\n",
        "    if idx < len(clip_metrics_over_train_epochs):\n",
        "        continue\n",
        "    print(train_epochs)\n",
        "    #cc\n",
        "    clip_metrics_thresholds, all_logits = run_evaluations(\n",
        "        (indices, embeddings),\n",
        "        train_indices,\n",
        "        eval_indices,\n",
        "        wi_y,\n",
        "        eval_y,\n",
        "        num_episodes,\n",
        "        num_ways,\n",
        "        class_names,\n",
        "        train_epochs=train_epochs,\n",
        "        num_augmentations=num_augmentations,\n",
        "        embeddings=clip_embeddings_test\n",
        "    )\n",
        "    clip_metrics_over_train_epochs.append(clip_metrics_thresholds)\n",
        "    #cc\n",
        "    logits_over_train_epochs.append(all_logits)\n",
        "\n",
        "#cc\n",
        "    fin_list = []\n",
        "    #cc the whole for loop\n",
        "    for a1 in wi_y:\n",
        "      fin_a1_list = []\n",
        "      for a2 in a1:\n",
        "        new_val = str(a2)\n",
        "        fin_a1_list.append(new_val)\n",
        "      fin_list.append(fin_a1_list)\n",
        "\n",
        "    with open(results_filename, 'w') as f:\n",
        "      #cc\n",
        "        results = {'metrics': clip_metrics_over_train_epochs,\n",
        "                   \"logits\": logits_over_train_epochs,\n",
        "                   \"true_labels\": fin_list}\n",
        "        json.dump(results, f)\n",
        "    auth.authenticate_user()\n",
        "    gauth = GoogleAuth()\n",
        "    gauth.credentials = GoogleCredentials.get_application_default()\n",
        "    drive = GoogleDrive(gauth)\n",
        "    delete_file_by_name(drive, folderid, results_filename)\n",
        "    save_to_drive(drive, folderid, results_filename)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qGo3102kFTIy"
      },
      "source": [
        "if trivial:\n",
        "    PLOT_DIR = \"NewMetrics_clip_zs_Sigmoid_IndoorSceneRecognition\" + \"_0t\" + str(num_ways) + \"w\" + str(num_shot) + \"s\" + str(num_augmentations) + \"a_trivial_plots\"\n",
        "else:\n",
        "    PLOT_DIR = \"NewMetrics_clip_zs_Sigmoid_IndoorSceneRecognition\" + \"_0t\" + str(num_ways) + \"w\" + str(num_shot) + \"s\" + str(num_augmentations) + \"a_plots\"\n",
        "os.mkdir(PLOT_DIR)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2bOX_AWEFTIz"
      },
      "source": [
        "all_metrics = ['accuracy', 'ap', 'map', 'c_f1', 'o_f1', 'c_precision', 'o_precision', 'c_recall', 'o_recall', 'top1_accuracy', 'top5_accuracy', 'classwise_accuracy', 'c_accuracy']\n",
        "\n",
        "final_dict = {}\n",
        "for ind_metric in all_metrics:\n",
        "  vals = []\n",
        "  final_array = []\n",
        "  for mt in clip_metrics_over_train_epochs:\n",
        "      ret_val = get_best_metric(mt,ind_metric,\"max\")\n",
        "      vals.append(ret_val)\n",
        "\n",
        "  final_array.append(vals)\n",
        "  final_dict[ind_metric] = final_array\n",
        "\n",
        "if trivial:\n",
        "    graph_filename = \"new_metrics\"+dataset_name+\"_0t\"+str(num_ways)+\"w\"+str(num_shot)+\"s\"+str(num_augmentations)+\"a_trivial_clip_zs_metrics_graphs.json\"\n",
        "else:\n",
        "    graph_filename = \"new_metrics\"+dataset_name+\"_0t\"+str(num_ways)+\"w\"+str(num_shot)+\"s\"+str(num_augmentations)+\"a_clip_zs_metrics_graphs.json\"\n",
        "\n",
        "with open(graph_filename, 'w') as f:\n",
        "        json.dump(final_dict, f)\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "delete_file_by_name(drive, folderid, graph_filename)\n",
        "save_to_drive(drive, folderid, graph_filename)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZjpIIX6FTIz"
      },
      "source": [
        "zip_dirname = PLOT_DIR + \".zip\"\n",
        "zip_source = PLOT_DIR\n",
        "! zip -r $zip_dirname $zip_source\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "save_to_drive(drive, folderid, zip_dirname)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBhFDrzlfTHf"
      },
      "source": [
        "# 13 way 1 shot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6u112WKcfTHg"
      },
      "source": [
        "## Picking indices"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-th0mOYQfTHg"
      },
      "source": [
        "num_ways = 13\n",
        "num_shot = 1\n",
        "num_eval = 10\n",
        "shuffle = False\n",
        "num_episodes = 100\n",
        "\n",
        "train_indices, eval_indices, wi_y, eval_y = prepare_indices(\n",
        "    num_ways, num_shot, num_eval, num_episodes, label_dictionary, val_labels, shuffle\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-wHhnx9F2v2"
      },
      "source": [
        "embedding_model = embedding_models.CLIPEmbeddingWrapper()\n",
        "num_augmentations = 0\n",
        "trivial=False\n",
        "\n",
        "indices, embeddings = embed_images(\n",
        "    embedding_model, \n",
        "    train_indices, \n",
        "    num_augmentations,\n",
        "    trivial=trivial\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sph7dLY0fTHh"
      },
      "source": [
        "## CLIP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0wBzaqB3fTHh"
      },
      "source": [
        "clip_embeddings_test_fn = \"clip\" + val_embeddings_filename_suffix\n",
        "clip_embeddings_test = get_ndarray_from_drive(drive, folderid, clip_embeddings_test_fn)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMTxo_ZaFXxM"
      },
      "source": [
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "if trivial:\n",
        "  #cc\n",
        "    results_filename = \"new_metrics\"+dataset_name+\"_0t\"+str(num_ways)+\"w\"+str(num_shot)+\"s\"+str(num_augmentations)+\"a_trivial_clip_zs_metrics_with_logits.json\"\n",
        "else:\n",
        "  #cc\n",
        "    results_filename = \"new_metrics\"+dataset_name+\"_0t\"+str(num_ways)+\"w\"+str(num_shot)+\"s\"+str(num_augmentations)+\"a_clip_zs_metrics_with_logits.json\"\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "download_file_by_name(drive, folderid, results_filename)\n",
        "if results_filename in os.listdir():\n",
        "    with open(results_filename, 'r') as f:\n",
        "      #cc\n",
        "        json_loaded = json.load(f)\n",
        "        #cc\n",
        "        clip_metrics_over_train_epochs = json_loaded['metrics']\n",
        "        #cc\n",
        "        logits_over_train_epochs = json_loaded[\"logits\"]\n",
        "else:\n",
        "    clip_metrics_over_train_epochs = []  \n",
        "    #cc  \n",
        "    logits_over_train_epochs = []   \n",
        "\n",
        "train_epochs_arr = [0]\n",
        "multi_label=False\n",
        "# metrics_vals = ['hamming', 'jaccard', 'f1_score'] # ['accuracy', 'f1_score']\n",
        "\n",
        "for idx, train_epochs in enumerate(train_epochs_arr):\n",
        "    if idx < len(clip_metrics_over_train_epochs):\n",
        "        continue\n",
        "    print(train_epochs)\n",
        "    #cc\n",
        "    clip_metrics_thresholds, all_logits = run_evaluations(\n",
        "        (indices, embeddings),\n",
        "        train_indices,\n",
        "        eval_indices,\n",
        "        wi_y,\n",
        "        eval_y,\n",
        "        num_episodes,\n",
        "        num_ways,\n",
        "        class_names,\n",
        "        train_epochs=train_epochs,\n",
        "        num_augmentations=num_augmentations,\n",
        "        embeddings=clip_embeddings_test\n",
        "    )\n",
        "    clip_metrics_over_train_epochs.append(clip_metrics_thresholds)\n",
        "    #cc\n",
        "    logits_over_train_epochs.append(all_logits)\n",
        "\n",
        "#cc\n",
        "    fin_list = []\n",
        "    #cc the whole for loop\n",
        "    for a1 in wi_y:\n",
        "      fin_a1_list = []\n",
        "      for a2 in a1:\n",
        "        new_val = str(a2)\n",
        "        fin_a1_list.append(new_val)\n",
        "      fin_list.append(fin_a1_list)\n",
        "\n",
        "    with open(results_filename, 'w') as f:\n",
        "      #cc\n",
        "        results = {'metrics': clip_metrics_over_train_epochs,\n",
        "                   \"logits\": logits_over_train_epochs,\n",
        "                   \"true_labels\": fin_list}\n",
        "        json.dump(results, f)\n",
        "    auth.authenticate_user()\n",
        "    gauth = GoogleAuth()\n",
        "    gauth.credentials = GoogleCredentials.get_application_default()\n",
        "    drive = GoogleDrive(gauth)\n",
        "    delete_file_by_name(drive, folderid, results_filename)\n",
        "    save_to_drive(drive, folderid, results_filename)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jeskwalZFXxN"
      },
      "source": [
        "if trivial:\n",
        "    PLOT_DIR = \"NewMetrics_clip_zs_Sigmoid_IndoorSceneRecognition\" + \"_0t\" + str(num_ways) + \"w\" + str(num_shot) + \"s\" + str(num_augmentations) + \"a_trivial_plots\"\n",
        "else:\n",
        "    PLOT_DIR = \"NewMetrics_clip_zs_Sigmoid_IndoorSceneRecognition\" + \"_0t\" + str(num_ways) + \"w\" + str(num_shot) + \"s\" + str(num_augmentations) + \"a_plots\"\n",
        "os.mkdir(PLOT_DIR)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_TYQpdqDFXxO"
      },
      "source": [
        "all_metrics = ['accuracy', 'ap', 'map', 'c_f1', 'o_f1', 'c_precision', 'o_precision', 'c_recall', 'o_recall', 'top1_accuracy', 'top5_accuracy', 'classwise_accuracy', 'c_accuracy']\n",
        "\n",
        "final_dict = {}\n",
        "for ind_metric in all_metrics:\n",
        "  vals = []\n",
        "  final_array = []\n",
        "  for mt in clip_metrics_over_train_epochs:\n",
        "      ret_val = get_best_metric(mt,ind_metric,\"max\")\n",
        "      vals.append(ret_val)\n",
        "\n",
        "  final_array.append(vals)\n",
        "  final_dict[ind_metric] = final_array\n",
        "\n",
        "if trivial:\n",
        "    graph_filename = \"new_metrics\"+dataset_name+\"_0t\"+str(num_ways)+\"w\"+str(num_shot)+\"s\"+str(num_augmentations)+\"a_trivial_clip_zs_metrics_graphs.json\"\n",
        "else:\n",
        "    graph_filename = \"new_metrics\"+dataset_name+\"_0t\"+str(num_ways)+\"w\"+str(num_shot)+\"s\"+str(num_augmentations)+\"a_clip_zs_metrics_graphs.json\"\n",
        "\n",
        "with open(graph_filename, 'w') as f:\n",
        "        json.dump(final_dict, f)\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "delete_file_by_name(drive, folderid, graph_filename)\n",
        "save_to_drive(drive, folderid, graph_filename)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4dkyC88hFXxO"
      },
      "source": [
        "zip_dirname = PLOT_DIR + \".zip\"\n",
        "zip_source = PLOT_DIR\n",
        "! zip -r $zip_dirname $zip_source\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "save_to_drive(drive, folderid, zip_dirname)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}