{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "new-repo-CLIP_UCF101_ZS_eval.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "RGlVnbugxFK2",
        "3rKe3HqM523g",
        "OLU-gp7n8__E",
        "9z1WQnXdLHy2",
        "UxTa8MVsvQCN",
        "tq-oEmFbThWq",
        "iLbRqaYxzbr7",
        "fJIGo_R86GQi",
        "g73bz0lqrU5e",
        "B5G7GsGDrU5u"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2wCVCAfpyE0A"
      },
      "source": [
        "# Mount Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4-7JU3Q7q_XT",
        "outputId": "73fb4aa1-1566-4310-b654-1a25398bc1a1"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vXAPOzOK4mTl",
        "outputId": "f2077a3c-5779-4d69-8e7b-d83d88e0ee47"
      },
      "source": [
        "!pip install -U -q PyDrive\n",
        "!pip install httplib2==0.15.0\n",
        "import os\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from pydrive.files import GoogleDriveFileList\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "from getpass import getpass\n",
        "import urllib\n",
        "\n",
        "# 1. Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "# Cloning CLIPPER to access modules.\n",
        "if 'CLIPPER' not in os.listdir():\n",
        "    cmd_string = 'git clone https://github.com/PAL-ML/CLIPPER.git'\n",
        "    os.system(cmd_string)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting httplib2==0.15.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/be/83/5e006e25403871ffbbf587c7aa4650158c947d46e89f2d50dcaf018464de/httplib2-0.15.0-py3-none-any.whl (94kB)\n",
            "\r\u001b[K     |███▌                            | 10kB 18.4MB/s eta 0:00:01\r\u001b[K     |███████                         | 20kB 22.7MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 30kB 26.1MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 40kB 21.3MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 51kB 12.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 61kB 10.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 71kB 11.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 81kB 12.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 92kB 13.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 102kB 7.2MB/s \n",
            "\u001b[?25hInstalling collected packages: httplib2\n",
            "  Found existing installation: httplib2 0.17.4\n",
            "    Uninstalling httplib2-0.17.4:\n",
            "      Successfully uninstalled httplib2-0.17.4\n",
            "Successfully installed httplib2-0.15.0\n",
            "Github User name: ashadhaz\n",
            "Password: ··········\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RGlVnbugxFK2"
      },
      "source": [
        "# Installation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gmgIrfT8hDNE"
      },
      "source": [
        "## Install multi label metrics dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b6xXPAFbe6Gp",
        "outputId": "9f4d3329-7ddd-499e-cfe3-8da4fae95f3e"
      },
      "source": [
        "! pip install scikit-learn==0.24"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting scikit-learn==0.24\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b1/ed/ab51a8da34d2b3f4524b21093081e7f9e2ddf1c9eac9f795dcf68ad0a57d/scikit_learn-0.24.0-cp37-cp37m-manylinux2010_x86_64.whl (22.3MB)\n",
            "\u001b[K     |████████████████████████████████| 22.3MB 1.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.24) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.24) (1.19.5)\n",
            "Collecting threadpoolctl>=2.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/f7/12/ec3f2e203afa394a149911729357aa48affc59c20e2c1c8297a60f33f133/threadpoolctl-2.1.0-py3-none-any.whl\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.24) (1.0.1)\n",
            "Installing collected packages: threadpoolctl, scikit-learn\n",
            "  Found existing installation: scikit-learn 0.22.2.post1\n",
            "    Uninstalling scikit-learn-0.22.2.post1:\n",
            "      Successfully uninstalled scikit-learn-0.22.2.post1\n",
            "Successfully installed scikit-learn-0.24.0 threadpoolctl-2.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3rKe3HqM523g"
      },
      "source": [
        "## Install CLIP dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "poS-WNDixIhY",
        "outputId": "c3942cbd-caec-4734-9de8-90758b6fc066"
      },
      "source": [
        "import subprocess\n",
        "\n",
        "CUDA_version = [s for s in subprocess.check_output([\"nvcc\", \"--version\"]).decode(\"UTF-8\").split(\", \") if s.startswith(\"release\")][0].split(\" \")[-1]\n",
        "print(\"CUDA version:\", CUDA_version)\n",
        "\n",
        "if CUDA_version == \"10.0\":\n",
        "    torch_version_suffix = \"+cu100\"\n",
        "elif CUDA_version == \"10.1\":\n",
        "    torch_version_suffix = \"+cu101\"\n",
        "elif CUDA_version == \"10.2\":\n",
        "    torch_version_suffix = \"\"\n",
        "else:\n",
        "    torch_version_suffix = \"+cu110\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CUDA version: 11.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uA-69W8M59nA",
        "outputId": "d3cba125-7114-4183-d7e9-b39f6d0d8d5f"
      },
      "source": [
        "! pip install torch==1.7.1{torch_version_suffix} torchvision==0.8.2{torch_version_suffix} -f https://download.pytorch.org/whl/torch_stable.html ftfy regex"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Collecting torch==1.7.1+cu110\n",
            "\u001b[?25l  Downloading https://download.pytorch.org/whl/cu110/torch-1.7.1%2Bcu110-cp37-cp37m-linux_x86_64.whl (1156.8MB)\n",
            "\u001b[K     |██████████████████              | 654.5MB 1.3MB/s eta 0:06:33\u001b[31mERROR: Exception:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/urllib3/response.py\", line 425, in _error_catcher\n",
            "    yield\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/urllib3/response.py\", line 507, in read\n",
            "    data = self._fp.read(amt) if not fp_closed else b\"\"\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/cachecontrol/filewrapper.py\", line 62, in read\n",
            "    data = self.__fp.read(amt)\n",
            "  File \"/usr/lib/python3.7/http/client.py\", line 461, in read\n",
            "    n = self.readinto(b)\n",
            "  File \"/usr/lib/python3.7/http/client.py\", line 505, in readinto\n",
            "    n = self.fp.readinto(b)\n",
            "  File \"/usr/lib/python3.7/socket.py\", line 589, in readinto\n",
            "    return self._sock.recv_into(b)\n",
            "  File \"/usr/lib/python3.7/ssl.py\", line 1071, in recv_into\n",
            "    return self.read(nbytes, buffer)\n",
            "  File \"/usr/lib/python3.7/ssl.py\", line 929, in read\n",
            "    return self._sslobj.read(len, buffer)\n",
            "ConnectionResetError: [Errno 104] Connection reset by peer\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/cli/base_command.py\", line 153, in _main\n",
            "    status = self.run(options, args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/commands/install.py\", line 382, in run\n",
            "    resolver.resolve(requirement_set)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/legacy_resolve.py\", line 201, in resolve\n",
            "    self._resolve_one(requirement_set, req)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/legacy_resolve.py\", line 365, in _resolve_one\n",
            "    abstract_dist = self._get_abstract_dist_for(req_to_install)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/legacy_resolve.py\", line 313, in _get_abstract_dist_for\n",
            "    req, self.session, self.finder, self.require_hashes\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/operations/prepare.py\", line 194, in prepare_linked_requirement\n",
            "    progress_bar=self.progress_bar\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/download.py\", line 465, in unpack_url\n",
            "    progress_bar=progress_bar\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/download.py\", line 316, in unpack_http_url\n",
            "    progress_bar)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/download.py\", line 551, in _download_http_url\n",
            "    _download_url(resp, link, content_file, hashes, progress_bar)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/download.py\", line 255, in _download_url\n",
            "    consume(downloaded_chunks)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/utils/misc.py\", line 641, in consume\n",
            "    deque(iterator, maxlen=0)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/download.py\", line 223, in written_chunks\n",
            "    for chunk in chunks:\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/utils/ui.py\", line 160, in iter\n",
            "    for x in it:\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/download.py\", line 212, in resp_read\n",
            "    decode_content=False):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/urllib3/response.py\", line 564, in stream\n",
            "    data = self.read(amt=amt, decode_content=decode_content)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/urllib3/response.py\", line 529, in read\n",
            "    raise IncompleteRead(self._fp_bytes_read, self.length_remaining)\n",
            "  File \"/usr/lib/python3.7/contextlib.py\", line 130, in __exit__\n",
            "    self.gen.throw(type, value, traceback)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/urllib3/response.py\", line 443, in _error_catcher\n",
            "    raise ProtocolError(\"Connection broken: %r\" % e, e)\n",
            "pip._vendor.urllib3.exceptions.ProtocolError: (\"Connection broken: ConnectionResetError(104, 'Connection reset by peer')\", ConnectionResetError(104, 'Connection reset by peer'))\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sYwBZS1N6A3d",
        "outputId": "a6167749-ad81-48e4-e92e-daa7f7f311df"
      },
      "source": [
        "! pip install ftfy regex\n",
        "! wget https://openaipublic.azureedge.net/clip/bpe_simple_vocab_16e6.txt.gz -O bpe_simple_vocab_16e6.txt.gz"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting ftfy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ce/b5/5da463f9c7823e0e575e9908d004e2af4b36efa8d02d3d6dad57094fcb11/ftfy-6.0.1.tar.gz (63kB)\n",
            "\r\u001b[K     |█████▏                          | 10kB 22.1MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 20kB 29.8MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 30kB 33.3MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 40kB 28.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 51kB 30.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 61kB 29.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 71kB 9.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (2019.12.20)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from ftfy) (0.2.5)\n",
            "Building wheels for collected packages: ftfy\n",
            "  Building wheel for ftfy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ftfy: filename=ftfy-6.0.1-cp37-none-any.whl size=41573 sha256=ca8c748e142ebbb1e5a3689611a1559896047a1e382cbe4bab31c7746274d0c0\n",
            "  Stored in directory: /root/.cache/pip/wheels/ae/73/c7/9056e14b04919e5c262fe80b54133b1a88d73683d05d7ac65c\n",
            "Successfully built ftfy\n",
            "Installing collected packages: ftfy\n",
            "Successfully installed ftfy-6.0.1\n",
            "--2021-05-19 07:30:46--  https://openaipublic.azureedge.net/clip/bpe_simple_vocab_16e6.txt.gz\n",
            "Resolving openaipublic.azureedge.net (openaipublic.azureedge.net)... 13.107.246.40, 13.107.213.40, 2620:1ec:bdf::40, ...\n",
            "Connecting to openaipublic.azureedge.net (openaipublic.azureedge.net)|13.107.246.40|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1356917 (1.3M) [application/octet-stream]\n",
            "Saving to: ‘bpe_simple_vocab_16e6.txt.gz’\n",
            "\n",
            "bpe_simple_vocab_16 100%[===================>]   1.29M  --.-KB/s    in 0.01s   \n",
            "\n",
            "2021-05-19 07:30:46 (97.2 MB/s) - ‘bpe_simple_vocab_16e6.txt.gz’ saved [1356917/1356917]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9oIcNBYB8lz3",
        "outputId": "dd93ae65-2850-4abf-b2b9-ac44e38ba16d"
      },
      "source": [
        "!pip install git+https://github.com/Sri-vatsa/CLIP # using this fork because of visualization capabilities"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/Sri-vatsa/CLIP\n",
            "  Cloning https://github.com/Sri-vatsa/CLIP to /tmp/pip-req-build-2oqzbdlv\n",
            "  Running command git clone -q https://github.com/Sri-vatsa/CLIP /tmp/pip-req-build-2oqzbdlv\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (6.0.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (2019.12.20)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (4.41.1)\n",
            "Collecting torch~=1.7.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/90/5d/095ddddc91c8a769a68c791c019c5793f9c4456a688ddd235d6670924ecb/torch-1.7.1-cp37-cp37m-manylinux1_x86_64.whl (776.8MB)\n",
            "\u001b[K     |████████████████████████████████| 776.8MB 23kB/s \n",
            "\u001b[?25hCollecting torchvision~=0.8.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/94/df/969e69a94cff1c8911acb0688117f95e1915becc1e01c73e7960a2c76ec8/torchvision-0.8.2-cp37-cp37m-manylinux1_x86_64.whl (12.8MB)\n",
            "\u001b[K     |████████████████████████████████| 12.8MB 40.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from ftfy->clip==1.0) (0.2.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch~=1.7.1->clip==1.0) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch~=1.7.1->clip==1.0) (3.7.4.3)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision~=0.8.2->clip==1.0) (7.1.2)\n",
            "Building wheels for collected packages: clip\n",
            "  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for clip: filename=clip-1.0-cp37-none-any.whl size=1368623 sha256=343828f4b2cfa62190d8d2ad6ecfc130a48ed00451ee407debd45d0f71e5bf9b\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-nrwj0e_b/wheels/cc/55/69/0d411dabbd5009fd069d47b47cf7839c54e595dc61725b307b\n",
            "Successfully built clip\n",
            "\u001b[31mERROR: torchtext 0.9.1 has requirement torch==1.8.1, but you'll have torch 1.7.1 which is incompatible.\u001b[0m\n",
            "Installing collected packages: torch, torchvision, clip\n",
            "  Found existing installation: torch 1.8.1+cu101\n",
            "    Uninstalling torch-1.8.1+cu101:\n",
            "      Successfully uninstalled torch-1.8.1+cu101\n",
            "  Found existing installation: torchvision 0.9.1+cu101\n",
            "    Uninstalling torchvision-0.9.1+cu101:\n",
            "      Successfully uninstalled torchvision-0.9.1+cu101\n",
            "Successfully installed clip-1.0 torch-1.7.1 torchvision-0.8.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OLU-gp7n8__E"
      },
      "source": [
        "## Install clustering dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6TLg9ozo9Hvc"
      },
      "source": [
        "!pip -q install umap-learn>=0.3.7"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9z1WQnXdLHy2"
      },
      "source": [
        "## Install dataset manager dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J1vvMx7_LLSp",
        "outputId": "9547b5cb-7a0c-4b34-f5c5-bfdd4bd26e4c"
      },
      "source": [
        "!pip install wget"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting wget\n",
            "  Downloading https://files.pythonhosted.org/packages/47/6a/62e288da7bcda82b935ff0c6cfe542970f04e29c756b0e147251b2fb251f/wget-3.2.zip\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-cp37-none-any.whl size=9681 sha256=ce12b81a1f35e972351842f29cf1e2fb3bea863d443030e475fd630ada2c7a21\n",
            "  Stored in directory: /root/.cache/pip/wheels/40/15/30/7d8f7cea2902b4db79e3fea550d7d7b85ecb27ef992b618f3f\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NzsubsEm72rr"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KZI62a6G74kw"
      },
      "source": [
        "# ML Libraries\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "import keras\n",
        "\n",
        "# Data processing\n",
        "import PIL\n",
        "import base64\n",
        "import imageio\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "\n",
        "from PIL import Image\n",
        "import cv2\n",
        "import imgaug.augmenters as iaa\n",
        "\n",
        "# Plotting\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from IPython.core.display import display, HTML\n",
        "from matplotlib import cm\n",
        "\n",
        "# Models\n",
        "import clip\n",
        "\n",
        "# Datasets\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "# Misc\n",
        "import progressbar\n",
        "import logging\n",
        "from abc import ABC, abstractmethod\n",
        "import time\n",
        "import urllib.request\n",
        "import os\n",
        "import itertools\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Modules\n",
        "from CLIPPER.code.ExperimentModules import embedding_models\n",
        "from CLIPPER.code.ExperimentModules import simclr_data_augmentations\n",
        "from CLIPPER.code.ExperimentModules.dataset_manager import DatasetManager\n",
        "from CLIPPER.code.ExperimentModules.clip_few_shot import CLIPFewShotClassifier\n",
        "from CLIPPER.code.ExperimentModules.utils import (save_npy, load_npy, \n",
        "                                                       get_folder_id, \n",
        "                                                       create_expt_dir, \n",
        "                                                       save_to_drive, \n",
        "                                                       load_all_from_drive_folder, \n",
        "                                                       download_file_by_name, \n",
        "                                                       delete_file_by_name)\n",
        "\n",
        "logging.getLogger('googleapicliet.discovery_cache').setLevel(logging.ERROR)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zU_gxQ0KbwMh"
      },
      "source": [
        "# Initialization & Constants\n",
        "\n",
        "**Edited**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bih5tBPdbx3u",
        "outputId": "00ec84d1-6ce1-45e9-8390-fde8d4ae39db"
      },
      "source": [
        "dataset_name = 'UCF101'\n",
        "folder_name = \"UCF101-Embeddings-28-02-21\"\n",
        "\n",
        "# Change parentid to match that of experiments root folder in gdrive\n",
        "parentid = '1bK72W-Um20EQDEyChNhNJthUNbmoSEjD'\n",
        "\n",
        "# Filepaths\n",
        "train_labels_filename = \"train_labels.npz\"\n",
        "val_labels_filename = \"val_labels.npz\"\n",
        "\n",
        "train_embeddings_filename_suffix = \"_embeddings_train.npz\"\n",
        "val_embeddings_filename_suffix = \"_embeddings_val.npz\"\n",
        "\n",
        "\n",
        "# Initialize sepcific experiment folder in drive\n",
        "folderid = create_expt_dir(drive, parentid, folder_name)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "title: UCF101-Embeddings-28-02-21, id: 1AR8qp1QPFv0M3jCIQKK8VA2xsNilo_pi\n",
            "Experiment folder already exists. WARNING: Following with this run might overwrite existing results stored.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UxTa8MVsvQCN"
      },
      "source": [
        "# Load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n6S124Jfwuu5",
        "outputId": "366e67f9-b8d2-408e-a85f-d4be43f27d5f"
      },
      "source": [
        "def get_ndarray_from_drive(drive, folderid, filename):\n",
        "    download_file_by_name(drive, folderid, filename)\n",
        "    return np.load(filename)['data']\n",
        "\n",
        "# train_labels = get_ndarray_from_drive(drive, folderid, train_labels_filename)\n",
        "val_labels = get_ndarray_from_drive(drive, folderid, val_labels_filename)\n",
        "# test_labels = get_ndarray_from_drive(drive, folderid, test_labels_filename)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading val_labels.npz from GDrive\n",
            "Downloading val_labels.npz from GDrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4p2JpaGHmqbl",
        "outputId": "381f7cfe-2244-431d-9882-3126d7d2dc81"
      },
      "source": [
        "dm = DatasetManager()\n",
        "\n",
        "test_data_generator = dm.load_dataset('ucf_101', split='val')\n",
        "\n",
        "class_names = dm.get_class_names()\n",
        "print(class_names)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1NwERNU-266cCK_uofbEXraArleazADRS\n",
            "To: /content/UCF_Images.zip\n",
            "218MB [00:01, 178MB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Found 13320 files belonging to 101 classes.\n",
            "Using 2664 files for validation.\n",
            "['Apply Eye Makeup', 'Apply Lipstick', 'Archery', 'Baby Crawling', 'Balance Beam', 'Band Marching', 'Baseball Pitch', 'Basketball', 'Basketball Dunk', 'Bench Press', 'Biking', 'Billiards', 'Blow Dry Hair', 'Blowing Candles', 'Body Weight Squats', 'Bowling', 'Boxing Punching Bag', 'Boxing Speed Bag', 'Breast Stroke', 'Brushing Teeth', 'Clean And Jerk', 'Cliff Diving', 'Cricket Bowling', 'Cricket Shot', 'Cutting In Kitchen', 'Diving', 'Drumming', 'Fencing', 'Field Hockey Penalty', 'Floor Gymnastics', 'Frisbee Catch', 'Front Crawl', 'Golf Swing', 'Haircut', 'Hammer Throw', 'Hammering', 'Handstand Pushups', 'Handstand Walking', 'Head Massage', 'High Jump', 'Horse Race', 'Horse Riding', 'Hula Hoop', 'Ice Dancing', 'Javelin Throw', 'Juggling Balls', 'Jump Rope', 'Jumping Jack', 'Kayaking', 'Knitting', 'Long Jump', 'Lunges', 'Military Parade', 'Mixing', 'Mopping Floor', 'Nunchucks', 'Parallel Bars', 'Pizza Tossing', 'Playing Cello', 'Playing Daf', 'Playing Dhol', 'Playing Flute', 'Playing Guitar', 'Playing Piano', 'Playing Sitar', 'Playing Tabla', 'Playing Violin', 'Pole Vault', 'Pommel Horse', 'Pull Ups', 'Punch', 'Push Ups', 'Rafting', 'Rock Climbing Indoor', 'Rope Climbing', 'Rowing', 'Salsa Spin', 'Shaving Beard', 'Shotput', 'Skate Boarding', 'Skiing', 'Skijet', 'Sky Diving', 'Soccer Juggling', 'Soccer Penalty', 'Still Rings', 'Sumo Wrestling', 'Surfing', 'Swing', 'Table Tennis Shot', 'Tai Chi', 'Tennis Swing', 'Throw Discus', 'Trampoline Jumping', 'Typing', 'Uneven Bars', 'Volleyball Spiking', 'Walking With Dog', 'Wall Pushups', 'Writing On Board', 'Yo Yo']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tq-oEmFbThWq"
      },
      "source": [
        "# Label dicts"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "54SEXo03TpA_"
      },
      "source": [
        "class_names = dm.get_class_names()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iLbRqaYxzbr7"
      },
      "source": [
        "# Create label dictionary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Emz85fNX0Vif",
        "outputId": "275ee70d-ad1d-4a1f-ff40-bb03bda90e31"
      },
      "source": [
        "unique_labels = np.unique(val_labels)\n",
        "print(len(unique_labels))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "22\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0TubS-RLzeVM"
      },
      "source": [
        "label_dictionary = {la:[] for la in unique_labels}\n",
        "\n",
        "for i in range(len(val_labels)):\n",
        "    la = val_labels[i]\n",
        "\n",
        "    label_dictionary[la].append(i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xP6ftkDCvKul"
      },
      "source": [
        "# CLIP zero shot eval"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJIGo_R86GQi"
      },
      "source": [
        "## Function definitions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pv68q8TH6tvG"
      },
      "source": [
        "def start_progress_bar(bar_len):\n",
        "    widgets = [\n",
        "        ' [', \n",
        "        progressbar.Timer(format= 'elapsed time: %(elapsed)s'), \n",
        "        '] ', \n",
        "        progressbar.Bar('*'),' (', \n",
        "        progressbar.ETA(), ') ', \n",
        "        ]\n",
        "    pbar = progressbar.ProgressBar(\n",
        "        max_value=bar_len, widgets=widgets\n",
        "        ).start()\n",
        "\n",
        "    return pbar"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DtoopzLRStTO"
      },
      "source": [
        "def prepare_indices(\n",
        "    num_ways,\n",
        "    num_shot,\n",
        "    num_eval,\n",
        "    num_episodes,\n",
        "    label_dictionary,\n",
        "    test_labels,\n",
        "    shuffle=False\n",
        "):\n",
        "    eval_indices = []\n",
        "    train_indices = []\n",
        "    wi_y = []\n",
        "    eval_y = []\n",
        "\n",
        "    label_dictionary = {la:label_dictionary[la] for la in label_dictionary if len(label_dictionary[la]) >= (num_shot+num_eval)}\n",
        "    unique_labels = list(label_dictionary.keys())\n",
        "\n",
        "    pbar = start_progress_bar(num_episodes)\n",
        "\n",
        "    for s in range(num_episodes):\n",
        "        # Setting random seed for replicability\n",
        "        np.random.seed(s)\n",
        "\n",
        "        _train_indices = []\n",
        "        _eval_indices = []\n",
        "\n",
        "        selected_labels = np.random.choice(unique_labels, size=num_ways, replace=False)\n",
        "        for la in selected_labels:\n",
        "            la_indices = label_dictionary[la]\n",
        "            select = np.random.choice(la_indices, size = num_shot+num_eval, replace=False)\n",
        "            tr_idx = list(select[:num_shot])\n",
        "            ev_idx = list(select[num_shot:])\n",
        "\n",
        "            _train_indices = _train_indices + tr_idx\n",
        "            _eval_indices = _eval_indices + ev_idx\n",
        "\n",
        "        if shuffle:\n",
        "            np.random.shuffle(_train_indices)\n",
        "            np.random.shuffle(_eval_indices)\n",
        "\n",
        "        train_indices.append(_train_indices)\n",
        "        eval_indices.append(_eval_indices)\n",
        "\n",
        "        _wi_y = test_labels[_train_indices]\n",
        "        _eval_y = test_labels[_eval_indices]\n",
        "\n",
        "        wi_y.append(_wi_y)\n",
        "        eval_y.append(_eval_y)\n",
        "\n",
        "        pbar.update(s+1)\n",
        "        \n",
        "    return train_indices, eval_indices, wi_y, eval_y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4M5aHABX7k8a"
      },
      "source": [
        "def embed_images(\n",
        "    embedding_model, \n",
        "    train_indices, \n",
        "    num_augmentations,\n",
        "    trivial=False\n",
        "    ):\n",
        "\n",
        "    def augment_image(image, num_augmentations, trivial):\n",
        "        \"\"\" Perform SimCLR augmentations on the image\n",
        "        \"\"\"\n",
        "\n",
        "        if np.max(image) > 1:\n",
        "            image = image/255\n",
        "\n",
        "        augmented_images = [image]\n",
        "\n",
        "        # augmentations = iaa.Sequential([\n",
        "        #     iaa.Affine(\n",
        "        #         translate_percent={'x':(-0.1, 0.1), 'y':(-0.1, 0.1)}, \n",
        "        #         rotate=(-15, 15),\n",
        "        #         shear=(-15, 15),\n",
        "        #     ),\n",
        "        #     iaa.Fliplr(0.5) \n",
        "        # ])\n",
        "\n",
        "        def _run_filters(image):\n",
        "            width = image.shape[1]\n",
        "            height = image.shape[0]\n",
        "            image_aug = simclr_data_augmentations.random_crop_with_resize(\n",
        "                image,\n",
        "                height,\n",
        "                width\n",
        "                )\n",
        "            image_aug = tf.image.random_flip_left_right(image_aug)\n",
        "            image_aug = simclr_data_augmentations.random_color_jitter(image_aug)\n",
        "            image_aug = simclr_data_augmentations.random_blur(\n",
        "                image_aug,\n",
        "                height,\n",
        "                width\n",
        "                )\n",
        "            image_aug = tf.reshape(image_aug, [image.shape[0], image.shape[1], 3])\n",
        "            image_aug = tf.clip_by_value(image_aug, 0., 1.)\n",
        "\n",
        "            return image_aug.numpy()\n",
        "\n",
        "        for _ in range(num_augmentations):\n",
        "            # aug_image = augmentations(image=image)\n",
        "\n",
        "            if trivial:\n",
        "                aug_image = image\n",
        "            else:\n",
        "                aug_image = _run_filters(image)\n",
        "            augmented_images.append(aug_image)\n",
        "\n",
        "        augmented_images = np.stack(augmented_images)\n",
        "        return augmented_images\n",
        "\n",
        "    embedding_model.load_model()\n",
        "\n",
        "    unique_indices = np.unique(np.array(train_indices))\n",
        "\n",
        "    ds = dm.load_dataset('ucf_101', split='val')\n",
        "    embeddings = []\n",
        "    IMAGE_IDX = 0\n",
        "    pbar = start_progress_bar(unique_indices.size+1)\n",
        "    num_done=0\n",
        "    for idx, item in enumerate(ds):\n",
        "        if idx in unique_indices:\n",
        "            image = item[IMAGE_IDX][0]\n",
        "            if num_augmentations > 0:\n",
        "                aug_images = augment_image(image, num_augmentations, trivial)\n",
        "            else:\n",
        "                aug_images = image\n",
        "            \n",
        "            processed_images = embedding_model.preprocess_data(aug_images)\n",
        "            embedding = embedding_model.embed_images(processed_images)\n",
        "            embeddings.append(embedding)\n",
        "            \n",
        "            num_done += 1\n",
        "            pbar.update(num_done+1)\n",
        "\n",
        "    embeddings = np.stack(embeddings)\n",
        "\n",
        "    return unique_indices, embeddings"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tyW-G0AD5_rn"
      },
      "source": [
        "def evaluate_model_for_episode(\n",
        "    model,\n",
        "    eval_x,\n",
        "    eval_y,\n",
        "    label_mapping,\n",
        "    filtered_classes,\n",
        "    metrics=['hamming', 'jaccard', 'subset_accuracy', 'ap', 'map', 'c_f1', 'o_f1', 'c_precision', 'o_precision', 'c_recall', 'o_recall', 'top1_accuracy', 'top5_accuracy', 'classwise_accuracy', 'c_accuracy'],\n",
        "    multi_label=True\n",
        "):\n",
        "    zs_weights = model.zeroshot_classifier(filtered_classes)\n",
        "    logits = model.predict_scores(eval_x, zs_weights).tolist()\n",
        "    \n",
        "    pred_y = model.predict_label(eval_x, zs_weights)\n",
        "    pred_y = [label_mapping[l] for l in pred_y]\n",
        "\n",
        "    met = model.evaluate_single_label_metrics(\n",
        "            eval_x, eval_y, label_mapping, zs_weights, metrics\n",
        "            )\n",
        "\n",
        "    return pred_y, met, logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l2cWOf7SQF66"
      },
      "source": [
        "def get_label_mapping_n_class_names(eval_y, class_names):\n",
        "  label_mapping = {}\n",
        "  unique_labels = np.unique(eval_y)\n",
        "  filtered_classes = [class_names[x] for x in unique_labels]\n",
        "\n",
        "  num_classes = len(unique_labels)\n",
        "  for c in range(num_classes):\n",
        "    label_mapping[c] = unique_labels[c]\n",
        "\n",
        "  return label_mapping, filtered_classes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-H9Kex59foo1"
      },
      "source": [
        "# chenni change\n",
        "def run_episode_through_model(\n",
        "    indices_and_embeddings,\n",
        "    train_indices, \n",
        "    eval_indices, \n",
        "    wi_y, \n",
        "    eval_y,\n",
        "    class_names,\n",
        "    num_augmentations=0,\n",
        "    train_epochs=None,\n",
        "    train_batch_size=5,\n",
        "    metrics=['hamming', 'jaccard', 'subset_accuracy', 'ap', 'map', 'c_f1', 'o_f1', 'c_precision', 'o_precision', 'c_recall', 'o_recall', 'top1_accuracy', 'top5_accuracy', 'classwise_accuracy', 'c_accuracy'],\n",
        "    embeddings=None,\n",
        "    multi_label=True\n",
        "):\n",
        "    metrics_values = {m:[] for m in metrics}\n",
        "    indices_and_embeddings\n",
        "    eval_x = embeddings[eval_indices]\n",
        "    ep_logits = []\n",
        "    \n",
        "    label_mapping, filtered_classes = get_label_mapping_n_class_names(eval_y, class_names)\n",
        "\n",
        "    clip_fs_parameters = {\n",
        "        \"num_classes\": num_ways,\n",
        "        \"input_dims\": eval_x.shape[-1],\n",
        "        \"multi_label\": multi_label\n",
        "    }\n",
        "\n",
        "    clip_fs_cls = CLIPFewShotClassifier(clip_fs_parameters)\n",
        "    \n",
        "    pred_labels, metrics_values, logits = evaluate_model_for_episode(\n",
        "            clip_fs_cls,\n",
        "            eval_x,\n",
        "            eval_y,\n",
        "            label_mapping,\n",
        "            filtered_classes,\n",
        "            metrics=metrics,\n",
        "            multi_label=False\n",
        "    )\n",
        "    ep_logits = logits\n",
        "#cc\n",
        "    return metrics_values, ep_logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mUpPZ6ti6IEp"
      },
      "source": [
        "def run_evaluations(\n",
        "    indices_and_embeddings,\n",
        "    train_indices, \n",
        "    eval_indices, \n",
        "    wi_y, \n",
        "    eval_y, \n",
        "    num_episodes, \n",
        "    num_ways,\n",
        "    class_names,\n",
        "    verbose=True,\n",
        "    normalize=True,\n",
        "    train_epochs=None,\n",
        "    train_batch_size=5,\n",
        "    metrics=['accuracy', 'ap', 'map', 'c_f1', 'o_f1', 'c_precision', 'o_precision', 'c_recall', 'o_recall', 'top1_accuracy', 'top5_accuracy', 'classwise_accuracy', 'c_accuracy'],\n",
        "    embeddings=None,\n",
        "    num_augmentations=0,\n",
        "    multi_label=True\n",
        "):\n",
        "    metrics_values = {m:[] for m in metrics}\n",
        "\n",
        "    all_logits = []\n",
        "\n",
        "    if verbose:\n",
        "        pbar = start_progress_bar(num_episodes)\n",
        "\n",
        "    for idx_ep in range(num_episodes):\n",
        "        _train_indices = train_indices[idx_ep]\n",
        "        _eval_indices = eval_indices[idx_ep]\n",
        "        _wi_y = [label for label in wi_y[idx_ep]]\n",
        "        _eval_y = [label for label in eval_y[idx_ep]]\n",
        "\n",
        "        met, ep_logits = run_episode_through_model(\n",
        "            indices_and_embeddings,\n",
        "            _train_indices, \n",
        "            _eval_indices, \n",
        "            _wi_y, \n",
        "            _eval_y,\n",
        "            class_names,\n",
        "            num_augmentations=num_augmentations,\n",
        "            train_epochs=train_epochs,\n",
        "            train_batch_size=train_batch_size,\n",
        "            embeddings=embeddings,\n",
        "            metrics=metrics,\n",
        "            multi_label=multi_label\n",
        "        )\n",
        "\n",
        "        all_logits.append(ep_logits)\n",
        "\n",
        "        for m in metrics:\n",
        "            metrics_values[m].append(met[m])\n",
        "\n",
        "        if verbose:\n",
        "            pbar.update(idx_ep+1)\n",
        "\n",
        "    return metrics_values, all_logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pVO25nDT88qK"
      },
      "source": [
        "def get_best_metric(mt, metric_name, optimal='max'):\n",
        "    if optimal=='max':\n",
        "        opt_value = np.max(np.mean(np.array(mt[metric_name]), axis=0))\n",
        "    if optimal=='min':\n",
        "        opt_value = np.min(np.mean(np.array(mt[metric_name]), axis=0))\n",
        "\n",
        "    return opt_value"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g8CNr6I4rU5d"
      },
      "source": [
        "# 5 way 5 shot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g73bz0lqrU5e"
      },
      "source": [
        "## Picking indices"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fm6j-e0brU5f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c261992-14a4-4e80-bcf8-f944fddcf9ba"
      },
      "source": [
        "num_ways = 5\n",
        "num_shot = 5\n",
        "num_eval = 15\n",
        "shuffle = False\n",
        "num_episodes = 100\n",
        "\n",
        "train_indices, eval_indices, wi_y, eval_y = prepare_indices(\n",
        "    num_ways, num_shot, num_eval, num_episodes, label_dictionary, val_labels, shuffle\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r                                                                               \r\r [elapsed time: 0:00:00] |                                  | (ETA:  --:--:--) "
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X5Ov81G5BUJe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd8c2abe-8db4-48a1-fc58-cafebaf3b918"
      },
      "source": [
        "embedding_model = embedding_models.CLIPEmbeddingWrapper()\n",
        "num_augmentations = 0\n",
        "trivial=False\n",
        "\n",
        "indices, embeddings = embed_images(\n",
        "    embedding_model, \n",
        "    train_indices, \n",
        "    num_augmentations,\n",
        "    trivial=trivial\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r                                                                               \r\r [elapsed time: 0:00:00] |                                  | (ETA:  --:--:--) "
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Found 13320 files belonging to 101 classes.\n",
            "Using 2664 files for validation.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " [elapsed time: 0:00:45] |**********************************| (ETA:  00:00:00) "
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lLW-8vrbrU5f"
      },
      "source": [
        "## CLIP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ghmre8xB5Tft",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d31f6181-019d-49b7-ab88-f3ebf426d75e"
      },
      "source": [
        "clip_embeddings_test_fn = \"clip\" + val_embeddings_filename_suffix\n",
        "clip_embeddings_test = get_ndarray_from_drive(drive, folderid, clip_embeddings_test_fn)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading clip_embeddings_val.npz from GDrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XGTJsU6yN0S0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11ef105d-b3c5-4daa-e1a4-651084c3ce66"
      },
      "source": [
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "if trivial:\n",
        "  #cc\n",
        "    results_filename = \"new_metrics\"+dataset_name+\"_0t\"+str(num_ways)+\"w\"+str(num_shot)+\"s\"+str(num_augmentations)+\"a_trivial_clip_zs_metrics_with_logits.json\"\n",
        "else:\n",
        "  #cc\n",
        "    results_filename = \"new_metrics\"+dataset_name+\"_0t\"+str(num_ways)+\"w\"+str(num_shot)+\"s\"+str(num_augmentations)+\"a_clip_zs_metrics_with_logits.json\"\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "download_file_by_name(drive, folderid, results_filename)\n",
        "if results_filename in os.listdir():\n",
        "    with open(results_filename, 'r') as f:\n",
        "      #cc\n",
        "        json_loaded = json.load(f)\n",
        "        #cc\n",
        "        clip_metrics_over_train_epochs = json_loaded['metrics']\n",
        "        #cc\n",
        "        logits_over_train_epochs = json_loaded[\"logits\"]\n",
        "else:\n",
        "    clip_metrics_over_train_epochs = []  \n",
        "    logits_over_train_epochs = []   \n",
        "\n",
        "train_epochs_arr = [0]\n",
        "multi_label=False\n",
        "# metrics_vals = ['hamming', 'jaccard', 'f1_score'] # ['accuracy', 'f1_score']\n",
        "\n",
        "for idx, train_epochs in enumerate(train_epochs_arr):\n",
        "    if idx < len(clip_metrics_over_train_epochs):\n",
        "        continue\n",
        "    print(train_epochs)\n",
        "    #cc\n",
        "    clip_metrics_thresholds, all_logits = run_evaluations(\n",
        "        (indices, embeddings),\n",
        "        train_indices,\n",
        "        eval_indices,\n",
        "        wi_y,\n",
        "        eval_y,\n",
        "        num_episodes,\n",
        "        num_ways,\n",
        "        class_names,\n",
        "        train_epochs=train_epochs,\n",
        "        num_augmentations=num_augmentations,\n",
        "        embeddings=clip_embeddings_test\n",
        "    )\n",
        "    clip_metrics_over_train_epochs.append(clip_metrics_thresholds)\n",
        "    #cc\n",
        "    logits_over_train_epochs.append(all_logits)\n",
        "\n",
        "#cc\n",
        "    fin_list = []\n",
        "    #cc the whole for loop\n",
        "    for a1 in wi_y:\n",
        "      fin_a1_list = []\n",
        "      for a2 in a1:\n",
        "        new_val = str(a2)\n",
        "        fin_a1_list.append(new_val)\n",
        "      fin_list.append(fin_a1_list)\n",
        "\n",
        "    with open(results_filename, 'w') as f:\n",
        "      #cc\n",
        "        results = {'metrics': clip_metrics_over_train_epochs,\n",
        "                   \"logits\": logits_over_train_epochs,\n",
        "                   \"true_labels\": fin_list}\n",
        "        json.dump(results, f)\n",
        "    auth.authenticate_user()\n",
        "    gauth = GoogleAuth()\n",
        "    gauth.credentials = GoogleCredentials.get_application_default()\n",
        "    drive = GoogleDrive(gauth)\n",
        "    #delete_file_by_name(drive, folderid, results_filename)\n",
        "    #save_to_drive(drive, folderid, results_filename)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r                                                                               \r\r [elapsed time: 0:00:00] |                                  | (ETA:  --:--:--) "
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " [elapsed time: 0:10:31] |**********************************| (ETA:  00:00:00) "
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V8uyo_T8rU5h"
      },
      "source": [
        "if trivial:\n",
        "    PLOT_DIR = \"NewMetrics_clip_zs_Sigmoid_\"+ dataset_name + \"_0t\" + str(num_ways) + \"w\" + str(num_shot) + \"s\" + str(num_augmentations) + \"a_trivial_plots\"\n",
        "else:\n",
        "    PLOT_DIR = \"NewMetrics_clip_zs_Sigmoid_\"+ dataset_name + \"_0t\" + str(num_ways) + \"w\" + str(num_shot) + \"s\" + str(num_augmentations) + \"a_plots\"\n",
        "os.mkdir(PLOT_DIR)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QTID6tl2rFhn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37604c30-2f99-4652-a14f-435437176b87"
      },
      "source": [
        "# chenni change whole block\n",
        "all_metrics = ['accuracy', 'ap', 'map', 'c_f1', 'o_f1', 'c_precision', 'o_precision', 'c_recall', 'o_recall', 'top1_accuracy', 'top5_accuracy', 'classwise_accuracy', 'c_accuracy']\n",
        "\n",
        "final_dict = {}\n",
        "for ind_metric in all_metrics:\n",
        "  vals = []\n",
        "  final_array = []\n",
        "  for mt in clip_metrics_over_train_epochs:\n",
        "      ret_val = get_best_metric(mt,ind_metric,\"max\")\n",
        "      vals.append(ret_val)\n",
        "\n",
        "  final_array.append(vals)\n",
        "  final_dict[ind_metric] = final_array\n",
        "\n",
        "if trivial:\n",
        "    graph_filename = \"new_metrics\"+dataset_name+\"_0t\"+str(num_ways)+\"w\"+str(num_shot)+\"s\"+str(num_augmentations)+\"a_trivial_clip_zs_metrics_graphs.json\"\n",
        "else:\n",
        "    graph_filename = \"new_metrics\"+dataset_name+\"_0t\"+str(num_ways)+\"w\"+str(num_shot)+\"s\"+str(num_augmentations)+\"a_clip_zs_metrics_graphs.json\"\n",
        "\n",
        "with open(graph_filename, 'w') as f:\n",
        "        json.dump(final_dict, f)\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "delete_file_by_name(drive, folderid, graph_filename)\n",
        "save_to_drive(drive, folderid, graph_filename)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uploaded new_metricsUCF101_0t5w5s0a_clip_zs_metrics_graphs.json to https://drive.google.com/drive/u/1/folders/1AR8qp1QPFv0M3jCIQKK8VA2xsNilo_pi\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FRc9bHsURFDX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a056e2ee-5967-44d9-fae2-912d15eac3e3"
      },
      "source": [
        "zip_dirname = PLOT_DIR + \".zip\"\n",
        "zip_source = PLOT_DIR\n",
        "! zip -r $zip_dirname $zip_source\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "save_to_drive(drive, folderid, zip_dirname)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  adding: NewMetrics_clip_zs_Sigmoid_MiniImagenet_0t5w5s0a_plots/ (stored 0%)\n",
            "Uploaded NewMetrics_clip_zs_Sigmoid_MiniImagenet_0t5w5s0a_plots.zip to https://drive.google.com/drive/u/1/folders/1AR8qp1QPFv0M3jCIQKK8VA2xsNilo_pi\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vrit1GAzrU5u"
      },
      "source": [
        "# 20 way 5 shot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5G7GsGDrU5u"
      },
      "source": [
        "## Picking indices"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yvDuWsZerU5u",
        "outputId": "986dded4-7f40-4430-e3e0-893ab11b3eec"
      },
      "source": [
        "num_ways = 20\n",
        "num_shot = 5\n",
        "num_eval = 5\n",
        "shuffle = False\n",
        "num_episodes = 100\n",
        "\n",
        "train_indices, eval_indices, wi_y, eval_y = prepare_indices(\n",
        "    num_ways, num_shot, num_eval, num_episodes, label_dictionary, val_labels, shuffle\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " [elapsed time: 0:00:00] |******************************    | (ETA:   0:00:00) "
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eLK5V-xFE6V9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8f248f3-ae8a-4ab8-9e60-7cb8a9f26bdd"
      },
      "source": [
        "embedding_model = embedding_models.CLIPEmbeddingWrapper()\n",
        "num_augmentations = 0\n",
        "trivial=False\n",
        "\n",
        "indices, embeddings = embed_images(\n",
        "    embedding_model, \n",
        "    train_indices, \n",
        "    num_augmentations,\n",
        "    trivial=trivial\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r                                                                               \r\r [elapsed time: 0:00:00] |                                  | (ETA:  --:--:--) "
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Found 13320 files belonging to 101 classes.\n",
            "Using 2664 files for validation.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " [elapsed time: 0:01:04] |********************************* | (ETA:   0:00:00) "
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YnvVBO2brU5u"
      },
      "source": [
        "## CLIP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EHwC6CiqrU5u",
        "outputId": "633eadcf-b998-49bb-b228-d2b5e283d582"
      },
      "source": [
        "clip_embeddings_test_fn = \"clip\" + val_embeddings_filename_suffix\n",
        "clip_embeddings_test = get_ndarray_from_drive(drive, folderid, clip_embeddings_test_fn)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading clip_embeddings_val.npz from GDrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BHkvWSCtFJnO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d183fdac-eeca-4e6a-a883-0c13f762e4b3"
      },
      "source": [
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "if trivial:\n",
        "  #cc\n",
        "    results_filename = \"new_metrics\"+dataset_name+\"_0t\"+str(num_ways)+\"w\"+str(num_shot)+\"s\"+str(num_augmentations)+\"a_trivial_clip_zs_metrics_with_logits.json\"\n",
        "else:\n",
        "  #cc\n",
        "    results_filename = \"new_metrics\"+dataset_name+\"_0t\"+str(num_ways)+\"w\"+str(num_shot)+\"s\"+str(num_augmentations)+\"a_metrics_clip_zs_with_logits.json\"\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "download_file_by_name(drive, folderid, results_filename)\n",
        "if results_filename in os.listdir():\n",
        "    with open(results_filename, 'r') as f:\n",
        "      #cc\n",
        "        json_loaded = json.load(f)\n",
        "        #cc\n",
        "        clip_metrics_over_train_epochs = json_loaded['metrics']\n",
        "        #cc\n",
        "        logits_over_train_epochs = json_loaded[\"logits\"]\n",
        "else:\n",
        "    clip_metrics_over_train_epochs = []  \n",
        "    #cc  \n",
        "    logits_over_train_epochs = []   \n",
        "\n",
        "train_epochs_arr = [0]\n",
        "multi_label=False\n",
        "# metrics_vals = ['hamming', 'jaccard', 'f1_score'] # ['accuracy', 'f1_score']\n",
        "\n",
        "for idx, train_epochs in enumerate(train_epochs_arr):\n",
        "    if idx < len(clip_metrics_over_train_epochs):\n",
        "        continue\n",
        "    print(train_epochs)\n",
        "    #cc\n",
        "    clip_metrics_thresholds, all_logits = run_evaluations(\n",
        "        (indices, embeddings),\n",
        "        train_indices,\n",
        "        eval_indices,\n",
        "        wi_y,\n",
        "        eval_y,\n",
        "        num_episodes,\n",
        "        num_ways,\n",
        "        class_names,\n",
        "        train_epochs=train_epochs,\n",
        "        num_augmentations=num_augmentations,\n",
        "        embeddings=clip_embeddings_test\n",
        "    )\n",
        "    clip_metrics_over_train_epochs.append(clip_metrics_thresholds)\n",
        "    #cc\n",
        "    logits_over_train_epochs.append(all_logits)\n",
        "\n",
        "#cc\n",
        "    fin_list = []\n",
        "    #cc the whole for loop\n",
        "    for a1 in wi_y:\n",
        "      fin_a1_list = []\n",
        "      for a2 in a1:\n",
        "        new_val = str(a2)\n",
        "        fin_a1_list.append(new_val)\n",
        "      fin_list.append(fin_a1_list)\n",
        "\n",
        "    with open(results_filename, 'w') as f:\n",
        "      #cc\n",
        "        results = {'metrics': clip_metrics_over_train_epochs,\n",
        "                   \"logits\": logits_over_train_epochs,\n",
        "                   \"true_labels\": fin_list}\n",
        "        json.dump(results, f)\n",
        "    auth.authenticate_user()\n",
        "    gauth = GoogleAuth()\n",
        "    gauth.credentials = GoogleCredentials.get_application_default()\n",
        "    drive = GoogleDrive(gauth)\n",
        "    delete_file_by_name(drive, folderid, results_filename)\n",
        "    save_to_drive(drive, folderid, results_filename)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r                                                                               \r\r [elapsed time: 0:00:00] |                                  | (ETA:  --:--:--) "
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " [elapsed time: 0:10:46] |**********************************| (ETA:  00:00:00) "
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Uploaded new_metricsUCF101_0t20w5s0a_metrics_clip_zs_with_logits.json to https://drive.google.com/drive/u/1/folders/1AR8qp1QPFv0M3jCIQKK8VA2xsNilo_pi\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w7-911OnFJnP"
      },
      "source": [
        "if trivial:\n",
        "    PLOT_DIR = \"NewMetrics_clip_zs_Sigmoid_\" + dataset_name + \"_0t\" + str(num_ways) + \"w\" + str(num_shot) + \"s\" + str(num_augmentations) + \"a_trivial_plots\"\n",
        "else:\n",
        "    PLOT_DIR = \"NewMetrics_clip_zs_Sigmoid_\" + dataset_name + \"_0t\" + str(num_ways) + \"w\" + str(num_shot) + \"s\" + str(num_augmentations) + \"a_plots\"\n",
        "os.mkdir(PLOT_DIR)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Sht4dvfFJnP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87abdd7f-7dfc-4924-c810-d117effb6241"
      },
      "source": [
        "all_metrics = ['accuracy', 'ap', 'map', 'c_f1', 'o_f1', 'c_precision', 'o_precision', 'c_recall', 'o_recall', 'top1_accuracy', 'top5_accuracy', 'classwise_accuracy', 'c_accuracy']\n",
        "\n",
        "final_dict = {}\n",
        "for ind_metric in all_metrics:\n",
        "  vals = []\n",
        "  final_array = []\n",
        "  for mt in clip_metrics_over_train_epochs:\n",
        "      ret_val = get_best_metric(mt,ind_metric,\"max\")\n",
        "      vals.append(ret_val)\n",
        "\n",
        "  final_array.append(vals)\n",
        "  final_dict[ind_metric] = final_array\n",
        "\n",
        "if trivial:\n",
        "    graph_filename = \"new_metrics\"+dataset_name+\"_0t\"+str(num_ways)+\"w\"+str(num_shot)+\"s\"+str(num_augmentations)+\"a_trivial_clip_zs_metrics_graphs.json\"\n",
        "else:\n",
        "    graph_filename = \"new_metrics\"+dataset_name+\"_0t\"+str(num_ways)+\"w\"+str(num_shot)+\"s\"+str(num_augmentations)+\"a_clip_zs_metrics_graphs.json\"\n",
        "\n",
        "with open(graph_filename, 'w') as f:\n",
        "        json.dump(final_dict, f)\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "delete_file_by_name(drive, folderid, graph_filename)\n",
        "save_to_drive(drive, folderid, graph_filename)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uploaded new_metricsUCF101_0t20w5s0a_clip_zs_metrics_graphs.json to https://drive.google.com/drive/u/1/folders/1AR8qp1QPFv0M3jCIQKK8VA2xsNilo_pi\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EWMwEGMLFJnQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c18a0ca-286b-4718-b370-09256c383092"
      },
      "source": [
        "zip_dirname = PLOT_DIR + \".zip\"\n",
        "zip_source = PLOT_DIR\n",
        "! zip -r $zip_dirname $zip_source\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "save_to_drive(drive, folderid, zip_dirname)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  adding: NewMetrics_clip_zs_Sigmoid_MiniImagenet_0t20w5s0a_plots/ (stored 0%)\n",
            "Uploaded NewMetrics_clip_zs_Sigmoid_MiniImagenet_0t20w5s0a_plots.zip to https://drive.google.com/drive/u/1/folders/1AR8qp1QPFv0M3jCIQKK8VA2xsNilo_pi\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RlIMyHeVfFEE"
      },
      "source": [
        "# 5 way 1 shot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7HPn06QE39N8"
      },
      "source": [
        "## Picking indices"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jup3-49v_iuU",
        "outputId": "5aba19b5-8f52-4765-ddfd-399fe58fdeb6"
      },
      "source": [
        "num_ways = 5\n",
        "num_shot = 1\n",
        "num_eval = 19\n",
        "shuffle = False\n",
        "num_episodes = 100\n",
        "\n",
        "train_indices, eval_indices, wi_y, eval_y = prepare_indices(\n",
        "    num_ways, num_shot, num_eval, num_episodes, label_dictionary, val_labels, shuffle\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r                                                                               \r\r [elapsed time: 0:00:00] |                                  | (ETA:  --:--:--) "
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1GTEuYo1Fo27",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "325f9460-5aa0-4bfb-aa11-bb05d74e75d4"
      },
      "source": [
        "embedding_model = embedding_models.CLIPEmbeddingWrapper()\n",
        "num_augmentations = 0\n",
        "trivial=False\n",
        "\n",
        "indices, embeddings = embed_images(\n",
        "    embedding_model, \n",
        "    train_indices, \n",
        "    num_augmentations,\n",
        "    trivial=trivial\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r                                                                               \r\r [elapsed time: 0:00:00] |                                  | (ETA:  --:--:--) "
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Found 13320 files belonging to 101 classes.\n",
            "Using 2664 files for validation.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " [elapsed time: 0:00:27] |********************************* | (ETA:   0:00:00) "
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M9dYlVFDF_vs"
      },
      "source": [
        "## CLIP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2y6RpvnEF_vs",
        "outputId": "14eebd80-226f-4c39-bbed-33cb368e117e"
      },
      "source": [
        "clip_embeddings_test_fn = \"clip\" + val_embeddings_filename_suffix\n",
        "clip_embeddings_test = get_ndarray_from_drive(drive, folderid, clip_embeddings_test_fn)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading clip_embeddings_val.npz from GDrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z6CQnw11FTIx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba314099-321b-4f04-cdea-444a0b5d14f6"
      },
      "source": [
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "if trivial:\n",
        "  #cc\n",
        "    results_filename = \"new_metrics\"+dataset_name+\"_0t\"+str(num_ways)+\"w\"+str(num_shot)+\"s\"+str(num_augmentations)+\"a_trivial_clip_zs_metrics_with_logits.json\"\n",
        "else:\n",
        "  #cc\n",
        "    results_filename = \"new_metrics\"+dataset_name+\"_0t\"+str(num_ways)+\"w\"+str(num_shot)+\"s\"+str(num_augmentations)+\"a_clip_zs_metrics_with_logits.json\"\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "download_file_by_name(drive, folderid, results_filename)\n",
        "if results_filename in os.listdir():\n",
        "    with open(results_filename, 'r') as f:\n",
        "      #cc\n",
        "        json_loaded = json.load(f)\n",
        "        #cc\n",
        "        clip_metrics_over_train_epochs = json_loaded['metrics']\n",
        "        #cc\n",
        "        logits_over_train_epochs = json_loaded[\"logits\"]\n",
        "else:\n",
        "    clip_metrics_over_train_epochs = []  \n",
        "    #cc  \n",
        "    logits_over_train_epochs = []   \n",
        "\n",
        "train_epochs_arr = [0]\n",
        "multi_label=False\n",
        "# metrics_vals = ['hamming', 'jaccard', 'f1_score'] # ['accuracy', 'f1_score']\n",
        "\n",
        "for idx, train_epochs in enumerate(train_epochs_arr):\n",
        "    if idx < len(clip_metrics_over_train_epochs):\n",
        "        continue\n",
        "    print(train_epochs)\n",
        "    #cc\n",
        "    clip_metrics_thresholds, all_logits = run_evaluations(\n",
        "        (indices, embeddings),\n",
        "        train_indices,\n",
        "        eval_indices,\n",
        "        wi_y,\n",
        "        eval_y,\n",
        "        num_episodes,\n",
        "        num_ways,\n",
        "        class_names,\n",
        "        train_epochs=train_epochs,\n",
        "        num_augmentations=num_augmentations,\n",
        "        embeddings=clip_embeddings_test\n",
        "    )\n",
        "    clip_metrics_over_train_epochs.append(clip_metrics_thresholds)\n",
        "    #cc\n",
        "    logits_over_train_epochs.append(all_logits)\n",
        "\n",
        "#cc\n",
        "    fin_list = []\n",
        "    #cc the whole for loop\n",
        "    for a1 in wi_y:\n",
        "      fin_a1_list = []\n",
        "      for a2 in a1:\n",
        "        new_val = str(a2)\n",
        "        fin_a1_list.append(new_val)\n",
        "      fin_list.append(fin_a1_list)\n",
        "\n",
        "    with open(results_filename, 'w') as f:\n",
        "      #cc\n",
        "        results = {'metrics': clip_metrics_over_train_epochs,\n",
        "                   \"logits\": logits_over_train_epochs,\n",
        "                   \"true_labels\": fin_list}\n",
        "        json.dump(results, f)\n",
        "    auth.authenticate_user()\n",
        "    gauth = GoogleAuth()\n",
        "    gauth.credentials = GoogleCredentials.get_application_default()\n",
        "    drive = GoogleDrive(gauth)\n",
        "    delete_file_by_name(drive, folderid, results_filename)\n",
        "    save_to_drive(drive, folderid, results_filename)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r                                                                               \r\r [elapsed time: 0:00:00] |                                  | (ETA:  --:--:--) "
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " [elapsed time: 0:10:44] |**********************************| (ETA:  00:00:00) "
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Uploaded new_metricsUCF101_0t5w1s0a_clip_zs_metrics_with_logits.json to https://drive.google.com/drive/u/1/folders/1AR8qp1QPFv0M3jCIQKK8VA2xsNilo_pi\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qGo3102kFTIy"
      },
      "source": [
        "if trivial:\n",
        "    PLOT_DIR = \"NewMetrics_clip_zs_Sigmoid_\" + dataset_name + \"_0t\" + str(num_ways) + \"w\" + str(num_shot) + \"s\" + str(num_augmentations) + \"a_trivial_plots\"\n",
        "else:\n",
        "    PLOT_DIR = \"NewMetrics_clip_zs_Sigmoid_\" + dataset_name + \"_0t\" + str(num_ways) + \"w\" + str(num_shot) + \"s\" + str(num_augmentations) + \"a_plots\"\n",
        "os.mkdir(PLOT_DIR)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2bOX_AWEFTIz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e66fb00-b58c-4bac-97d7-1f47cf482341"
      },
      "source": [
        "all_metrics = ['accuracy', 'ap', 'map', 'c_f1', 'o_f1', 'c_precision', 'o_precision', 'c_recall', 'o_recall', 'top1_accuracy', 'top5_accuracy', 'classwise_accuracy', 'c_accuracy']\n",
        "\n",
        "final_dict = {}\n",
        "for ind_metric in all_metrics:\n",
        "  vals = []\n",
        "  final_array = []\n",
        "  for mt in clip_metrics_over_train_epochs:\n",
        "      ret_val = get_best_metric(mt,ind_metric,\"max\")\n",
        "      vals.append(ret_val)\n",
        "\n",
        "  final_array.append(vals)\n",
        "  final_dict[ind_metric] = final_array\n",
        "\n",
        "if trivial:\n",
        "    graph_filename = \"new_metrics\"+dataset_name+\"_0t\"+str(num_ways)+\"w\"+str(num_shot)+\"s\"+str(num_augmentations)+\"a_trivial_clip_zs_metrics_graphs.json\"\n",
        "else:\n",
        "    graph_filename = \"new_metrics\"+dataset_name+\"_0t\"+str(num_ways)+\"w\"+str(num_shot)+\"s\"+str(num_augmentations)+\"a_clip_zs_metrics_graphs.json\"\n",
        "\n",
        "with open(graph_filename, 'w') as f:\n",
        "        json.dump(final_dict, f)\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "delete_file_by_name(drive, folderid, graph_filename)\n",
        "save_to_drive(drive, folderid, graph_filename)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uploaded new_metricsUCF101_0t5w1s0a_clip_zs_metrics_graphs.json to https://drive.google.com/drive/u/1/folders/1AR8qp1QPFv0M3jCIQKK8VA2xsNilo_pi\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZjpIIX6FTIz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a06335e-f6d5-4294-98b9-9ab5df7a8840"
      },
      "source": [
        "zip_dirname = PLOT_DIR + \".zip\"\n",
        "zip_source = PLOT_DIR\n",
        "! zip -r $zip_dirname $zip_source\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "save_to_drive(drive, folderid, zip_dirname)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  adding: NewMetrics_clip_zs_Sigmoid_MiniImagenet_0t5w1s0a_plots/ (stored 0%)\n",
            "Uploaded NewMetrics_clip_zs_Sigmoid_MiniImagenet_0t5w1s0a_plots.zip to https://drive.google.com/drive/u/1/folders/1AR8qp1QPFv0M3jCIQKK8VA2xsNilo_pi\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBhFDrzlfTHf"
      },
      "source": [
        "# 20 way 1 shot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6u112WKcfTHg"
      },
      "source": [
        "## Picking indices"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-th0mOYQfTHg",
        "outputId": "e7e87783-75ad-40a5-d641-48740d65723e"
      },
      "source": [
        "num_ways = 20\n",
        "num_shot = 1\n",
        "num_eval = 10\n",
        "shuffle = False\n",
        "num_episodes = 100\n",
        "\n",
        "train_indices, eval_indices, wi_y, eval_y = prepare_indices(\n",
        "    num_ways, num_shot, num_eval, num_episodes, label_dictionary, val_labels, shuffle\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " [elapsed time: 0:00:00] |***************************       | (ETA:   0:00:00) "
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-wHhnx9F2v2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ebd62ed-5776-4366-ef05-86e24c430008"
      },
      "source": [
        "embedding_model = embedding_models.CLIPEmbeddingWrapper()\n",
        "num_augmentations = 0\n",
        "trivial=False\n",
        "\n",
        "indices, embeddings = embed_images(\n",
        "    embedding_model, \n",
        "    train_indices, \n",
        "    num_augmentations,\n",
        "    trivial=trivial\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r                                                                               \r\r [elapsed time: 0:00:00] |                                  | (ETA:  --:--:--) "
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Found 13320 files belonging to 101 classes.\n",
            "Using 2664 files for validation.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " [elapsed time: 0:00:40] |**********************************| (ETA:  00:00:00) "
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sph7dLY0fTHh"
      },
      "source": [
        "## CLIP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0wBzaqB3fTHh",
        "outputId": "1be03f7a-c93a-433e-b63a-e58c4598c113"
      },
      "source": [
        "clip_embeddings_test_fn = \"clip\" + val_embeddings_filename_suffix\n",
        "clip_embeddings_test = get_ndarray_from_drive(drive, folderid, clip_embeddings_test_fn)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading clip_embeddings_val.npz from GDrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMTxo_ZaFXxM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b37f187-f0df-4ad1-c911-07170b89d8f2"
      },
      "source": [
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "if trivial:\n",
        "  #cc\n",
        "    results_filename = \"new_metrics\"+dataset_name+\"_0t\"+str(num_ways)+\"w\"+str(num_shot)+\"s\"+str(num_augmentations)+\"a_trivial_clip_zs_metrics_with_logits.json\"\n",
        "else:\n",
        "  #cc\n",
        "    results_filename = \"new_metrics\"+dataset_name+\"_0t\"+str(num_ways)+\"w\"+str(num_shot)+\"s\"+str(num_augmentations)+\"a_clip_zs_metrics_with_logits.json\"\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "download_file_by_name(drive, folderid, results_filename)\n",
        "if results_filename in os.listdir():\n",
        "    with open(results_filename, 'r') as f:\n",
        "      #cc\n",
        "        json_loaded = json.load(f)\n",
        "        #cc\n",
        "        clip_metrics_over_train_epochs = json_loaded['metrics']\n",
        "        #cc\n",
        "        logits_over_train_epochs = json_loaded[\"logits\"]\n",
        "else:\n",
        "    clip_metrics_over_train_epochs = []  \n",
        "    #cc  \n",
        "    logits_over_train_epochs = []   \n",
        "\n",
        "train_epochs_arr = [0]\n",
        "multi_label=False\n",
        "# metrics_vals = ['hamming', 'jaccard', 'f1_score'] # ['accuracy', 'f1_score']\n",
        "\n",
        "for idx, train_epochs in enumerate(train_epochs_arr):\n",
        "    if idx < len(clip_metrics_over_train_epochs):\n",
        "        continue\n",
        "    print(train_epochs)\n",
        "    #cc\n",
        "    clip_metrics_thresholds, all_logits = run_evaluations(\n",
        "        (indices, embeddings),\n",
        "        train_indices,\n",
        "        eval_indices,\n",
        "        wi_y,\n",
        "        eval_y,\n",
        "        num_episodes,\n",
        "        num_ways,\n",
        "        class_names,\n",
        "        train_epochs=train_epochs,\n",
        "        num_augmentations=num_augmentations,\n",
        "        embeddings=clip_embeddings_test\n",
        "    )\n",
        "    clip_metrics_over_train_epochs.append(clip_metrics_thresholds)\n",
        "    #cc\n",
        "    logits_over_train_epochs.append(all_logits)\n",
        "\n",
        "#cc\n",
        "    fin_list = []\n",
        "    #cc the whole for loop\n",
        "    for a1 in wi_y:\n",
        "      fin_a1_list = []\n",
        "      for a2 in a1:\n",
        "        new_val = str(a2)\n",
        "        fin_a1_list.append(new_val)\n",
        "      fin_list.append(fin_a1_list)\n",
        "\n",
        "    with open(results_filename, 'w') as f:\n",
        "      #cc\n",
        "        results = {'metrics': clip_metrics_over_train_epochs,\n",
        "                   \"logits\": logits_over_train_epochs,\n",
        "                   \"true_labels\": fin_list}\n",
        "        json.dump(results, f)\n",
        "    auth.authenticate_user()\n",
        "    gauth = GoogleAuth()\n",
        "    gauth.credentials = GoogleCredentials.get_application_default()\n",
        "    drive = GoogleDrive(gauth)\n",
        "    delete_file_by_name(drive, folderid, results_filename)\n",
        "    save_to_drive(drive, folderid, results_filename)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r                                                                               \r\r [elapsed time: 0:00:00] |                                  | (ETA:  --:--:--) "
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " [elapsed time: 0:10:57] |**********************************| (ETA:  00:00:00) "
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Uploaded new_metricsUCF101_0t20w1s0a_clip_zs_metrics_with_logits.json to https://drive.google.com/drive/u/1/folders/1AR8qp1QPFv0M3jCIQKK8VA2xsNilo_pi\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jeskwalZFXxN"
      },
      "source": [
        "if trivial:\n",
        "    PLOT_DIR = \"NewMetrics_clip_zs_Sigmoid_\" + dataset_name + \"_0t\" + str(num_ways) + \"w\" + str(num_shot) + \"s\" + str(num_augmentations) + \"a_trivial_plots\"\n",
        "else:\n",
        "    PLOT_DIR = \"NewMetrics_clip_zs_Sigmoid_\" + dataset_name + \"_0t\" + str(num_ways) + \"w\" + str(num_shot) + \"s\" + str(num_augmentations) + \"a_plots\"\n",
        "os.mkdir(PLOT_DIR)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_TYQpdqDFXxO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c47fa5b0-8628-4c57-a668-cc63007b0cbd"
      },
      "source": [
        "all_metrics = ['accuracy', 'ap', 'map', 'c_f1', 'o_f1', 'c_precision', 'o_precision', 'c_recall', 'o_recall', 'top1_accuracy', 'top5_accuracy', 'classwise_accuracy', 'c_accuracy']\n",
        "\n",
        "final_dict = {}\n",
        "for ind_metric in all_metrics:\n",
        "  vals = []\n",
        "  final_array = []\n",
        "  for mt in clip_metrics_over_train_epochs:\n",
        "      ret_val = get_best_metric(mt,ind_metric,\"max\")\n",
        "      vals.append(ret_val)\n",
        "\n",
        "  final_array.append(vals)\n",
        "  final_dict[ind_metric] = final_array\n",
        "\n",
        "if trivial:\n",
        "    graph_filename = \"new_metrics\"+dataset_name+\"_0t\"+str(num_ways)+\"w\"+str(num_shot)+\"s\"+str(num_augmentations)+\"a_trivial_clip_zs_metrics_graphs.json\"\n",
        "else:\n",
        "    graph_filename = \"new_metrics\"+dataset_name+\"_0t\"+str(num_ways)+\"w\"+str(num_shot)+\"s\"+str(num_augmentations)+\"a_clip_zs_metrics_graphs.json\"\n",
        "\n",
        "with open(graph_filename, 'w') as f:\n",
        "        json.dump(final_dict, f)\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "delete_file_by_name(drive, folderid, graph_filename)\n",
        "save_to_drive(drive, folderid, graph_filename)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uploaded new_metricsUCF101_0t20w1s0a_clip_zs_metrics_graphs.json to https://drive.google.com/drive/u/1/folders/1AR8qp1QPFv0M3jCIQKK8VA2xsNilo_pi\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4dkyC88hFXxO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9024945f-2013-428c-b736-f4f248575af0"
      },
      "source": [
        "zip_dirname = PLOT_DIR + \".zip\"\n",
        "zip_source = PLOT_DIR\n",
        "! zip -r $zip_dirname $zip_source\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "save_to_drive(drive, folderid, zip_dirname)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  adding: NewMetrics_clip_zs_Sigmoid_MiniImagenet_0t20w1s0a_plots/ (stored 0%)\n",
            "Uploaded NewMetrics_clip_zs_Sigmoid_MiniImagenet_0t20w1s0a_plots.zip to https://drive.google.com/drive/u/1/folders/1AR8qp1QPFv0M3jCIQKK8VA2xsNilo_pi\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}